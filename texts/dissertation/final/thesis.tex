% !TeX spellcheck = pt_BR
% !TeX encoding = UTF-8
% Escolha: portugues ou ingles ou espanhol.
% Para a versão final do texto, após a defesa acrescente final:

% Para o exame de qualificação
%\documentclass[portugues, qualificacao]{ic-tese}
% Para a tese
%\documentclass[portugues]{ic-tese}
% Para a versão final da tese
\documentclass[Portugues,Final]{ic-tese-v3}

% Extra macros para escrita 
%\usepackage{ic-extras}

\usepackage[latin1,utf8]{inputenc}
\usepackage{pifont}

%% Math
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{commath}

\usepackage{amsfonts}% revise se precisa ou não
\usepackage{nccmath}% revise se precisa ou não

%\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{graphicx}

\usepackage{listings}
\usepackage{float}
\usepackage{enumitem}

\usepackage{wasysym}
\usepackage{rotating}
\usepackage{xurl}

%\usepackage{fancyhdr}
%\usepackage{cite}
%\usepackage{booktabs}
%\usepackage{hyperref}
%\usepackage{subfigure}
%\usepackage{xparse}
\usepackage{amssymb}
%\usepackage{svg}

\renewcommand{\lstlistingname}{Código}
\renewcommand{\lstlistlistingname}{Lista de \lstlistingname s}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.85,0.85,0.85}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblack}{rgb}{0,0,0}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  %keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  %ndkeywordstyle=\color{codegray}\bfseries,
  %identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  %commentstyle=\color{purple}\ttfamily,
  %stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstdefinestyle{codigo}{
    backgroundcolor=\color{codegray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codeblack},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

\lstset{style=codigo}

\DeclarePairedDelimiter{\round}\lfloor\rceil
\newcommand\citetxt[1]{%
  \citeauthor{#1}~(\citeyear{#1})}
\newcommand\citepar[1]{%
  (\citeauthor{#1}, \citeyear{#1})} 


%% tight space around equations
%\apptocmd\normalsize{%
%  \abovedisplayskip=7pt plus 2pt minus 7pt
%  \abovedisplayshortskip=0pt plus 3pt
%  \belowdisplayskip=7pt plus 2pt minus 6pt
%  \belowdisplayshortskip=4pt plus 3pt minus 4pt
%}{}{}
%
%\abovedisplayskip=0pt
%\belowdisplayskip=0pt

\newcommand{\conc}{\mathbin{\Vert}}

% Adaptive norm operator
% https://tex.stackexchange.com/a/70130/7561
% With this setup, \norm*{...} will produce automatically sized double-bar
% fence symbols around the macro's, whereas (say) \norm[\bigg]{...} will
% generate \bigg double-bar symbols. There are four sizing modifiers: \big,
% \Big, \bigg, and \Bigg. 

%https://tex.stackexchange.com/q/151984/7561
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\kl}{\operatorname{KL}\infdivx}
\newcommand{\skl}{\operatorname{SKL}\infdivx}

% https://tex.stackexchange.com/questions/23773/a-centered-plus-minus-symbol
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

% https://tex.stackexchange.com/a/141685/7561
\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase

% https://tex.stackexchange.com/a/171959/7561
\newcommand\reallywidehat[1]{\ThisStyle{%
    \setbox0=\hbox{$\SavedStyle#1$}%
    \stackengine{-1.0\ht0+.5pt}{$\SavedStyle#1$}{%
      \stretchto{\scaleto{\SavedStyle\mkern.15mu\char'136}{1.5\wd0}}{1.4\ht0}%
    }{O}{c}{F}{T}{S}%
}}
\newcommand{\hkl}{\widehat{\operatorname{KL}}\infdivx}

% Conditional independence symbol
% https://tex.stackexchange.com/questions/218631/symbol-for-not-conditionally-independent/
\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}

% declare my argmin and argmax operators
% https://tex.stackexchange.com/a/5255/7561
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% declaring an expectation operator to behave like sum
% https://tex.stackexchange.com/a/23436/7561
\makeatletter
\DeclareRobustCommand\bigop[1]{%
  \mathop{\vphantom{\sum}\mathpalette\bigop@{#1}}\slimits@
}
\newcommand{\bigop@}[2]{%
  \vcenter{%
    \sbox\z@{$#1\sum$}%
    \hbox{\resizebox{\ifx#1\displaystyle.9\fi\dimexpr\ht\z@+\dp\z@}{!}{$\m@th#2$}}%
  }%
}
\makeatother

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords ---}} #1
}
\providecommand{\palavraschave}[1]
{
  \small	
  \textbf{\textit{Palavras-chave ---}} #1
}

%\newcommand{\E}{\DOTSB\bigop{\mathbb{E}}}
\DeclareMathOperator*{\E}{\mathbb{E}}

% redefining \times operator
\let\oldtimes\times
\def\times{{\mkern1mu\oldtimes\mkern1mu}}



\usepackage{subfigure}
\newcommand{\cmark}{\ding{51}}%

%\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\product{\langle}{\rangle}%

\usepackage{cellspace}
\setlength\cellspacetoplimit{6pt}
\setlength\cellspacebottomlimit{4pt}

%% Units
\usepackage{siunitx}

%% Tables
\usepackage{booktabs}

\usepackage{url}


% Para acrescentar comentários ao PDF descomente:
\usepackage
%  [pdfauthor={nome do autor},
%   pdftitle={titulo},
%   pdfkeywords={palavra-chave, palavra-chave},
%   pdfproducer={Latex with hyperref},
%   pdfcreator={pdflatex}]
{hyperref}

% Os cores podem ser mudados
\hypersetup{
%  linkcolor={red!50!black},
%  citecolor={blue!50!black},
%  urlcolor={blue!80!black}
}

\begin{document}

% Escolha entre autor ou autora:
\autor{Thales Eduardo Nazatto}
%\autora{Nome da Autora}

% Sempre deve haver um título em português:
\titulo{Um arcabouço semi\textendash autônomo para treino de modelos de Aprendizado de Máquina com enfoque em métricas de \textit{Fairness}}
%\title{text}

% Se a língua for o inglês ou o espanhol defina:
%\title{The Dissertation or Thesis Title in English or Spanish}

% Escolha entre orientador ou orientadora. Inclua os títulos acadêmicos:
%\orientador{Prof. Dr. Gerberth Adín Ramírez Rivera}
\orientadora{Profa. Dra. Cecília Mary Fischer Rubira}

% Escolha entre coorientador ou coorientadora, se houver.  Inclua os títulos acadêmicos:
\coorientador{Prof. Dr. Leonardo Montecchi}
%\coorientadora{Prof. Dra. Eng. Lic. Nome da Co-Orientadora}

% Escolha entre mestrado ou doutorado:
\mestrado
%\doutorado

% Se houve cotutela, defina:
%\cotutela{Universidade Nova de Plutão}

\datadadefesa{08}{11}{2023}

% Para a versão final defina:
\avaliadorA{Profa. Dra. Cecília Mary Fischer Rubira}{IC/UNICAMP}
\avaliadorB{Prof. Dr. Fabio Kon}{IME/USP}
\avaliadorC{Prof. Dr. Marcos Medeiros Raimundo}{IC/UNICAMP}
%\avaliadorD{Prof. Dr. Quarto Avaliador}{Instituição do quarto avaliador}
%\avaliadorE{Prof. Dr. Quinto Avaliador}{Instituição do quinto avaliador}
%\avaliadorF{Prof. Dr. Sexto Avaliador}{Instituição do sexto avaliador}
%\avaliadorG{Prof. Dr. Sétimo Avaliador}{Instituição do sétimo avaliador}
%\avaliadorH{Prof. Dr. Oitavo Avaliador}{Instituição do oitavo avaliador}


% Para incluir a ficha catalográfica em PDF na versão final, descomente e ajuste:
\fichacatalografica{Ficha-Catalografica-Protocolo-712043869.pdf}


% Este comando deve ficar aqui:
% \frontmatter
\paginasiniciais


% Se houver dedicatória, descomente:
%\prefacesection{Dedicatória}
%A dedicatória deve ocupar uma única página.

% Se houver epígrafe, descomente e edite:
\begin{epigrafe}
{\it
Você não consegue ligar os pontos olhando pra frente; você só consegue ligá-los olhando pra trás. Então você tem que confiar que os pontos se ligarão algum dia no futuro. Você tem que confiar em algo – seu instinto, destino, vida, carma, o que for. Esta abordagem nunca me desapontou, e fez toda diferença na minha vida.}

\hfill (Steve Jobs)
\end{epigrafe}


% Agradecimentos ou Acknowledgements ou Agradecimientos
\prefacesection{Agradecimentos}
%Os agradecimentos devem ocupar uma única página.
Primeiramente, à minha família, pela dedicação que tiveram em me criar, pela liberdade de escolha para fazer o que gosto e pela compreensão atual de que hoje seguimos caminhos completamente distintos, apesar de mantermos contato constantemente.

A meus orientadores, Profa. Dra. Cecília Mary Fischer Rubira e Prof. Dr. Leonardo Montecchi, pelo desafio de orientar uma dissertação com temas fora de seus domínios de estudo, e também ao Prof. Dr. Gerberth Adín Ramírez Rivera, que me aceitou como orientador anteriormente, foi compreensivo no momento de minha desistência e que pude fazer reflexões com tal experiência que levei para esta dissertação final de alguma forma.

A todas as pessoas com quem morei em Campinas nesses anos desde que comecei a frequentar aulas, presentes nas Repúblicas Borritos, KioBio e Galinheiro, pelos momentos de lazer que me fizeram relaxar das tensões encaradas em um curso de Pós-Graduação.

A todas as pessoas que conheci na época em que eu estudei em Rio Claro e reencontrei em Campinas, e também a todas as pessoas que mantive contato de Rio Claro desde que comecei a frequentar aulas, principalmente por entender que as conexões e comunicações se mantém mesmo quando um ciclo se fecha e um ciclo diferente é iniciado.

A todas as pessoas que trabalhei junto na CI\&T, Dextra e Zup, pela compreensão de que o estudo também é essencial para a evolução profissional de uma pessoa.

E finalmente, a todas as pessoas que pude conhecer na Unicamp, pela troca de experiências e pelo contato com pessoas de altíssimo nível e acima de todas as minhas expectativas.

% Sempre deve haver um resumo em português:
\begin{resumo}
O uso crescente de Aprendizado de Máquina em soluções digitais criou condições para o diálogo social sobre vieses e discriminações presentes nos dados, e requeriram a criação de novos algoritmos e métricas de \textit{Fairness} para garantir decisões mais justas. Entretanto, a análise do Cientista de Dados para obter os melhores modelos é mais complexa pela necessidade de equilibrar o aprimoramento das métricas de \textit{Fairness} com uma possível queda das métricas de avaliação tradicionais, considerando o contexto, e por uma maior variedade de algoritmos que podem ser utilizados isoladamente ou em conjunto durante o treinamento. Este trabalho apresenta uma solução para o treino destes modelos que é operada de forma semi-autônoma para encontrar configurações mais otimizadas em diferentes contextos, através de um módulo com variados algoritmos pré-implementados e a arquitetura MAPE-K para auxiliar na avaliação do equilíbrio ideal. Foram realizados diversos estudos de caso para determinar a viabilidade da solução proposta na resolução desses problemas e se a solução implementada é extensível a futuros algoritmos sem grandes esforços. Diante desses estudos, foi notado que a solução pode auxiliar o Cientista de Dados a ter melhor compreensão do processo de treinamento, possibilitando estudos de Engenharia de Software no auxílio ao treinamento de modelos confiáveis em Aprendizado de Máquina.

\palavraschave{Aprendizado de Máquina, Inteligência Artificial Ética, Responsabilidade Social em Inteligência Artificial, Computação Autônoma, Métricas de \textit{Fairness}}
\end{resumo}


% Sempre deve haver um abstract:
\begin{abstract}
The growing use of \textit{Machine Learning} in digital solutions has created conditions for social dialogue on biases and discrimination in data, and has required the creation of new algorithms and \textit{Fairness} metrics to ensure fairer decisions. However, the Data Scientist's analysis to obtain the best models is more complex due to the need to balance the improvement of \textit{Fairness} metrics with a possible decline in traditional evaluation metrics considering the context, as well as a greater variety of algorithms that can be used independently or in combination during training. This work presents a solution for the training of these models that is operated in a semi-autonomous way to identify more optimized configurations across different contexts, through a module with several pre-implemented algorithms and the MAPE-K architecture to assist in evaluating the ideal balance. Several case studies were conducted to determine the feasibility of the proposed solution in solving these problems and if the implemented solution is extensible to future algorithms with minimal effort. In view of these studies, it was noted that the solution can help the Data Scientist in gaining a better understanding of the training process, enabling Software Engineering studies to help in the training of trustworthy \textit{Machine Learning} models.

\keywords{Machine Learning, Artificial Intelligence Ethics, Social Responsibility in Artificial Intelligence, Autonomic Computing, Fairness Metrics}
\end{abstract}



% Se houver um resumo em espanhol, descomente:
%\begin{resumen}
% A mesma regra aplica-se.
%\end{resumen}


% A lista de figuras é opcional:
\listoffigures

% A lista de tabelas é opcional:
\listoftables

% A lista de abreviações e siglas é opcional:
% \prefacesection{Lista de Abreviações e Siglas}

% A lista de símbolos é opcional:
% \prefacesection{Lista de Símbolos}

% Quem usa o pacote nomencl pode incluir:
%\renewcommand{\nomname}{Lista de Abreviações e Siglas}
%\printnomenclature[3cm]


% O sumário vem aqui:
\tableofcontents


% E esta linha deve ficar bem aqui:
% \mainmatter
\fimdaspaginasiniciais

\chapter{Introdução}

% O corpo da dissertação ou tese começa aqui:

Técnicas de Inteligência Artificial e Aprendizado de Máquina (\textit{Machine Learning}, ou ML) já são utilizadas há bastante tempo no ramo da Computação. Ramos como robótica e jogos são grandes exemplos, dada a necessidade nos mesmos de automatizar comportamentos que seriam tidos como triviais para um ser humano mas que são difíceis de serem traduzidos em código. Entretanto, nos últimos anos ocorreu um crescimento no uso dessas tecnologias em aplicações tradicionais, com estimativa de US\$ 57 bilhões em investimentos em 2021, 480\% maior em relação a 2017~\cite{Deloitte_2018}. No Brasil, o número de empresas de IA aumentou de 120 em 2018 para 206 em 2020~\cite{CIO_2021}.

Isso se mostra possível devido a grande quantidade de dados processada diariamente pelas empresas, que coletam estatísticas toda vez que um usuário acessa suas aplicações. Com esses dados, podem traçar diferentes perfis e usar soluções de ML para ter tomadas de decisão mais assertivas com o objetivo de melhorar a experiência de usuário e corrigir problemas. Porém, muitas dessas soluções foram projetadas sem pensar em governança de dados como requisito de projeto, e se mostram ineficientes quando ela é tida em consideração.

\section{Contexto}

Governança de dados é um tema que entrou em evidência recentemente em países como o Brasil: Iniciativas como a LGPD - Lei Geral de Proteção de Dados \cite{LGPD_2021}, de 14 de agosto de 2018, mostram como as aplicações e seus dados possuem cada vez mais influência na sociedade moderna. E nesse ponto muitas aplicações baseadas em Aprendizado de Máquina falham: muitas implementações são baseadas em caixas pretas, onde o determinante para estabelecer a confiança no modelo implementado é sua entrada e sua saída. Um efeito colateral dessa estratégia é a exposição de vieses que, embora sejam vistos como não-intencionais pelos desenvolvedores, refletem preconceitos existentes da sociedade atual. Uma entrada de dados enviesada resulta em um modelo de ML que realiza discriminações em sua classificação~\cite{Buolamwini_2018}, e considerando que as métricas de avaliação tradicionais avaliam a totalidade do conjunto de dados em vez de grupos específicos, discriminações não são facilmente percebidas por elas.

Devido a esse problema, existem métricas para determinar o quão o modelo está preparado para dados sensíveis, passíveis de discriminações, termo que é conhecido como \textit{Fairness} \cite{Begley_2021}. Com a evolução das pesquisas na comunidade acadêmica, também foram criados algoritmos para redução dos vieses presentes nos conjuntos de dados, como por exemplo \textit{Reweighing}~\cite{Kamiran_2011}, \textit{Adversarial Debiasing}~\cite{Zhang_2018} e \textit{Reject Option Classification}~\cite{Kamiran_2012}.

\section{Problemas encontrados}

Embora suas premissas tenham bastante utilidade no cenário do mundo de hoje, uma solução voltada ao treino de modelos de ML mais justos adicionam alguns problemas em relação a um modelo de ML tradicional:

\begin{itemize}
\item \textbf{Aumento da complexidade na configuração arquitetural utilizada:} Em uma solução voltada ao treino de modelos de ML tradicionais, os algoritmos utilizados são executados, em geral, durante a fase de treinamento. Embora hajam técnicas e algoritmos que podem ser aplicados antes deste processo para melhorar a modelagem do problema, ao adicionar o cálculo de métricas de \textit{Fairness} como requisito desta aplicação aumentamos a gama de algoritmos a se testar para obter melhores resultados, que podem ser executados antes, durante e após a fase de treinamento.
\item \textbf{Aumento da complexidade na avaliação e calibração de um modelo de ML:} Para medir o cálculo de métricas de \textit{Fairness} são necessárias novas métricas especificamente para esse fim, que aumentam o tempo para análise e validação se o modelo final é bom o suficiente ou não. Além do maior número de métricas a se avaliar, é provável que, quando um modelo de ML mais justo é considerado como requisito, um modelo treinado com algoritmos voltados para reduzir vieses e com um determinado conjunto de dados seja avaliado com métricas de avaliação piores do que um modelo treinado com algoritmos tradicionais, uma vez que o algoritmo voltado para reduzir vieses precisa fazer concessões para que grupos considerados com privilégios e grupos considerados sem privilégios tenham os resultados mais semelhantes possíveis. Esse compromisso precisa ser considerado nos resultados finais para que o aprimoramento das métricas de \textit{Fairness} compense uma hipotética queda das métricas de avaliação tradicionais.
\end{itemize}

\section{Objetivo}

O objetivo desta dissertação de mestrado é desenvolver uma estrutura para o treinamento de modelos de \textit{Machine Learning} de forma semi-autônoma, que permita que o cientista de dados obtenha a melhor configuração arquitetural do processo, resolvendo os dois problemas principais:

\begin{itemize}
\item Facilitar a criação de modelos justos e confiáveis com a automatização do seguinte processo: Preparação dos dados, treinamento e avaliação.
\item Estabelecer um balanceamento entre métricas de avaliação tradicionais com métricas de \textit{Fairness} para obter modelos mais justos com eficiência.
\end{itemize}

\section{Solução Proposta}

A solução é dividida em 4 módulos principais: Engenharia de dados, Módulo de ML, Gerenciador Autonômico e uma Interface Humano-Computador. Tais módulos sintetizam os processos de Organização e Análise da IBM Analytics and AI Reference Architecture~\cite{IBM_2021}.

Para o desenvolvimento do Módulo de ML, será utilizado o padrão de arquitetura \textit{Pipes-and-Filters}~\cite{Garlan_1993}. Para o Gerenciador Autonômico, será utilizada a arquitetura MAPE-K~\cite{IBM_2005} para analisar uma base de conhecimento e prover o melhor pipeline seguindo regras pré-determinadas. A Interface Humano-Computador foi criada nos moldes de uma aplicação web. O código deste desenvolvimento foi disponibilizado no GitHub\footnote{Repositório Git contendo os códigos deste projeto: \url{https://github.com/tenazatto/MsC}} para avaliação e testes em estudos posteriores.

\section{Resultados Obtidos}

A escolha do padrão de arquitetura \textit{Pipes-and-Filters} se mostrou favorável para o desenvolvimento de um \textit{workflow} para aplicações envolvendo IA, permitindo que ele seja modular e que sejam feitas evoluções sem exigir grandes esforços. O uso da arquitetura MAPE-K também se mostrou favorável, permitindo diversos resultados para diferentes contextos de problema e uma simplificação da análise realizada pelo Cientista de Dados, podendo resultar em economia de tempo. Ela também possibilitou um balanço entre métricas de avaliação e métricas de \textit{Fairness} através da adição de pesos para cada métrica na parte de análise. Embora os pesos não sejam parte da arquitetura, a divisão presente na arquitetura permite que o desenvolvimento seja pensado de maneira mais clara.

A obtenção de metadados do Módulo de ML se mostrou essencial para alimentar o Gerenciador Autonômico e possibilitou a análise e tomadas de decisão baseadas em dados. É possível realizar análises mais detalhadas conforme novas execuções forem realizadas, consequentemente podendo resultar em melhores configurações arquiteturais para contextos distintos. Posteriormente, é possível focar em melhorar a análise do Gerenciador Autonômico adicionando novos indicadores, expandir o Módulo de ML com técnicas não exploradas e explorar processos de operação e infra-estrutura, também conhecidos como MLOps, para melhorar a confiabilidade dos modelos por meio de mecanismos para implantação dos modelos de ML e notificações.

\section{Organização}

O restante da dissertação se organizará da seguinte forma: o Capítulo 2 descreve os conceitos que irão ser abordados neste projeto e trabalhos relacionados; o Capítulo 3 mostra como a solução é organizada e como ela se encaixaria em um processo de desenvolvimento; o Capítulo 4 mostra detalhes de implementação e seu uso; os Capítulos 5, 6 e 7 discutem os resultados obtidos aravés dos Estudos de Caso e, finalmente, o Capítulo 8 estabelece as conclusões, considerações finais e sugestões de trabalhos futuros e evoluções.

\chapter{Background e Trabalhos Relacionados}

Este capítulo fornece uma visão geral dos principais conceitos e dos trabalhos relacionados neste trabalho. Os tópicos que exploram Ciência de Dados e Engenharia de Dados abordarão seus conceitos, semelhanças e diferenças. Os tópicos que exploram \textit{Machine Learning} e \textit{Fairness} abordarão seus conceitos, as métricas utilizadas, algoritmos principais e algoritmos focados para redução de vieses. Os tópicos que exploram Engenharia de Software abordarão arquiteturas e padrões utilizados neste trabalho, como MAPE-K e \textit{Pipes-and-Filters}, e conceitos explorados em aplicações de ML, como AI Reference Architecture e MLOps. Estes conhecimentos são necessários para entender a abordagem que esta dissertação apresenta, propondo uma integação entre Dados, Engenharia de Software e Machine Learning de maneira coesa.

\section{Ciência de Dados e Engenharia de Dados}
\label{sec:CienciaEngDados}

\subsection{Engenharia de Dados}

A engenharia de dados é o meio para entender um processo. Os dados podem ser gerados de várias maneiras, ou um subconjunto dos dados disponíveis pode usar técnicas de análise de dados de estatísticas, aprendizado de máquina, reconhecimento de padrões ou redes neurais, juntamente com outras tecnologias, como visualização, otimização, sistemas de banco de dados. Dados, ferramentas de prototipagem e elicitação de conhecimento. O objetivo é usar os dados disponíveis ou gerar mais dados e assim entender o processo que está sendo investigado. O processo de analisar os dados, criar novas ferramentas de análise especificamente para a tarefa e trabalhar com especialistas do domínio é um aspecto fundamental dessa tarefa de engenharia. Atualmente engenharia de dados é muito utilizada em conjunto com o termo \textit{Big Data}, para a limpeza, tratamento e estabelecimento de processos para governança de grandes volumes de dados.

O termo \textit{Big Data} apareceu uma vez como conceito em 1974 e novamente em editoriais em 2006 e 2007, e somente em 2008 seu uso como conceito começou a aparecer regularmente em artigos científicos, mas implementações desse conceito começaram a partir de 2010~\cite{Raban_2020}. Quanto a engenharia de dados, embora periódicos como o IEEE Transactions on Knowledge and Data Engineering, cuja primeira edição foi lançada em 1989, e conferências como a IEEE International Conference on Data Engineering (ICDE), cuja primeira edição foi realizada em 1984, possam ser consideradas pontapés iniciais para discussões e artigos acadêmicos, o embrião da engenharia de dados vem do \textit{paper} \textit{"A Business Intelligence System"}, de 1958~\cite{Panoply_2017}. Nele, é mencionada a ideia de sistemas rodados por máquinas para abstrair e padronizar informações de vários setores da sociedade, como industrial, científico e de organizações governamentais, e como estas podem disseminar informação de uma maneira mais eficiente. Embora os anos 50/60 foram o embrião para o conceito, os anos 70/80 o amadureceram e construíram a base para a estrutura de engenharia de dados~\cite{Panoply_2017}.

Nos anos 70/80, os problemas em engenharia de dados eram classificados de acordo com 3 atributos~\cite{IEEE_1989}:

\begin{itemize}
\item \textbf{Completude do conhecimento e dos dados:} Os dados e o conhecimento disponíveis no ambiente poderiam ser considerados como \textit{completos} ou \textit{incompletos}. Se estiverem \textit{completos}, não seria necessário conhecimento adicional para a resolução do problema. Se estiverem \textit{incompletos}, como no caso de grandes volumes de dados onde é exigida uma sumarização mais detalhada, seria necessário determinar heurísticas para encontrar um conjunto de dados mais específico para a resolução do problema.
\item \textbf{Exatidão do conhecimento e dos dados:} Os dados e o conhecimento disponíveis no ambiente poderiam ser considerados como exatos ou inexatos. Se estiverem exatos, poderiam ser representados em uma forma numérica ou lógica, como datas e coordenadas. Se estiverem inexatos, o número de casos possíveis seria infinito e seria impossível enumerar ou representar todos eles, sendo necessário determinar heurísticas para definir um número finito de possibilidades ou redefinir o significado de exatidão para que o que está disponível possa ser tratado como exato, como o reconhecimento de um objeto em uma imagem ou a definição do menor custo em uma determinada rota.
\item \textbf{Conhecimento sobre o objetivo e especificações do problema:} Um objetivo de um problema pode ser bem ou mal definido. Um objetivo bem definido podia ser medido e representado em termos de parâmetros para que seja possível comparar a qualidade de uma solução com outra, enquanto um objetivo mal definido envolvia parâmetros que não podem ser mensurados ou não poder ser medido, sendo impossível comparar a qualidade de soluções alternativas e necessitando de tratamentos adicionais para que o objetivo deixe de ser mal definido e passe a ser bem definido. 
\end{itemize}

Dado estas categorias, é possível pensar em soluções que conversam com os objetivos da Engenharia de Software: Abordar diversos tipos de conceitos; como teoria, projeto, desenvolvimento, avaliação e manutenção de novos dados e técnicas, metodologias e sistemas de gerenciamento de conhecimento; em prol do desenvolvimento e manutenção de sistemas e soluções com qualidade e com o custo mais viável possível. Estes sistemas podem ter questões relacionadas ao cumprimento desses objetivos incluindo estudos sobre os aspectos teóricos, ferramentas e metodologias de projeto, tradeoffs de projeto, representação e programabilidade, algoritmos e controle, confiabilidade e tolerância a falhas e projetos usando tecnologias existentes e emergentes. É tarefa do engenheiro de dados elaborar processos que reflitam tais questões em um sistema com objetivos definidos.

\subsection{Ciência de Dados}

Ciência de dados como um conceito precedeu o \textit{Big Data}, sendo um conjunto de princípios fundamentais que apoiam e orientam a extração baseada em princípios de informação e conhecimento de dados~\cite{Raban_2020}. Essa definição enfatiza a estreita relação de ciência de dados com a mineração de dados e, consequentemente, com a engenharia de dados. O termo foi cunhado nos anos 60 para descrever uma nova profissão que daria suporte à compreensão e interpretação da grande quantidade de dados que estavam sendo acumulados na época~\cite{Foote_2021}. Na academia, o termo começou a ser utilizado de modo mais formal a partir de 2001, quando os títulos dos artigos científicos começaram a usar, embora artigos já estavam focando em ciência de dados e usando o termo desde os anos 60~\cite{Raban_2020}~\cite{Foote_2021}. 

A estatística e o uso de modelos estatísticos estão profundamente enraizados no campo da ciência de dados, pois começou com estatísticas e evoluiu para incluir conceitos e práticas principalmente para aplicações de IA. À medida que mais e mais dados se tornam disponíveis, por meio de comportamentos e tendências registradas em softwares espalhados pela Internet ou mesmo por aplicações empresariais, as empresas os coletam e armazenam em quantidades cada vez maiores. Uma vez que as portas foram abertas por empresas que buscam aumentar os lucros e impulsionar melhores tomadas de decisão, seu uso, em conjunto com \textit{Big Data}, começou a ser aplicado a outros campos, como medicina, engenharia e ciências sociais.

Um cientista de dados funcional, ao contrário de um estatístico geral, tem compreensão da arquitetura de software e entende várias linguagens de programação. O cientista de dados define o problema, identifica as principais fontes de informação e projeta a estrutura para coletar e rastrear os dados necessários. O software é normalmente responsável por coletar, processar e modelar os dados. Ele usa os princípios da ciência de dados e toda sua visão multidisciplinar para obter conhecimentos mais profundos. A ciência de dados continua a evoluir como uma disciplina que usa ciência da computação e metodologia estatística para fazer previsões úteis e obter insights em uma ampla gama de campos. Embora seja usada em áreas como astronomia e medicina, também é usada nos negócios para ajudar a tomar decisões mais inteligentes.

\subsection{O ciclo do dado: Semelhanças e diferenças entre Engenharia de Dados e Ciência de Dados}

Nos tempos atuais, empresas e instituições acadêmicas utilizam Engenharia de Dados e Ciência de Dados para otimizar as suas aplicações de IA. Embora esses dois termos se refiram a áreas distintas, com profissionais que possuem funções distintas, na prática é frequente que cientistas de dados também realizem funções de engenharia de dados e vice-versa por ambas se complementarem na criação de aplicações de IA. Um bom exemplo de como estas áreas se conversam está no \textit{paper} \textit{Concise Survey of Computer Methods}, de 1974~\cite{Panoply_2017}. Nele, Peter Naur detalha diversos métodos de processamento de dados e aplicações, o que poderia definir como um, mas a citação mais importante de seu \textit{paper} está justamente no termo ciência de dados. Também define que a utilidade de um dado e de seus processos deriva de sua aplicação no desenvolvimento e manutenção de modelos da realidade~\cite{Foote_2021}. Por isso, a existência de um processo que trate os dados é tão importante quanto a construção de um modelo através de seus dados. Ao entender como os dados devem ser usados, é possível criar processos que auxiliem a otimizar os resultados obtidos e criar modelos cada vez mais próximos da realidade.

A principal razão para ambas as áreas se conversarem está no ciclo de vida do dado, fundamental para entender as oportunidades e os desafios de aproveitar ao máximo os dados digitais. Como um ser vivo, os dados têm um ciclo de vida, desde o nascimento, passando por uma vida ativa até alguma forma de expiração, podendo ser "imortalizado" dependendo de sua importância. Também como um organismo vivo e inteligente, sobrevive em um ambiente que fornece suporte físico, contexto social e significado existencial~\cite{Berman_2018}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{images/data_life_cycle.jpg}
\caption {Ciclo de vida e ecossistema do dado~\cite{Berman_2018}.}
\label{fig:cicloDado}
\end{figure}

Este ciclo, ilustrado na Figura \ref{fig:cicloDado}, é dividido em 5 etapas:

\begin{itemize}
\item \textbf{Aquisição:} Refere-se a processos de criação e coleta de dados, através de experimentos, sensores, pesquisas, sistemas, simulações, entre outros processos
\item \textbf{Limpeza:} Refere-se a processos de limpeza, organização e filtragem dos dados adquiridos.
\item \textbf{Uso/Reuso:} Refere-se a aplicações que os dados podem ter para a aquisição de novos conhecimentos, como análise, mineração, modelagem, enriquecimento, sintetização de novos dados, visualização e tomadas de decisão.
\item \textbf{Publicação:} Refere-se ao compartilhamento destes dados após seu uso, podendo ter como meio alguma plataforma de compartilhamento, bases de dados ou artigos acadêmicos.
\item \textbf{Preservação/Destruição:} Refere-se a processos de armazenamento, curadoria e validação do dado após seu uso e publicação. No contexto de validação, se o mesmo ainda continua útil para o contexto em que foi coletado, podendo ser atualizado ou destruído em caso negativo. Também pode ser comprimido ou indexado caso espaço ou desempenho sejam considerados gargalos para os usuários que forem usar os dados publicados.
\end{itemize}

Como exemplo, é possível citar os dados para experimentos do Grande Colisor de Hádrons (LHC), representando colisões de partículas dentro de um túnel com 27 km de circunferência para testar as previsões de várias teorias da física de partículas e da física de alta energia~\cite{Berman_2018}. A maioria dos dados gerados é tecnicamente irrelevante e são descartados, mas isso não impede que uma enorme quantidade de dados seja influente e continue a ser analisada e preservada. Em 2012, dados sobre experimentos do LHC forneceram fortes evidências para o bóson de Higgs, apoiando a veracidade do Modelo Padrão da Física. Esta descoberta científica foi a "Descoberta do Ano" de 2012 da revista Science e o Prêmio Nobel de Física em 2013.

As estimativas são de que, em 2040, haverá de 10 exabytes a 100 exabytes de dados influentes produzidos pelo LHC. Os dados retidos do LHC são anotados, preparados para preservação e arquivados em mais de uma dúzia de locais físicos. O resultado desse processo é divulgado à comunidade para análise e uso em mais de 100 outros locais de pesquisa. Além do desenvolvimento de protocolos de administração, disseminação e uso de dados, o ecossistema de dados do LHC também fornece um modelo econômico que suporta de forma sustentável os dados e sua infraestrutura. Essa combinação entre administração dos dados e administração econômica permitem que os dados sejam mantidos.

O diagrama do ciclo de vida dos dados descrito na figura e o exemplo do LHC sugerem um conjunto contínuo de ações e transformações nos dados, mas em muitas comunidades científicas e disciplinas hoje essas etapas são isoladas. Os cientistas de domínio se concentram em gerar e usar dados. Cientistas da computação podem se concentrar em questões de plataforma e desempenho, incluindo mineração, organização, modelagem e visualização, bem como os mecanismos para extrair significado dos dados por meio de aprendizado de máquina e outras abordagens. Estatísticos podem se concentrar na matemática dos modelos de risco e inferência~\cite{Berman_2018}. Engenheiros de dados podem se concentrar na administração e preservação de dados gerados pelo cientista de domínio e no \textit{backend} do pipeline, seguindo a aquisição, decisões e ações no domínio da publicação, arquivamento e curadoria. Cientistas de dados podem unir o trabalho dos cientistas da computação e dos estatísticos para extrair novos conhecimentos e conhecer tomadas de decisão mais eficientes.


\section{Machine Learning}
\label{sec:MachineLearning}

Aprendizado de Máquina (\textit{Machine Learning}, em inglês) pode ser definido como “a prática de usar algoritmos para coletar dados, aprender com eles, e então fazer uma determinação ou predição sobre alguma coisa no mundo. Então, em vez de implementar as rotinas de software manualmente, com um gama específica de instruções para completar uma tarefa em particular, a máquina é `treinada` usando uma quantidade grande de dados e algoritmos que dão e ela a habilidade de aprender como executar a tarefa”~\cite{Copeland_2016}. Com isso, o computador consegue a habilidade de realizar determinado cálculo ou tarefa sem que necessite de programação adicional ou interferência humana para isso.

O Aprendizado de Máquina é fortemente relacionado com a Estatística, uma vez que seus métodos e parte de seus algoritmos, como regressões, tiveram como base modelos estatísticos e a análise de seus dados. As tarefas de aprendizado podem ser classificadas em três categorias básicas~\cite{MLWikipedia_2021}~\cite{MLSAS_2021}:

\begin{itemize}
\item \textbf{Aprendizado supervisionado}: O treinamento é realizado por meio de exemplos rotulados, como uma entrada na qual a saída desejada é conhecida. Através de métodos como classificação, regressão e \textit{gradient boosting}, o aprendizado supervisionado utiliza padrões para prever valores de rótulos em dados que não estão rotulados. O aprendizado supervisionado é comumente empregado em aplicações nas quais dados históricos preveem eventos futuros prováveis.
\item \textbf{Aprendizado não-supervisionado}: É utilizado em dados que não possuem rótulos históricos. A “resposta certa” não é informada ao sistema, o algoritmo deve descobrir o que está sendo mostrado. O objetivo é explorar os dados e encontrar alguma estrutura dentro deles. Técnicas populares incluem mapas auto-organizáveis, mapeamento por proximidade, agrupamento \textit{k-means} e decomposição em valores singulares. Esses algoritmos também são utilizados para segmentar tópicos de texto, recomendar itens e identificar pontos discrepantes nos dados.
\item \textbf{Aprendizado por reforço}: O algoritmo descobre através de testes do tipo “tentativa e erro” quais ações rendem as maiores recompensas. Este tipo de aprendizado possui três componentes principais: o agente (o aprendiz ou tomador de decisão), o ambiente (tudo com que o agente interage) e ações (o que o agente pode fazer). O objetivo é que o agente escolha ações que maximizem a recompensa esperada em um período de tempo determinado. O agente atingirá o objetivo muito mais rápido se seguir uma boa política, então o foco do aprendizado por reforço é descobrir a melhor política.
\end{itemize}

O termo se tornou muito mais evidente com a possibilidade da implementação do \textit{Deep Learning}, que é uma técnica que utiliza Redes Neurais Artificiais para atingir seus resultados. Redes Neurais Artificiais são modelos computacionais inspirados no sistema nervoso do cérebro, onde temos neurônios divididos em camadas e conectados entre si, podendo ser abstraído conforme ilustração na Figura \ref{fig:NeuralNetwork}. Dependendo da tarefa a ser realizada, cada neurônio atribui uma ponderação para os dados que entram e a saída final é determinada pelo total dessas ponderações~\cite{Copeland_2016}. As redes neurais utilizadas em \textit{Deep Learning} possuem, ao menos, duas camadas de neurônios entre a camada que recebe os dados de entrada e a camada final que faz o tratamento final dos dados de saída.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{images/deep_neural_network.jpg}
\caption {Exemplo de uma rede neural utilizada em \textit{Deep Learning}.}
\label{fig:NeuralNetwork}
\end{figure}

Com a evolução da computação, o treino de uma tarefa passou a ser cada vez mais viável, uma vez que a execução de algoritmos de Aprendizado de Máquina é computacionalmente muito custosa, especialmente quando redes neurais são utilizadas. Sua viabilidade também se prova quando tais tarefas se tornam efetivas no mundo real: Como exemplo, reconhecimento de imagens por máquinas treinadas através de deep learning em alguns cenários possuem uma taxa de acerto maior que a de humanos~\cite{Copeland_2016}.

\subsection{Métricas de Avaliação}
\label{sec:EvaluationMetrics}

Tradicionalmente em um problema de classificação binária, uma previsão do classificador pode ter 4 tipos de resultados em uma matriz de confusão, presente na Tabela \ref{tbl:ConfusionMatrix}. Os Verdadeiros Positivos (VP, ou TP pelo termo em inglês \textit{True Positives}) e Verdadeiros Negativos (VN, ou TN pelo termo em inglês \textit{True Negatives}) são classificações corretamente previstas pelo classificador. Um Falso Positivo (FP) ocorre quando dados previstos para estar na classe positiva pertencem à classe negativa em seu resultado real, e um Falso Negativo (FN) ocorre quando dados previstos para estar na classe negativa pertencem à classe positiva em seu resultado real.

\begin{table}[h]
\label{tbl:ConfusionMatrix}
\begin{center}
  \caption{Matriz de confusão}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{\textbf{Classe prevista}}\\
    \hline
    \multirow{3}{*}[-3ex]{\rotatebox[origin=c]{90}{\textbf{Classe atual}}} &  & \textbf{Positiva} & \textbf{Negativa}\\[3ex]
    \cline{2-4}
     & \textbf{Positiva} & \makecell{Verdadeiros Positivos\\(VP/TP)} & \makecell{Falsos Positivos\\(FP)}\\[3ex]
    \cline{2-4}
     & \textbf{Negativa} & \makecell{Falso Negativo\\(FN)} & \makecell{Verdadeiros Negativos\\(VN/TN)}\\[3ex]
    \hline
  \end{tabular}}
\end{center}
\end{table}

A taxa de sucesso, também conhecida como \textbf{acurácia}, é o número de previsões corretas dividido pelo número total de resultados:

\begin{equation}
Acc = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

Ela é considerada um indicador da realização de um bom treinamento e de um bom funcionamento do modelo obtido. Entretanto, não é a melhor métrica para interpretar situações onde a qualidade dos dados utilizados interfere no resultado final, como classes desequilibradas.

A \textbf{precisão} enfatiza situações quando o número de falsos positivos é alto e mostra a precisão das previsões positivas. É a fração de casos positivos previstos corretamente para estar na classe positiva de todos os casos positivos previstos no modelo:

\begin{equation}
P = \frac{TP}{TP + FP}
\end{equation}

O \textbf{recall}, ou sensibilidade, enfatiza situações quando o número de falsos negativos é alto.  É a fração de casos positivos previstos corretamente para estar na classe positiva de todos os casos positivos reais:

\begin{equation}
R = \frac{TP}{TP + FN}
\end{equation}

O \textbf{F1-score} é um caso especial de F-score, uma medida geral da acurácia de um modelo que combina precisão e recall. Para tal, usa-se a média harmônica entre ambos para realizar a medição:

\begin{equation}
F1 = 2 \times\frac{P \times R}{P + R}
\end{equation}

O \textbf{AUC} (\textit{Area under the ROC Curve}), também chamada de precisão balanceada, pode interpretar a capacidade do classificador de evitar falsos positivos e falsos negativos. Se sai melhor que a acurácia em situações que ela não é apropriada, como o desequilíbrio entre classes já citado anteriormente:

\begin{equation}
AUC = \frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right)
\end{equation}

\subsection{Tipos de Algoritmos de Machine Learning}
\label{sec:MLAlgorithms}

Existem diversos algoritmos de \textit{Machine Learning} utilizados para resolver os problemas de aprendizado supervisionado, não-supervisionado e por reforço. Nesta seção serão abordados apenas os algoritmos utilizados por este trabalho de Mestrado.

\subsubsection{Regressão Logística}

A Regressão Logística é um método de classificação baseado na aplicação da técnica de Regressão Linear. Como na Regressão Linear, é assumida uma relação linear entre os recursos e calcula a soma ponderada dos recursos mais um termo de viés. Neste método, o resultado não é utilizado diretamente, mas calcula a logística do resultado. Quando peso das \textit{features} é $w$ e o termo de viés é $b$, a probabilidade estimada do modelo de regressão logística é a seguinte:

\begin{equation}
\hat{p} = \sigma (w^{T}x + b)
\end{equation}

Logo após, a função logística $\sigma$ é calculada:

\begin{equation}
\sigma (z) = \frac{1}{1 + exp(-z)}
\end{equation}

Uma vez que o modelo de Regressão Logística estimou a probabilidade $\hat{p}$, a previsão de classificação é feita considerando a seguinte regra:

\begin{equation}
	\begin{aligned}
	\hat{p} = 
	\begin{cases}
	1 & \text{Se $\hat{p} \geqslant 0,5$}\\
	0 & \text{Se $\hat{p} < 0,5$}\\
	\end{cases}
	\end{aligned}
\end{equation}

O objetivo do treinamento de um modelo de classificação é de minimizar a diferença entre o resultado real $y$ e o resultado previsto $\hat{y}$ para as $m$ amostras presentes para treinamento. Para medir o quão próximo a função resultante do treinamento se aproxima do conjunto de dados, a seguinte função de custo é calculada conforme a equação abaixo. A função $L(\hat{y}, y)$ utilizada pode variar, tendo como exemplos a função por entropia cruzada ou a função por diferença de quadrados.

\begin{equation}
J(w,b) = \frac{1}{m} \sum^{m}_{i=1} L(\hat{y}^{(i)}, y^{(i)})
\end{equation}

Durante o treinamento do modelo de Regressão Logística, é preciso encontrar os parâmetros $w$ e $b$ que minimizem a função de custos globais. Como a função de custo é convexa, diferentes métodos de otimização, como o gradiente descendente, garantem encontrar o mínimo global.

Para lidar com problemas com múltiplas classes para classificação, é possível calcular uma função de custo separada para cada rótulo de classe por observação e somar os resultados, técnica conhecida como \textit{One-Vs-All}~\cite{Rifkin_2004}. Outra técnica que pode generalizar o método de regressão logística para suportar várias classes diretamente é chamada de Regressão Softmax, ou Regressão Logística Multinomial, utilizando a função Softmax para substituir a função de probabilidade da Regressão Logística convencional:

\begin{equation}
\hat{p}(Y_i = n) = \frac{e^{\beta_{n} \cdot \textbf{X}_{i}}}{1 + \sum^{K-1}_{k=1} e^{\beta_{k} \cdot \textbf{X}_{i}}}
\end{equation}

\subsubsection{\textit{Support Vector Machines} (SVM)}

\textit{Support Vector Machines} (SVM) é um algoritmo de aprendizado de máquina que pode ser usado para detecção linear, não linear, de classificação, regressão e até mesmo detecção de anomalias (\textit{Outliers}). A ideia fundamental por trás dessa técnica é que as classes de saída são separadas com um hiperplano maximizando a margem (distância máxima entre os pontos de dados de ambas as classes)~\cite{Steinwart_2008}. As \textit{Support Vector Machines} criam um ou vários hiperplanos em um espaço de $n$ dimensões. A dimensão dos hiperplanos depende do número de \textit{features}. Quando há duas \textit{features}, é apenas uma linha, ou quando há três \textit{features}, é um plano bidimensional.

Para lidar com conjuntos de dados não lineares, uma abordagem é adicionar mais \textit{features}, como um recurso polinomial, ou a outra abordagem é usar \textit{kernels}, mapeando os dados de entrada de $n$ dimensões para um espaço de dimensão superior, onde os dados podem ser separados linearmente.

\subsubsection{Regressão Kernel Ridge}

A Regressão Kernel Ridge (\textit{Kernel Ridge Regression}/KRR) é um algoritmo de aprendizado de máquina proveniente da combinação de duas operações: A Regressão Ridge com o que é conhecido como "truque do kernel"~\cite{Witten_2016}. A Regressão Ridge substitui a função de custo tradicional por uma com um termo de penalização incluído, conforme ilustrado na Equação \ref{eqn:kernelLeastSquares}, utilizando o método dos mínimos quadrados. O "truque do kernel" é uma técnica mátemática que permite exemplificar problemas não-lineares de forma linear utilizando kernels, reduzindo a complexidade para uma simples operação matricial.

\begin{equation}
\label{eqn:kernelLeastSquares}
\sum_{i} (y_{i} - w^{T}x_i)^{2} - \lambda \norm{w}^{2}
\end{equation}

Por causa de tais operações, A Regressão Kernel Ridge exige mais processamento do que uma regressão tradicional. No entanto, é vantajoso usá-la em casos que um ajuste não-linear é desejado ou onde há mais atributos do que elementos no conjunto de treinamento. Em casos onde há mais elementos no conjunto de treinamento do que atributos, a Regressão Kernel Ridge peca por não abranger o conceito utilizado nas \textit{Support Vector Machines}, onde é necessário somar apenas o conjunto de vetores de suporte ao invés de todo o conjunto de treinamento. No entanto, por causa deste mesmo conceito as \textit{Support Vector Machines} exigem mais processamento.

\subsubsection{Árvores de Decisão (\textit{Decision Trees})}

Árvores de decisão (\textit{Decision Trees}) são um grupo de algoritmos de aprendizado de máquina que podem ser usados para classificação e regressão. Eles são os métodos de aprendizado mais comuns que são muito poderosos e capazes de ajustar conjuntos de dados complexos. Os algoritmos de Árvore de Decisão são baseados em uma abordagem de dividir e conquistar para os problemas de classificação~\cite{Witten_2016}. Uma Árvore de Decisão é feita pelo processo contínuo de dividir o conjunto de dados nos atributos da melhor maneira possível em diferentes classes até que um critério de parada específico seja alcançado. Nas Árvores de Decisão, as observações sobre os itens são mostradas em ramificações e as conclusões das observações são mostradas nos nós. Existem três tipos diferentes de nós: os nós raiz que indicam o início do processo de decisão e não têm arestas de entrada, os nós internos que têm exatamente uma entrada e pelo menos duas arestas de saída e os nós finais (ou as folhas).

Sempre que o rótulo de classificação de destino assume valores discretos, a Árvore de Decisão é chamada de árvore de classificação e, sempre que recebe valores contínuos, é chamada de árvore de regressão. Um dos méritos do uso de Árvores de Decisão é que elas exigem pouco pré-processamento de dados e não há necessidade de escala. Seu treinamento geralmente é realizado com uma árvore menos complicada e mais abrangente. A complexidade de uma Árvore de Decisão pode ser controlada usando critérios de parada e métodos de poda~\cite{Rokach_2005}, existindo quatro métricas diferentes para poder medí-la: o número total de nós, o número total de folhas, a profundidade da árvore e o número de atributos usados.

Os algoritmos usados para construir uma Árvore de Decisão a partir de um conjunto de dados são chamados de indutores. Normalmente, o objetivo desses algoritmos é encontrar a Árvore de Decisão ótima minimizando o erro de generalização, considerando o número mínimo de nós e a profundidade mínima da árvore. Os algoritmos da Árvore de Decisão funcionam de forma \textit{top-down}, pois escolhem a melhor variável em cada estágio que pode dividir o conjunto de dados em um atributo específico. Diferentes indutores usam critérios diferentes para encontrar a melhor variável.

\subsubsection{\textit{Random Forest}}

\textit{Random Forest} é um algoritmo de aprendizado de máquina que combina a simplicidade das Árvores de Decisão com a flexibilidade, resultando em melhorias na acurácia~\cite{Breiman_2001}. Neste algoritmo, o \textit{Bagging}, uma técnica que gera uma coleção de classificadores introduzindo subconjuntos randômicos na entrada do algoritmo~\cite{Witten_2016}, é utilizado para gerar uma coleção de Árvores de Decisão simplificadas com a finalidade de generalizar o resultado no conjunto da obra.

No final, a classe selecionada é a que for mais citada dentre essa coleção. Esta simplificação, em geral, otimiza o treinamento em relação a uma Árvore de Decisão tradicional.

\subsubsection{\textit{Gradient Boosting}}

\textit{Gradient Boosting}, também conhecido como \textit{Gradient Boosting Machine} (GBM) ou \textit{Gradient Boosted Regression Tree} (GBRT)~\cite{Chen_2016}, é um algoritmo de aprendizado de máquina que faz a classificação através da composição de pequenos modelos pela utilização do \textit{Boosting}~\cite{Friedman_2000}~\cite{Hastie_2009}. \textit{Gradient Boosting} faz uso de \textit{Boosting} para gradualmente aproximar um melhor modelo, de modo a somar submodelos ao modelo composto. Árvores de Decisão tendem a gerar \textit{overfitting}, e o \textit{Gradient Boosting} é uma possibilidade para solucionar este problema.

\textit{Boosting} é uma combinação de modelos simples, chamados de modelos fracos, onde tipicamente estes modelos são Árvores de Decisão. O algoritmo combina classificadores fracos com intuito de produzir um classificador forte. Diferente do \textit{Bagging}, a obtenção de subconjuntos vindos do conjunto de dados não é feita de maneira aleatória, e sim feita priorizando a substituição de dados mal classificados, que ganham um peso maior para serem substituídos nas iterações seguintes~\cite{Hastie_2009}.

\subsubsection{Redes Neurais Artificiais}

As Redes Neurais Artificiais, muitas vezes chamadas apenas de Redes Neurais, são um grupo de métodos de modelagem de dados estatísticos não lineares, que a princípio foram inspirados nos cérebros e nas estruturas dos neurônios biológicos. Elas são a base do aprendizado profundo, sendo escaláveis e capazes de trabalhar com grandes tarefas complexas de aprendizado de máquina, como serviços de reconhecimento de imagem e reconhecimento de fala. O processo simplificado para treinamento de uma Rede Neural é o seguinte:

\begin{enumerate}[label=\textbf{\arabic*.}]
\item Os dados de entrada são fornecidos à rede, eles se propagam pelas camadas e o processo de encaminhamento produz a previsão.
\item Calcula-se o erro entre o produto previsto e o produto real (função de custo).
\item A Rede Neural usa um algoritmo de otimização para ajustar os pesos de forma a reduzir a função custo.
\item O processo de encaminhamento inicia novamente e continua até que a taxa de erro seja minimizada a um determinado valor, ou até um certo número de iterações.
\end{enumerate}

As Redes Neurais Artificiais não são novas para os sistemas de computação, pois foram introduzidas pela primeira vez por Warren McCulloch e Walter Pitts em 1943~\cite{McCulloch_1943}. Desde então, o desenvolvimento dessas técnicas passou por altos e baixos até ter uma adoção considerável graças aos avanços significativos tanto no poder computacional dos hardwares quanto nas otimizações de algoritmos implementadas nos softwares. Atualmente, é possível treinar Redes Neurais de grande complexidade, e há uma enorme quantidade de dados disponíveis para uso em bancos de dados. Elas podem ser divididas em dois grupos principais~\cite{Singh_2009}:

\begin{itemize}
\item \textbf{Redes retroalimentadas (\textit{Feedforward})}: Nessas redes, o fluxo de dados se move apenas na direção direta da camada de entrada para os nós de saída. Não há alimentação ou loop no sistema.
\item \textbf{Redes recorrentes}: Este tipo de rede pode ter a opção de feedback e reutiliza os dados dos estágios posteriores para os estágios anteriores.
\end{itemize}

Algoritmos de otimização são métodos usados para minimizar o valor da função de custo ajustando os parâmetros internos do modelo. Algumas das técnicas de otimização mais comuns nas estruturas de aprendizado profundo são as seguintes: \textit{Stochastic Gradient Descent} (SGD)~\cite{Schmidt_2013}, \textit{Momentum}~\cite{Polyak_1964}, \textit{Nesterov Accelerated Gradient} (NAG)~\cite{Sutskever_2013}, \textit{Adaptive Gradient} (AdaGrad)~\cite{Duchi_2011}, \textit{Root mean square prop} (RMSprop)~\cite{Graves_2013}, \textit{Adaptive moment estimation}, (Adam)~\cite{Kingma_2014}, \textit{Nesterov and Adam optimizer} (Nadam)~\cite{Dozat_2016}.

Toda Rede Neural consiste em alguns nós (neurônios), conexões ponderadas entre os nós e uma abordagem computacional chamada função de ativação usada para definir a saída de cada neurônio. Diferentes tipos de funções de ativação podem ser usados com esta técnica, como sigmóide, tanh (tangente hiperbólica) ou ReLU (sigla de \textit{Rectified Linear Unit}).

Redes Neurais consistem em diferentes camadas. O tipo mais simples de Rede Neural inclui uma camada de entrada que recebe informações de fontes externas, como valores de atributos do conjunto de dados de entrada. A camada de saída gera a saída da rede e as camadas ocultas conectam a camada de entrada e a camada de saída entre si. O valor de entrada de cada nó em cada camada é calculado pela soma de todos os nós de entrada multiplicada pelo respectivo peso da interconexão entre os nós~\cite{Erb_1993}.

\section{\textit{Fairness} em Algoritmos de Aprendizado de Máquina}
\label{sec:Fairness}

Como a coleta de dados está presente atualmente no dia-a-dia de variados setores da sociedade, o uso de Aprendizado de Máquina é extremamente versátil para tomadas de decisão, podendo ser utilizados em problemas como admissão de universidades, contratações, análise de crédito e reconhecimento de doenças. Com o aumento dessa influência, o uso de dados sensíveis em um contexto determinado também aumentou, e temas como uma IA ética e conceitos como vieses nos dados e \textit{Fairness} passaram a serem discutidas não apenas na Computação, mas em áreas como Direito. Algoritmos são mais objetivos, rápidos e são capazes de considerar uma grande magnitude de recursos que pessoas não são capazes. Entretanto, até o presente momento eles não são capazes de diferenciar contextos sociais, onde um resultado mais eficiente de acordo com os dados disponíveis podem amplificar as desigualdades sociais e tomar decisões de modo injusto~\cite{Mehrabi_2021}. 

Estes dados sensíveis, tendo como exemplos cor de pele, raça, sexo, idade e altura, são considerados atributos protegidos, que precisam ser classificados e processados antes da execução de um algoritmo de Aprendizado de Máquina, determinarão como o algoritmo se comportará e, consequentemente, afetará suas métricas~\cite{Mougan_2022}. Os grupos de dados provenientes destes atributos protegidos são considerados grupos protegidos, que podem ser divididos em dois grupos: o grupo privilegiado, que possui vantagens no contexto do problema, e o grupo não-privilegiado, que possui desvantagens no contexto do problema e, portanto, sujeito a discriminação.

É possível descrever o conceito de \textit{Fairness} no contexto de aprendizagem supervisionada, onde um modelo $f$ pode prever um conjunto de resultados $y$ a partir de um conjunto de \textit{features} $x$, evitando discriminação injusta em relação a um atributo protegido $a$. É permitido, mas não exigido, que $a$ seja um componente de $x$~\cite{Begley_2021}. Em outras palavras, um modelo de ML considerado justo é aquele onde a correlação de seu resultado é baixa em relação a dados de entrada considerados como sensíveis a discriminações.

Geralmente, as descrições de justiça se dividem em dois grupos principais: justiça individual e justiça de grupo. O objetivo da justiça individual é que indivíduos semelhantes devem obter resultados semelhantes, enquanto na justiça de grupo, cada um dos grupos definidos pelo atributo protegido devem ser tratados igualmente. No geral, os estudos atuais costumam realizar seus experimentos em casos de justiça de grupo, uma vez que o escopo de justiça de grupo é muito mais amplo e tende a exemplificar melhor a relação entre dados, relações sociais e vieses do mundo atual.

\subsection{Métricas de \textit{Fairness}}
\label{sec:FairnessMetrics}

Para avaliar a justiça de um modelo, as métricas utilizadas diferem das métricas utilizadas para avaliação do modelo, que possuem o propósito de verificar se um modelo tem previsões confiáveis ou não. As métricas de \textit{Fairness} possuem um propósito diferente, pois verificam os dados de forma mais intimista. Elas não medem o modelo como um todo, mas o quanto os grupos e registros avaliados estão próximos dos outros. Enquanto as métricas mais tradicionais avaliam a performance do modelo e seus dados como um todo e seus resultados gerais, as métricas de \textit{Fairness} avaliam se os resultados gerais também se refletem em grupos específicos, para verificar se não há disparidade ou discriminação nos resultados propostos.

Assim como algumas métricas utilizadas para avaliação, muitas métricas utilizam Verdadeiros Positivos, Verdadeiros Negativos, Falsos positivos e Falsos Negativos para analisar o quão justo o modelo é. Entretanto, diferente da acurácia, precisão e recall utilizados anteriormente, a medição das discriminações utiliza outras métricas, utilizadas ou não para avaliar a performance, para estabelecer novas métricas mais adequadas para a sua finalidade.

Exemplos de métricas utilizadas para isso são a Taxa de Verdadeiros Positivos e a Taxa de Falsos Positivos. Enquanto a \textbf{Taxa de Verdadeiros Positivos} (ou TPR, pelo termo em inglês \textbf{True Positive Rate}) é outro termo para denominar o recall, a \textbf{Taxa de Falsos Positivos} (ou FPR, pelo termo em inglês \textbf{False Positive Rate}) é a fração de casos negativos previstos incorretamente como estando na classe positiva de todos os casos positivos reais:

\begin{equation}
FPR = \frac{FP}{FP + TN}
\end{equation}

Dada essas métricas iniciais, considerando $Y=1$ a classe positiva, $Z=0$ o grupo não-privilegiado e $Z=1$ o grupo privilegiado, algumas das definições de \textit{Fairness} mais usadas são as seguintes:

\begin{itemize}
\item \textbf{Diferença de paridade estatística (\textit{Statistical parity difference}), ou discriminação~\cite{Zemel_2013}:} Esta métrica determina a diferença de chances entre o grupo privilegiado e o grupo não-privilegiado de receber uma previsão favorável. É baseada na seguinte fórmula:

\begin{equation}
Pr(Y=1|Z=0)-Pr(Y=1|Z=1)
\end{equation}
 
Aqui, o viés ou paridade estatística é a diferença entre a probabilidade de que um indivíduo aleatório retirado dos não-privilegiados seja rotulado como 1 e a probabilidade de que um indivíduo aleatório dos privilegiados seja rotulado como 1. Portanto, um valor próximo de 0 é considerado justo.

\item \textbf{Diferença de oportunidade igual (\textit{Equal opportunity difference})~\cite{Biswas_2020}:} Esta métrica determina determina a diferença de chances entre o grupo privilegiado e o grupo não-privilegiado de receber uma previsão favorável que corresponda a realidade. É a diferença entre a taxa de verdadeiros positivos do grupo não-privilegiado e a taxa de verdadeiros positivos do grupo privilegiado:

\begin{equation}
TPR_{Z=0} - TPR_{Z=1}
\end{equation}
 
Um valor próximo de 0 é considerado justo. Um classificador binário satisfaz a igualdade de oportunidades quando a taxa de verdadeiros positivos de ambos os grupos são iguais~\cite{Hardt_2016}

\item \textbf{Diferença de probabilidade média (\textit{Average odds difference})~\cite{Biswas_2020}:} Esta métrica determina a diferença de previsões incorretas entre o grupo privilegiado e o grupo não-privilegiado. Ela usa a taxa de falsos positivos e a taxa de verdadeiros positivos para calcular a tendência, calculando a igualdade de probabilidades com a fórmula:

\begin{equation}
\frac{1}{2}(|FPR_{Z=0} - FPR_{Z=1}|+|TPR_{Z=0} - TPR_{Z=1}|)
\end{equation}
 
O valor precisa ser próximo a 0 para ser considerado justo.

\item \textbf{Impacto de disparidade (\textit{Disparate impact})~\cite{Biswas_2020}:} Para esta métrica, é usada a seguinte fórmula:

\begin{align*}
\frac{Pr(Y=1|Z=0)}{Pr(Y=1|Z=1)}
\end{align*}

Usa as probabilidades condicionais utilizadas na diferença de paridade estatística, mas como é calculada como uma proporção o valor esta métrica possui um comportamento não-linear. Desta forma, um valor próximo de 1 é considerado justo. Um valor que não esteja no intervalo $\left[0,8; 1,25 \right]$ indica um grande desbalanceamento do modelo treinado~\cite{Feldman_2015}

\item \textbf{Índice de Theil (\textit{Theil index})~\cite{Speicher_2018}:} Esta métrica é utilizada não apenas para determinar \textit{Fairness} entre o grupo privilegiado e o grupo não-privilegiado, mas também para determinar \textit{Fairness} entre indivíduos. Em outras palavras, que indivíduos similares recebam previsões similares. Também é conhecida como índice de entropia generalizado, mas com o valor de $\alpha$ usado neste índice igual a 1~\cite{Speicher_2018}. É calculado com a seguinte fórmula:

\begin{align*}
\frac{1}{n}\sum^{n}_{i=0}\frac{b_i}{\mu}\ln{\frac{b_i}{\mu}}
\end{align*}

Onde n é a quantidade de elementos, $b_i$ é obtido com a fórmula $b_i = \hat{y}_i - y_i + 1$ e $\mu$ é a média de todos os elementos $b_i$. Nesta fórmula, $y_i$ é o conjunto de saídas, $\hat{y}_i$ é o conjunto de previsões dadas pelo modelo. Também precisa ser próximo a 0 para ser considerado justo.

\end{itemize}

\subsection{Algoritmos para redução de vieses}
\label{sec:FairnessAlgorithms}

Há diversos tipos de algoritmos diferentes na inteligência artificial para redução de vieses, a fim de garantir \textit{Fairness} nos projetos de Aprendizado de Máquina. É possível classificá-los em três categorias diferentes: Algoritmos de pré-processamento, de processamento e de pós-processamento.

Os algoritmos de \textbf{pré-processamento} tentam eliminar a discriminação transformando os dados, antes de executar o algoritmo de treinamento. Tais algoritmos podem ser usados caso seja permitida a modificação dos dados de treinamento~\cite{dAlessandro_2017}, e a ideia por trás de tais algoritmos é que suas previsões serão mais balanceadas se o classificador for treinado com os dados já balanceados~\cite{Kamiran_2009}. Nesta categoria se enquadram os seguintes algoritmos:

\begin{itemize}
\item \textbf{Reposição (\textit{Reweighing})~\cite{Kamiran_2011}:} Pondera os exemplos em cada combinação de grupo e rótulo de maneira diferente para garantir a justiça antes da classificação.

\item \textbf{Removedor de impacto de disparidade (\textit{Disparate impact remover})~\cite{Feldman_2015}:} Edita valores de \textit{features} para otimizar \textit{Fairness} em cada grupo enquanto preserva a ordem de classificação dentro dos mesmos.

\item \textbf{Aprendizado de representações justas (LFR, ou \textit{Learning fair representations})~\cite{Zemel_2013}:} Encontra uma representação latente que codifica os dados, mas ofusca informações dos atributos protegidos.

\item \textbf{Pré-processamento otimizado (\textit{Optimized pre-processing})~\cite{Calmon_2017}:} Aprende uma transformação probabilística que edita \textit{features} e rótulos nos dados abordando primariamente \textit{Fairness} em cada grupo, e garante a fidelidade de dados através de restrições para controle de distorção.
\end{itemize}

Os algoritmos de \textbf{processamento} tentam realizar modificações nos algoritmos de treinamento para mitigar a discriminação durante o processo de treinamento do modelo. Se for permitido fazer mudanças no processo de treinamento, então os algoritmos podem ser usados incorporando mudanças na função de custo ou impondo restrições~\cite{Mehrabi_2021}. Nesta categoria se enquadram os seguintes algoritmos:

\begin{itemize}
\item \textbf{Remoção de viés adversário (Adversarial debiasing)~\cite{Zhang_2018}:} Aprende um classificador que maximiza a precisão e reduz a capacidade de um adversário de depender do atributo protegido nas previsões. Essa abordagem leva a um classificador justo, pois as previsões realizadas pelo classificador não possuem nenhuma informação de discriminação nos grupos.

\item \textbf{Removedor de preconceito (Prejudice remover)~\cite{Feldman_2015}:} Técnica que adiciona no algoritmo escolhido um termo de regularização baseado na discriminação.

\item \textbf{Meta-Algoritmo para classificações justas (Meta-Algorithm for Fair Classification)~\cite{Celis_2019}:} Aprende um classificador compatível com uma gama grande de métricas de Fairness, sendo prático o suficiente para abrangê-las sem grande perda de performance.

\item \textbf{Justiça de Subgrupo Diversificado (Rich Subgroup Fairness)~\cite{Kearns_2018}:} Aprende um classificador que procura equalizar as taxas de falsos positivos e falsos negativos entre os dados que envolvem atributos protegidos, considerados como subgrupos.

\item \textbf{Redução por Gradiente Exponencial (Exponentiated Gradient Reduction)~\cite{Agarwal_2018}:} Aprende um classificador baseado em Gradiente Exponencial que tende a minimizar o erro de uma classificação ponderada.

\item \textbf{Redução por busca em grade (Grid Search Reduction)~\cite{Agarwal_2018}~\cite{Agarwal_2019}:} Aprende um classificador baseado na busca em uma grade de valores que tende a minimizar o erro de uma classificação ponderada. É mais simples e impreciso que a Redução por Gradiente Exponencial, mas sua escolha pode ser razoável se a quantidade de métricas de Fairness a serem consideradas for pequena.

\end{itemize}

Os algoritmos de \textbf{pós-processamento} utilizam um conjunto de validação, que não foi envolvido no processo de treinamento para melhorar a imparcialidade das previsões~\cite{dAlessandro_2017}. Quando não há possibilidade de fazer alterações nos dados de treinamento ou no treinamento do modelo, apenas algoritmos de pós-processamento podem ser usados. Nesta categoria se enquadram os seguintes algoritmos:

\begin{itemize}
\item \textbf{Pós-processamento para igualdade de probabilidade (Equalized Odds Postprocessing)~\cite{Hardt_2016}:} Resolve um problema linear cuja saída é um conjunto de probabilidades e altera os rótulos de saída entre o grupo privilegiado e o grupo não-privilegiado para equalizar estas probabilidades.

\item \textbf{Pós-processamento calibrado para igualdade de probabilidade (Calibrated Equalized Odds Postprocessing)~\cite{Pleiss_2017}:} Otimiza um conjunto de probabilidades através de uma calibração das previsões do classificador obtido e altera os rótulos de saída entre o grupo privilegiado e o grupo não-privilegiado para equalizar estas probabilidades.

\item \textbf{Classificação baseada em Rejeição de Opções (Reject Option-based Classification)~\cite{Kamiran_2012}:} Dá resultados favoráveis para grupos não privilegiados e resultados desfavoráveis para grupos privilegiados de acordo com uma faixa de confiança.

\end{itemize}

\section{Engenharia de Software para Aplicações de ML}
\label{sec:EngSoftware}

Quando se fala de Arquitetura e Engenharia de Software, se fala da definição dos componentes de software, suas propriedades externas, e seus relacionamentos com outros softwares para fazer com que um sistema seja documentável, reusável e testável. A preocupação está em como um sistema deve ser organizado e com a estrutura geral desse sistema. Dado as definições sobre IA já detalhadas, é possível encaixar Aprendizado de Máquina na forma de um processo bem definido, de forma que é possível sistematizar todo esse processo na forma de componentes e definir formas em que o modelo resultante deste processo é disponibilizado para aplicações externas.

No processo, ilustrado na Figura \ref{fig:MLProcess}, o conjunto de dados passa por um pré-processamento e é dividido em dois conjuntos, um para treinamento e outro para teste. O conjunto de treino é utilizado para o algoritmo realizar o processo de treinamento, obtendo um modelo após o término desse processo. O conjunto de testes é utilizado para mensurar se o modelo obtido no processo de treinamento realiza previsões confiáveis ou não.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{images/ML_Process.jpg}
\caption {Processo padrão para aprendizado de máquina}
\label{fig:MLProcess}
\end{figure}

\subsection{Arquitetura de Software}

O consenso sobre a definição do termo arquitetura de software foi atingido com a adoção do padrão IEEE 1471, que define arquitetura de software como \textit{"a organização fundamental de um sistema incorporado em seus componentes, seus relacionamentos entre si e com o meio ambiente e os princípios que orientam seu projeto e evolução"}. Com esta definição, o componente e o conector são reforçados como conceitos centrais da arquitetura de software~\cite{Bosch_2004}.

O nível de projeto da arquitetura de software no desenvolvimento de uma aplicação vai além dos algoritmos e estruturas de dados da computação. Incluem fatores como organização, protocolos de comunicação, acesso a dados, atribuição de funcionalidades, escalabilidade, performance, composição e seleção do \textit{design} ideal~\cite{Garlan_1993}. É possível tratar uma arquitetura de um sistema específico como uma coleção de componentes juntamente com uma descrição dos conectores, que define as interações entre os componentes.

\subsection{Arquitetura de referência para IA}

Um estilo de arquitetura define uma família de tais sistemas em termos de um padrão de organização estrutural, e determina o vocabulário de componentes e conectores que podem ser usados em instâncias desse estilo, juntamente com um conjunto de restrições sobre como eles podem ser combinados~\cite{Garlan_1993}. A decisão sobre tal estilo depende da solução e dos requisitos de um sistema. Ela pode adicionar novos componentes, incrementá-los com novos requisitos ou adicionar restrições sobre eles~\cite{Bosch_2004}.

Um exemplo de arquitetura que generaliza todo o processo, desde a necessidade de negócio até a implantação do modelo de IA, é a IBM Analytics and AI Reference Architecture~\cite{IBM_2021}, ilustrada na Figura \ref{fig:AIReferenceArchitecture}. Nela, são definidos os seguintes requisitos não-funcionais: performance, estabilidade, segurança, escalabilidade, manutenibilidade e regulamentações de privacidade/\textit{compliance}, e pode ser classificada em 4 grupos principais envolvendo diversos tipos de processos e componentes:

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{images/ai-analytics-ref-diagram-analyze.jpg}
\caption {IBM Analytics and AI Reference Architecture~\cite{IBM_2021}.}
\label{fig:AIReferenceArchitecture}
\end{figure}

\begin{itemize}
\item \textbf{Coleta:} Relaciona os processos de coleta, armazenamento e transformação de diversas fontes de dados, estruturadas ou não estruturadas, para determinados repositórios (\textit{Data Lakes}).
\item \textbf{Organização:} Relaciona os processos de organização e estruturação dos dados nos presentes nos \textit{Data Lakes} e necessários para o uso das aplicações que envolvem a análise dos dados. Dependendo do uso pode-se aplicar processos de Governança.
\item \textbf{Análise:} Relaciona os processos para desenvolvimento de aplicações de IA e relatórios após a organização dos dados para tomadas de decisão e uso de aplicações externas.
\item \textbf{Infusão:} Relaciona os processos de disponibilização desses dados e conhecimento obtidos na fase de análise para aplicações externas.
\end{itemize}

\subsection{Arquitetura MAPE-K}

Em 2001, Paul Horn introduziu o conceito de Computação Autônoma como possível solução para a crescente complexidade dos sistemas da época, onde previa-se que os mesmos se tornariam muito grandes e complexos até mesmo para os profissionais mais qualificados configurarem e realizarem manutenção. Tal conceito qualifica sistemas de computação que podem se autogerenciar com relação aos objetivos de alto nível dados pelos administradores e é derivado da biologia, dado a grande variedade e hierarquia de sistemas autônomos presentes na natureza e na sociedade~\cite{Kephart_2003}. 

Em um ambiente autônomo e autogerenciado, os componentes de sistema podem incorporar como funcionalidade um \textit{loop} de controle, que podem ser categorizados em 4 categorias principais. Essas categorias são consideradas atributos dos componentes do sistema e são definidas como~\cite{IBM_2005}:

\begin{itemize}
\item \textbf{Auto-configuração:} Pode se adaptar dinamicamente a mudanças no ambiente. Um componente autoconfigurável realiza esta adaptação usando políticas fornecidas pelo profissional. Tais mudanças podem incluir a implantação de novos componentes ou a remoção dos existentes, ou mudanças drásticas nas características do sistema. A adaptação dinâmica ajuda a garantir força e produtividade contínuas da infraestrutura, resultando em crescimento e flexibilidade dos negócios.
\item \textbf{Auto-cura:} Pode descobrir, diagnosticar e reagir a interrupções. Um componente auto-curável pode detectar falhas no sistema e iniciar ações corretivas baseadas em políticas sem interromper o ambiente. A ação corretiva pode envolver um produto alterando seu próprio estado ou efetuando mudanças em outros componentes do ambiente. Com isso, o sistema se torna mais resiliente porque as operações cotidianas possuem menos probabilidade de falhar.
\item \textbf{Auto-otimização:} Pode monitorar e ajustar recursos automaticamente. Um componente auto-otimizável pode se ajustar para atender às necessidades do usuário. As ações de ajuste podem significar realocar recursos para melhorar a utilização geral, como em resposta a cargas de trabalho que mudam dinamicamente, ou garantir que processamentos possam ser concluídos em tempo hábil. A auto-otimização ajuda a fornecer um alto padrão de serviço para quem vai utilizar o sistema. Sem funções de auto-otimização, não há uma maneira fácil de re-escalonar os recursos de infraestrutura quando um aplicativo não os usa totalmente.
\item \textbf{Auto-proteção:} Pode antecipar, detectar, identificar e proteger contra ameaças de qualquer lugar. Um componente de autoproteção pode detectar comportamentos hostis à medida que ocorrem e tomar ações corretivas para se tornarem menos vulneráveis. Os comportamentos hostis podem incluir acesso e uso não autorizados, infecção e proliferação de vírus e ataques de negação de serviço. Os recursos de autoproteção permitem que as empresas apliquem consistentemente políticas de segurança e privacidade.
\end{itemize}

Para a Computação Autônoma acontecer, é implementado um Elemento Autônomo~\cite{Abbas_2010}, um componente de software que gerencia partes do sistema baseando-se em um \textit{loop} MAPE-K (\textit{Monitor, Analyze, Plan, Execute, and Knowledge}), ilustrado na Figura \ref{fig:MAPEK}. O MAPE-K é um conceito que constitui um \textit{loop} de controle, usado para monitorar e controlar um ou mais elementos gerenciados. Um elemento gerenciado (\textit{Managed Element}) pode ser um hardware, como uma controlador de uma linha de produção, um software, como um gerenciador de banco de dados, outro Elemento Autônomo ou funcões específicas, como balanceamento de carga. Um \textit{loop} de controle MAPE-K é dividido da seguinte forma:

\begin{itemize}
\item \textbf{Monitoramento (\textit{Monitor}):} Esta parte é responsável por monitorar os recursos gerenciados e coletar, agregar e filtrar dados. O monitoramento é feito por meio de um sensor (\textit{Sensor}) ou mais sensores.
\item \textbf{Análise (\textit{Analyze}):} Analisa os dados relatados pela parte do monitor. A análise visa compreender qual é o estado atual do sistema e se há medidas para serem tomadas.
\item \textbf{Planejamento (\textit{Plan}):} Um plano de ação é preparado no
base dos resultados da análise. O plano é uma série de medidas que irão mover o sistema de seu estado atual para um estado desejado.
\item \textbf{Execução (\textit{Execute}):} O plano é executado e controlado.
Um efetor (\textit{Effector}) ou mais executam as ações planejadas no recurso.
\item \textbf{Conhecimento (\textit{Knowledge}):} A base de conhecimento é central e acessível por todas as partes do \textit{loop}. Separado a partir de dados coletados e analisados, ele contém conhecimento adicional, como modelos de arquitetura, modelos de metas, políticas e planos de mudança.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/MAPE-K.png}
\caption {Diagrama de funcionamento da arquitetura MAPE-K~\cite{Abbas_2010}.}
\label{fig:MAPEK}
\end{figure}

\subsection{Arquitetura \textit{Pipes-and-Filters}}

Em uma arquitetura \textit{Pipes-and-Filters}, ilustrada na Figura \ref{fig:PipeandFilter}, cada componente tem um conjunto de entradas e um conjunto de saídas. Um componente lê \textit{streams} (ou fluxos) de dados em suas entradas e produz \textit{streams} de dados em suas saídas, abstraindo a entrega de um resultado como um todo. O \textit{stream} de entrada é transformado de modo que a saída comece a ser produzida antes da entrada ser completamente consumida. Por isso, os componentes são chamados de filtros (\textit{filters}). Os conectores deste estilo servem como condutores para os \textit{streams}, transmitindo as saídas de um filtro para as entradas de outro. Por isso, os conectores são chamados de tubos (\textit{pipes})~\cite{Garlan_1993}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{images/PipeandFilter.png}
\caption {Diagrama de uma arquitetura \textit{Pipes-and-Filters}.}
\label{fig:PipeandFilter}
\end{figure}

Os filtros devem ser independentes, isto é, não devem compartilhar estado com outros filtros e, durante a programação, o ideal é que os filtros independam da ordem de processamento. Suas especificações podem restringir os dados transportados nos \textit{pipes} de entrada e nos \textit{pipes} de saída, mas não conhecem quaisquer outros filtros, ou componentes, conectados por seus \textit{pipes}. Especializações comuns desse estilo incluem \textit{pipelines}, que restringem as topologias a sequências lineares de filtros; \textit{Pipes} limitados, que restringem a quantidade de dados que podem ser passados em um \textit{pipe}, e \textit{pipes} tipados, que exigem que os dados passados entre dois filtros tenham um tipo bem definido.

O uso da arquitetura \textit{Pipes-and-Filters} possui como vantagens:

\begin{itemize}
\item Permite que o Arquiteto de Software/Desenvolvedor entendam o comportamento geral de entrada/saída de um sistema como uma composição simples dos comportamentos dos filtros individuais.
\item Suporta a reutilização: quaisquer dois filtros podem ser conectados, desde que concordem com os dados que estão sendo transmitidos entre eles. 
\item Os sistemas podem ser facilmente mantidos e aprimorados: novos filtros podem ser adicionados a sistemas existentes e filtros antigos podem ser substituídos por outros melhorados.
\item Permitem certos tipos de análise especializada, como análise de rendimento e de impasse.
\item Naturalmente suportam a execução simultânea: Cada filtro pode ser implementado como uma tarefa separada e potencialmente executado em paralelo com outros filtros.
\end{itemize}

Como desvantagens, é possível citar:

\begin{itemize}
\item Geralmente levam a uma organização de processamento em lote. Embora os filtros possam processar dados de forma incremental, uma vez que os filtros são inerentemente independentes, o Arquiteto de Software é forçado a pensar em cada filtro como fornecendo uma transformação completa dos dados de entrada em dados de saída. 
\item Por sua natureza de transformação de dados, os sistemas que usam a arquitetura \textit{Pipes-and-Filters} normalmente não são bons para lidar com aplicativos interativos. Esse problema é mais grave quando são necessárias atualizações de exibição incrementais, porque o padrão de saída para atualizações incrementais é radicalmente diferente do padrão para saída de filtro.
\item Podem ser prejudicados por terem que manter correspondências entre dois \textit{streams} separados, mas relacionados. 
\item Podem forçar um resultado médio na transmissão de dados em situações onde muitos filtros sejam encadeados de uma vez, resultando em trabalho adicional para cada filtro separar seus dados e analisar o que for necessário. Isso, por sua vez, pode levar tanto à perda de desempenho quanto ao aumento da complexidade na escrita dos próprios filtros.
\end{itemize}

\section{MLOps}
\label{sec:MLOps}


No caso de MLOps, ele pode ser visto como o processo de implantar os melhores modelos de \textit{Machine Learning} para o ambiente de produção, unindo os campos de \textit{Machine Learning}, DevOps e Engenharia de Dados. 
O termo MLOps é usado para definir uma junção entre modelos de Aprendizado de máquina (ML) e DevOps, focado principalmente na governança e gerenciamento do ciclo de vida de modelos de ML e de decisão operacionalizados. Os principais recursos incluem integração contínua/entrega contínua, ou \textit{continuous integration}/\textit{continuous delivery} (CI/CD), ambientes de desenvolvimento, sistema de controle de versão, armazenamento e reversão de modelos. O termo pode ser visto como o processo de implantar os melhores modelos de ML para o ambiente de produção, definindo uma estrutura e plataforma para gerenciamento completo do ciclo de vida de artefatos de aplicativos de Aprendizado de Máquina, conforme ilustração na Figura \ref{fig:MLOpsLoop} e unindo os campos de Aprendizado de Máquina, DevOps e Engenharia de Dados.

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/code-validate-deploy-loop.png}
\caption {Ciclo resumido do processo presente em MLOps~\cite{Tripathi_2021}}
\label{fig:MLOpsLoop}
\end{figure}

A operacionalização dos procedimentos em Software já é uma prática comum de ser adotada, mas em aplicações de ML sua adoção é mais recente, possuindo desafios e estilos de dívidas técnicas diferentes de uma aplicação tradicional de Software~\cite{Sculley_2015}:

\begin{itemize}
    \item \textbf{Complexidade dos modelos implantados:} A prática tradicional de Engenharia de Software mostrou que o uso de encapsulamento e design modular ajudam a criar código sustentável no qual é fácil de fazer isoladamente mudanças e melhorias. Tais práticas ajudam a expressar a consistência das entradas e saídas de informações de um determinado componente. Infelizmente, é difícil traduzir essas abstrações para sistemas de ML prescrevendo um comportamento pretendido específico. De fato, ML é necessária exatamente nos casos em que o comportamento desejado não pode ser efetivamente expresso na lógica do Software sem dependência de dados externos. O mundo real não se encaixa totalmente de forma encapsulada.
    \item \textbf{Dependência dos dados:} Dívidas técnicas relativas a dependências no código são apontadas como uma dos principais contribuintes para a complexidade do código e dívida técnica em configurações clássicas de Engenharia de Software. Entretanto, as dependências de dados em sistemas de ML possuem uma capacidade semelhante para aumentar a dívida, mas podem ser mais difíceis de detectar. As dependências no código podem ser identificadas por meio de análise estática por compiladores e ligadores. Sem ferramentas semelhantes para dependências de dados, não é difícil construir grandes cadeias de dependências de dados que podem ser difíceis de arrumar.
    \item \textbf{Ciclos de \textit{Feedback}:} Uma das principais características dos sistemas de ML é que eles geralmente acabam influenciando seu próprio comportamento se forem atualizados ao longo do tempo. Isso leva a uma forma de dívida técnica de análise, na qual é difícil prever o comportamento de um determinado modelo antes de ele ser lançado. Esses ciclos de \textit{feedback} podem assumir diferentes formas, mas são mais difíceis de detectar e resolver se ocorrerem gradualmente ao longo do tempo, como pode ser o caso quando os modelos são atualizados com pouca frequência.
    \item \textbf{Anti-padrões nas aplicações:} É comum que sistemas de ML acabem com grandes dívidas em padrões de design. Dívidas como \textit{glue code} (integração de sistemas teoricamente incompatíveis), \textit{pipeline jungles} (\textit{pipelines} de grande complexidade e com difícil rastreamento de problemas, geralmente com etapas fortemente acopladas), código não utilizado, dívidas de abstração, \textit{code smells} (códigos que podem indicar problemas subjacentes em um componente ou sistema) devem ser evitados ou refatorados sempre que possível.
    \item \textbf{Dívidas de configuração:} Qualquer sistema grande tem uma ampla gama de opções configuráveis, incluindo quais \textit{features} são usadas, como os dados são selecionados, uma ampla variedade de configurações de aprendizado específicas de algoritmos, potencial pré ou pós-processamento, métodos de verificação, entre outras. A grande quantidade de opções e a relação que essas tem umas com as outras no sistema torna a configuração difícil de modificar corretamente e difícil de raciocinar. No entanto, erros na configuração podem custar caro, levando a sérias perdas de tempo, desperdício de recursos de computação ou problemas de produção.
    \item \textbf{Mudanças do mundo externo:} Sistemas de ML geralmente interagem diretamente com o mundo externo, e este raramente é estável, criando um custo de manutenção contínuo. Para isso, é importante prever os vieses e determinar limiares para as previsões que estes sistemas irão realizar.
    \item \textbf{Testagem e fidelidade dos dados:} Embora estejam relacionados a itens já mencionados (dependência dos dados e mudanças do mundo externo), a garantia de que estes desafios estejam bem controlados começa durante o desenvolvimento dos modelos. Dados de entrada testados e fidedignos às situações presentes do mundo garantem este controle, e procedimentos para realizar esta tarefa constituem em outro desafio para garantia da qualidade da aplicação.
    \item \textbf{Processos e cultura do time:} É importante criar equipes que se importem na melhora do modelo e do seu próprio processo de implantação. Sistemas maduros podem ter dezenas ou centenas de modelos rodando simultaneamente, tornando-se complexos ao longo do tempo e necessitando de melhoria contínua dos processos para manter a qualidade.
\end{itemize}

O processo de um projeto de MLOps, conforme ilustrado na Figura \ref{fig:MLOpsLoop}, deve dar suporte à automação, integração e monitoramento em todas as etapas da construção de um modelo de ML, incluindo treinamento, integração, teste, lançamento, implantação e gerenciamento de infraestrutura~\cite{Testi_2022}. Com tal processo, projeta-se um pipeline automatizado utilizando o recurso de CI/CD, conforme exemplo mostrado na Figura \ref{fig:MLOpsPipeline}. CI deve permitir a quem desenvolve experimentos orquestrados para análise, validação e preparação de dados, treinamento, avaliação e validação de novos modelos de ML, e CD deve permitir um pipeline que implante um modelo de ML definido como estável em produção, realize o monitoramento deste e implante outro modelo de ML caso as métricas deste monitoramento sejam consideradas insatisfatórias.

\begin{figure}[H]
\centering
\includegraphics[scale=0.33]{images/mlops-pipeline-example.png}
\caption {Exemplo de Pipeline de MLOps com ambientes de Desenvolvimento, \textit{Staging} e Produção~\cite{Tripathi_2021}}
\label{fig:MLOpsPipeline}
\end{figure}

O ciclo de vida de um modelo de ML tem metodologias diferentes para se adequar a diferentes cenários e tipos de dados. A abordagem mais utilizada por cientistas de dados é o Cross-Industry Standard Process for Data Mining (CRISP-DM)~\cite{Shearer_2000}, introduzido em 1996 pela Daimler Chrysler. Especialistas podem emprestar as metodologias CRISP-DM padrão e tentar aplicá-las ao pipeline de MLOps, envolvendo dois papeis: O Cientista de Dados, responsável pelo treinamento e teste do modelo, e o Engenheiro de \textit{Machine Learning}, responsável pela produção e implantação. Tais papeis trabalhariam em conjunto em tarefas envolvendo as seguintes etapas:

\begin{itemize}
    \item Análise do Problema do Negócio
    \item Definição e entendimento dos dados
    \item Metodologia analítica de ML
    \item Componentes de um Pipeline de CI/CD
    \item Acionamento automatizado do Pipeline
    \item Armazenamento de registro de modelo
    \item Monitoramento e desempenho
    \item Serviço para implantação de modelos ML em Produção
\end{itemize}

Isso pode ser feito por uma aplicação própria para realizar este gerenciamento, por funcionalidades implementadas na própria aplicação de ML ou através de \textit{frameworks} de automação como o Amazon SageMaker~\cite{Amazon_2023}, que podem poupar tempo para desenvolvimento de uma nova aplicação a troco de um pagamento de acordo com a demanda utilizada. Estes frameworks podem cuidar de toda a parte de gerenciamento dos dados, modelagem/treinamento ou operação/implantação, e como são modulares e cuidam apenas de uma destas três categorias do processo, podem ser escolhidas ou não de acordo com a necessidade do projeto e conhecimento da equipe.

\section{Trabalhos Relacionados}
\label{sec:TrabalhosRelacionados}

Os principais trabalhos encontrados envolvendo métricas de \textit{Fairness} e arquitetura de sistemas pensando em \textit{Fairness} como requisito vêm do time de engenharia do LinkedIn.~\cite{Geyik_2019} descreve quais métricas foram utilizadas e quais algoritmos foram implementados para ranqueamento dos dados.~\cite{Kenthapadi_2019} descreve como o conceito de \textit{Fairness} pode ser implementado em uma aplicação de IA, e mostra a arquitetura sobre ela.

Quanto ao processo de desenvolvimento e arquiteturas utilizadas para definir a estrutura final, a IBM AI Reference Architecture~\cite{IBM_2021} foi utilizada no processo como modelo para desenvolvimento de aplicações. Uma arquitetura comparável à arquitetura \textit{Pipes-and-Filters} é a FBFlow do Facebook~\cite{Dunn_2016}, onde o uso de DAGs (\textit{Direct Acyclic Graphs}) para a execução de um determinado processo ou \textit{workflow} é uma alternativa simples de ser implementada e interpretada pelos desenvolvedores, definindo possível conectar o processo de ML e realizar o cálculo de métricas de forma automatizada. Há diferenças entre DAGs e \textit{Pipes-and-Filters}, mas o objetivo de ambos acaba sendo o mesmo: Conectar diversos processos menores para formar um sistema coeso.

Para o desenvolvimento da parte de análise realizada na arquitetura MAPE-K, uma abordagem similar à aplicada é a \textit{Logic Scoring of Preference} (LSP), utilizada em trabalhos como \textit{"LSP method and its use for evaluation of Java IDEs"}~\cite{Dujmovic_2006}, que foi utilizada primariamente para a avaliação de aplicações e define pontuações através de agregações e critérios bem definidos para a avaliação das mesmas. Métodos de \textit{Multi-Criteria Decision Making} (MCDM) como o \textit{Analytic Hierarchy Process} (AHP), presente no livro \textit{"The analytic hierarchy process : planning, priority setting, resource allocation"}~\cite{Saaty_1980}, também possuem uma abordagem bastante semelhante a realizada e podem ser relacionados, embora não tenham nenhuma relação com os conceitos abordados. Um outro trabalho que pode ser mencionado é o artigo \textit{"The VADA Architecture for Cost-Effective Data Wrangling"}~\cite{Konstantinou_2017}, que utiliza pesos para dar contexto aos dados medidos. Embora não seja o principal tema, indicar a importância dos dados analisados garantiu a autonomia necessária sem exigir grandes modificações.

A tabela~\ref{tbl:CategorizacaoTrabalhosRelacionados} categoriza tais trabalhos de acordo com os conceitos relacionados neste capítulo, divididos nas seguintes colunas: "Ciência/Eng. Dados"~para a seção \ref{sec:CienciaEngDados}, "ML"~para a seção \ref{sec:MachineLearning}, "Fairness" para a seção \ref{sec:Fairness}, "Eng. Software"~para a seção \ref{sec:EngSoftware} e "MLOps"~para a seção \ref{sec:MLOps}.

\begin{sidewaystable}[h]
\begin{center}
  \caption{Categorização dos trabalhos relacionados}
  \label{tbl:CategorizacaoTrabalhosRelacionados}
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{cccccccccc}
    \toprule
    \multicolumn{2}{c}{\multirow{2}{*}{\bf Referências}} & \multicolumn{2}{c}{\multirow{2}{*}{\bf Descrição}} & \multicolumn{5}{c}{Conceitos}\\
    \cmidrule{5-9}
%    \hline
    \multicolumn{4}{c}{} & \bf Ciência/Eng. Dados & \bf ML & \bf Fairness & \bf Eng. Software & \bf MLOps\\
    \midrule
    \multicolumn{2}{c}{Geyik et al., 2019~\cite{Geyik_2019}} & \multicolumn{2}{c}{\makecell{Métricas utilizadas \\ Ranqueamento dos dados}} &  & \cmark & \cmark &  & \\[3ex]
%    \hline
    \multicolumn{2}{c}{Kenthapadi et al., 2019~\cite{Kenthapadi_2019}} & \multicolumn{2}{c}{Arquitetura/implementação de métricas de Fairness} &  & \cmark & \cmark & \cmark & \cmark \\[2ex]
%    \hline
    \multicolumn{2}{c}{\makecell{IBM Analytics and \\ AI Reference Architecture~\cite{IBM_2021}}} & \multicolumn{2}{c}{\makecell{Arquitetura de referência para Dados e ML}} & \cmark & \cmark &  & \cmark & \\[3ex]
%    \hline
    \multicolumn{2}{c}{\makecell{FBFlow - Facebook \\ (Dunn, 2016)~\cite{Dunn_2016}}} & \multicolumn{2}{c}{\makecell{Uso de DAGs para determinar \textit{workflow} \\ Execução/interpretação de processos de ML}} & \cmark & \cmark &  &  & \cmark\\
%    \hline
    \multicolumn{2}{c}{\makecell{Konstantinou et al.,2017}~\cite{Konstantinou_2017}} & \multicolumn{2}{c}{\makecell{Arquitetura para entrelaçamento de dados \\ Utilização de pesos para determinar contexto aos dados}} &  &  &  & \cmark & \\[3ex] 
%    \hline
    \multicolumn{2}{c}{\makecell{Saaty, 1980}~\cite{Saaty_1980}} & \multicolumn{2}{c}{\makecell{Hierarquização de múltiplos critérios \\ Definição de pesos, ranqueamento e tomada de decisão}} &  &  &  &  & \\[3ex] 
%    \hline
    \multicolumn{2}{c}{\makecell{Dujmovic et al.,2006}~\cite{Dujmovic_2006}} & \multicolumn{2}{c}{\makecell{Uso de \textit{Logic Scoring of Preference} \\ Definição de pontuações para avaliação de aplicações}} &  &  &  & \cmark & \\[3ex]    
    \bottomrule
  \end{tabular}}
\end{center}
\end{sidewaystable}

\chapter{Arcabouço Autonômico Proposto}
\label{sec:metodologia}

Neste capítulo, será abordado todo o processo para obtenção da solução e o detalhamento de 2 dos 4 módulos apresentados na Introdução: Módulo de ML e Gerenciador Autonômico. Com esta estrutura foi possível exibir os resultados presentes nos capítulos que detalham os Estudos de Caso, onde diferentes configurações arquiteturais puderam ser exibidas dependendo dos dados treinados e da calibração aplicada.

\section{Arquitetura da solução}

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{images/Arcabouco_Autonomico_Proposto.jpg}
\caption {Componentes arquiteturais da solução}
\label{fig:ComponentesArquiteturais}
\end{figure}

A arquitetura da solução é composta por 4 módulos:

\begin{itemize}
    \item {\textbf{Engenharia de Dados:}} Módulo com o objetivo de executar processos de limpeza e transformação de dados.
	\item {\textbf{Módulo de ML:}} Módulo que executa um \textit{Pipeline} capaz de automatizar uma aplicação de ML, com estágios de preparação de dados (Pré-processamento), treinamento (Processamento) e avaliação dos resultados (Pós-processamento) para a geração de um modelo final.
     \item {\textbf{Gerenciador Autonômico:}} Módulo contendo um \textit{loop} MAPE-K que controla o Módulo de ML como um Elemento Gerenciado para automatizar parte das atividades a serem executadas.
     \item {\textbf{Interface:}} Módulo cujo objetivo é prover uma experiência de usuário mais simples e intuitiva para o Cientista de Dados para configurar e iniciar o Gerenciador Autonômico.
\end{itemize}

A integração entre esses módulos é ilustrada na Figura \ref{fig:ComponentesArquiteturais}. O módulo de Engenharia de Dados será utilizado pelo Engenheiro de Dados e retorna um conjunto de dados com os tratamentos necessários. A Interface será utilizada pelo Cientista de Dados, que determinaria os parâmetros necessários dependendo do contexto do problema. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.375]{images/Diagrama_Sequencia_Manual.jpg}
\caption {Diagrama de sequência de uma execução manual no Módulo de ML}
\label{fig:DiagramaSequenciaManual}
\end{figure}

É possível executar o Módulo de ML de forma manual ou auxiliado pelo Gerenciador Autonômico, e o processo de sua execução manual pode ser visualizada no Diagrama de Sequência presente na Figura \ref{fig:DiagramaSequenciaManual}. Antes da execução, o Engenheiro de Dados manipula os conjuntos de dados com os processos de limpeza e transformação implementados no módulo de Engenharia de Dados, e os armazena na Base de Conhecimento. Desta forma, o Cientista de Dados pode iniciar o Módulo de ML ao passar os parâmetros necessários para a execução. Durante a execução, o Módulo de ML inicia o Monitor do Gerenciador Autonômico para coletar dados durante a execução, recupera o conjunto de dados armazenado na Base de Conhecimento, e executa as etapas de Pré-processamento, Processamento e Pós-processamento definidas pelos parâmetros passados para a geração do modelo final. Ao finalizar a execução, o Monitor salva todos os dados coletados na Base de Conhecimento para usos futuros.

\begin{sidewaysfigure}
\clearpage
\includegraphics[scale=0.445]{images/Diagrama_Sequencia_Auto.jpg}
\caption {Diagrama de sequência de uma execução no Arcabouço Autonômico Proposto}
\label{fig:DiagramaSequenciaAuto}
\end{sidewaysfigure}

O processo de execução auxiliado pelo Gerenciador Autonômico pode ser visualizado no Diagrama de Sequência presente na Figura \ref{fig:DiagramaSequenciaAuto}. Através do módulo de Interface, o Cientista de Dados escolhe um Conjunto de Dados e solicita a execução do Gerenciador Autonômico. Este, por sua vez, verifica todas as execuções passadas no Módulo de Machine Learning que utilizaram o conjunto de dados modificado e estão armazenadas na Base de Conhecimento. Em seguida, realiza uma análise detalhada com cálculos (que serão descritos posteriormente) e, em um loop, verifica as configurações de planos de execução já implementados. O Gerenciador Autonômico retorna as melhores opções de execução, permitindo que o Cientista de Dados escolha a mais adequada e solicite uma nova execução para o Módulo de ML. Esta execução é equivalente a já explicada na Figura \ref{fig:DiagramaSequenciaManual}, e o Gerenciador Autonômico busca os dados desta execução para exibição na Interface ao Cientista de Dados.

\section{Módulo de ML}
\label{sec:ModuloML}

O Módulo de ML segue uma estrutura de passos, ilustrados na Figura \ref{fig:FairnessPipeline}, que é dividida em 3 componentes principais:

\begin{itemize}
	\item \textbf{Pré-Processamento:} Abrange os passos de preparação do dado necessários para a aplicação de um algoritmo para treinamento. Quando \textit{Fairness} entra no escopo de requisitos, também se aplica preparações baseadas no atributo protegido e pode-se aplicar ou não algoritmos para redução de vieses de pré-processamento.
	\item \textbf{Processamento/Treinamento:} Abrange o treinamento do dado preparado no componente de Pré-Processamento através de um algoritmo específico. Quando \textit{Fairness} entra no escopo de requisitos, tal algoritmo pode conter a redução de vieses em seu treinamento ou não.
	\item \textbf{Pós-Processamento/Validação:} Abrange o cálculo de métricas e a verificação se o modelo gerado nesta execução atingiu os valores adequados. Quando \textit{Fairness} entra no escopo de requisitos, pode-se aplicar ou não algoritmos para redução de vieses de pós-processamento.
\end{itemize}

O módulo é parametrizado para proporcionar ao Cientista de Dados controle sobre a configuração arquitetural utilizada em sua execução, considerando o conjunto de dados, o atributo protegido e os algoritmos empregados. Essa parametrização não apenas facilita a programação de planos de execução no Gerenciador Autonômico, mas também permite a flexibilidade necessária. Ao adotar o padrão de arquitetura \textit{Pipes-and-Filters}, cada etapa, conforme ilustrado na figura, é composta por diversos \textit{Pipes} e Filtros intercambiáveis, dependendo da parametrização escolhida.

Os algoritmos utilizados por este módulo estão presentes nas seções \ref{sec:MLAlgorithms} e \ref{sec:FairnessAlgorithms}, e as métricas calculadas neste módulo estão presentes nas seções \ref{sec:EvaluationMetrics} e \ref{sec:FairnessMetrics}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{images/ml-fairness-pipeline.jpg}
\caption {Componentes e passos do Módulo de ML}
\label{fig:FairnessPipeline}
\end{figure}

Cada passo está presente em um processo padrão para desenvolvimento de um modelo de Aprendizado de Máquina, e se comportam da seguinte maneira:

\begin{itemize}
\item \textbf{Pré-Processamento:}
	\begin{itemize}
	\item \textbf{Inicia Monitor do Gerenciador Autonômico:} Ao iniciar uma execução, o Monitor do Gerenciador Autonômico é iniciado para efetuar a coleta de metadados presentes durante toda a execução.
	\item \textbf{Obtenção do conjunto de dados e seus metadados:} De acordo com o parâmetro passado para o Módulo de ML, um \textit{Pipe} relativo ao conjunto de dados em questão é selecionado, e os metadados presentes neste Pipe podem ser utilizados para uma gravação em arquivo caso necessário.
	\item \textbf{Obtenção do atributo protegido, grupos e classes de classificação:}De acordo com o parâmetro passado para o Módulo de ML, um \textit{Pipe} relativo ao atributo protegido em questão é selecionado, e informações de grupos privilegiados e não-privilegiados e classes para classificação também estão presentes neste Pipe.
	\item \textbf{Divisão do conjunto de dados (Treino/Validação/Testes):} Um Filtro relativo a divisão do conjunto de dados (entre conjunto de treino, conjunto de validação e conjunto de testes) é executado e suas divisões resultantes são colocadas em um \textit{Pipe}.
	\item \textbf{Pré-Processamento dos dados com base no atributo protegido:} As informações de atributo protegido são passadas para um Filtro para realizar um pré-processamento no conjunto de dados, preparando o conjunto de dados para usos em algoritmos para redução de vieses.
	\item \textbf{Execução(ou não) de algoritmo para redução de viés:} De acordo com o parâmetro passado para o Módulo de ML, um algoritmo para redução de viés na fase de Pré-Processamento é colocado ou não para ser executado no \textit{Workflow}. 
	\end{itemize}
\item \textbf{Processamento/Treinamento:}
	\begin{itemize}
	\item \textbf{Execução de algoritmo de treinamento com(ou sem) redução de viés:} De acordo com o parâmetro passado para o Módulo de ML, um algoritmo de treinamento é colocado ou não para ser executado.
	\end{itemize}
\item \textbf{Pós-Processamento/Validação:}
	\begin{itemize}
	\item \textbf{Execução(ou não) de algoritmo para redução de viés:} De acordo com o parâmetro passado para o Módulo de ML, um algoritmo para redução de viés na fase de Pós-Processamento é colocado ou não para ser executado no módulo.
	\item \textbf{Cálculo de métricas (Avaliação/Fairness):} É realizado uma predição do algoritmo treinado com o conjunto de teste, e as predições são comparadas com os valores verdadeiros para obtenção das métricas.
	\end{itemize}
\end{itemize}

\section{Gerenciador Autonômico}
\label{sec:GerenciadorAutonomico}

Os dados armazenados na Base de Conhecimento a cada execução do Módulo de ML vão ser utilizados como insumos para análise no Gerenciador Autonômico, seguindo o modelo de arquitetura MAPE-K. Após algumas execuções do Módulo de ML, a Base de Conhecimento possui informação suficiente para a realização da análise do Gerenciador Autonômico, que ajudaria a selecionar a melhor configuração arquitetural para o módulo gerar melhores modelos. Após a obtenção de tal Base de Conhecimento, é possível verificar como cada componente do Gerenciador Autonômico se comportará, conforme já ilustrado na Figura \ref{fig:ComponentesArquiteturais}.

\subsection{Monitor}

O Monitor é iniciado junto com a execução do Módulo de ML, conforme já ilustrado na Figura \ref{fig:FairnessPipeline}. Os metadados que podem ser guardados são os seguintes:

\begin{itemize}
\item \textbf{Pré-Processamento:} Parâmetros referentes a configuração arquitetural executada pelo módulo (Conjunto de dados, Atributo protegido e Algoritmo de pré-processamento) e "assinatura"/checksum do conjunto de dados utilizado.
\item \textbf{Processamento:} Parâmetros referentes a configuração arquitetural executada (Algoritmo de treinamento) e parâmetros utilizados para a aplicação do treinamento.
\item \textbf{Pós-Processamento:} Parâmetros referentes a configuração arquitetural executada (Algoritmo de pós-processamento) e Métricas do modelo resultante (Métricas de Avaliação e métricas de \textit{Fairness}).
\end{itemize}

Os dados coletados por execução são organizados em dois conjuntos diferentes: Um contendo os metadados e informações referentes a configuração arquitetural executada pelo Módulo de ML, e outro contendo as métricas obtidas pelo modelo resultante dessa execução, ambos possuindo um identificador da execução para estabelecer consistência entre os dados dos dois conjuntos. Com tal organização, é facilitado o desenvolvimento do Analisador.

\subsection{Analisador}

A Figura \ref{fig:AnaliseModuloML} exemplifica o procedimento de análise para todas as configurações arquiteturais. As execuções realizadas no Módulo de ML que foram coletadas pelo Monitor e armazenadas na Base de Conhecimento são extraídas e analisadas por suas métricas. Nesta análise, cada execução recebe pontuações consolidando os valores de suas métricas de Avaliação e suas métricas de \textit{Fairness}. Posteriormente, as análises realizadas são agrupadas em suas respectivas configurações arquiteturais, que recebem uma pontuação definitiva. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{images/Procedimento_Analise.jpg}
\caption {Procedimento de análise para execuções no módulo de ML}
\label{fig:AnaliseModuloML}
\end{figure}

As pontuações de cada configuração arquitetural são agrupadas e colocadas em um conjunto de dados. Para o desenvolvimento de melhores estratégias, alguns metadados, como data de execução, são obtidos e agrupados junto às pontuações. O conjunto resultante deste processo é passado ao Planejador.

\subsubsection{Análise de métricas}

Para a análise de cada execução feita pelo Módulo de ML, é realizado um cálculo de pesos das métricas. O motivo de existir esse cálculo é mensurar o contexto do problema de acordo com uma análise prévia do Cientista de Dados, e consolidar as métricas de Avaliação e métricas de \textit{Fairness} para simplificar as estratégias de planejamento.

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{images/Arvore_Metricas.jpg}
\caption {Árvore de métricas a serem analisadas}
\label{fig:ArvoreMetricas}
\end{figure}

Para um melhor isolamento dos contextos, foi estabelecida para este cálculo uma árvore de métricas, ilustrada na Figura \ref{fig:ArvoreMetricas}, onde as mesmas foram divididas em dois grupos (Métricas de Avaliação e Métricas de \textit{Fairness}), e foram atribuídos as métricas referentes a cada grupo, presentes nas seções \ref{sec:EvaluationMetrics} e \ref{sec:FairnessMetrics}. Para cada grupo, dependendo do contexto do problema, são atribuídos os pesos $w_E$ para o grupo de Métricas de Avaliação e $w_F$ para o grupo Metricas de \textit{Fairness}. Com isso, a forma utilizada para consolidar todas as métricas foi através de uma pontuação $S$ definida por uma média ponderada, exibida na equação \ref{eqn:totalScore}, entre as pontuações $S_E$ e $S_F$, que são calculadas isoladamente.

\begin{equation}
\label{eqn:totalScore}
	S = \frac{w_F \times S_{F} + w_E \times S_{E}}{w_F + w_E}
\end{equation}

Para o cálculo de $S_E$ e $S_F$, que são as pontuações respectivas referentes ao grupo de Métricas de Avaliação e ao grupo de Métricas de \textit{Fairness}, também é utilizado o cálculo de média ponderada e são atribuídos pesos distintos para cada métrica, sendo $w_{m_{E_i}}$ para o grupo de Métricas de Avaliação e $w_{m_{F_i}}$ para o grupo Metricas de \textit{Fairness}. Entretanto, há algumas diferenças a se considerar dado a natureza das métricas presente nos grupos. Para o caso de $S_E$, todas as métricas calculadas estão presentes no intervalo $\left[0,1 \right]$ e seus resultados são diretamente proporcionais, indicando que os melhores resultados também são os que possuem maiores valores. Isto implica no uso de uma média ponderada sem adaptações.

\begin{equation}
\label{eqn:EvaluationScore}
	\begin{aligned}
	S_{E} = \frac{\sum_{i=1}^{n_{m_E}} w_{m_{E_i}} \times m_{E_{i}}}{\sum_{i=1}^{n_{m_E}} w_{m_{E_i}}}
	\end{aligned}
\end{equation}

Nela, $n_{m_E}$ é o número de métricas de avaliação utilizadas (para a árvore definida na Figura \ref{fig:ArvoreMetricas}, $n_{m_E} = 5$), $w_{m_{E_i}}$ é o peso dado para uma determinada métrica de avaliação e $m_{E_{i}}$ é o valor de uma determinada métrica de avaliação na execução analisada.

Entretanto, para o caso de $S_F$, ocorrem 3 intervalos e situações distintas para as métricas de \textit{Fairness} presentes no grupo:

\begin{itemize}
	\item Para o caso das métricas \textit{Statistical parity difference}, \textit{Equal opportunity difference} e \textit{Average odds difference}, estas métricas envolvem a diferença entre 2 ou mais indicadores presentes no intervalo $\left[0,1 \right]$, estando no intervalo $\left[-1,1 \right]$ e sendo 0 considerado o melhor resultado.
	\item Para o caso da métrica \textit{Theil index}, o resultado está no intervalo $\left[0,1 \right]$, porém é inversamente proporcional: 0 é considerado o melhor resultado e 1 o pior
	\item Para o caso da métrica \textit{Disparate impact}, esta é a única métrica que envolve a razão entre dois indicadores, estando no intervalo $\left[0,+\infty \right[$ e sendo 1 considerado o melhor resultado.
\end{itemize}

Para normalizar as métricas $m_{F_i}$ a uma métrica $m'_{F_i}$ no intervalo $\left[0,1 \right]$, cada caso foi tratado isoladamente. Para o primeiro caso, foi utilizado $m'_{F_i} = 1-\lvert m_{F_i} \rvert$. Para o segundo caso, poderia-se utilizar $m'_{F_i} = 1-m_{F_i}$, mas o cálculo utilizado no primeiro caso também satisfaz o mesmo intervalo e produz os mesmos resultados. Para o terceiro caso, por se tratar de uma razão e pelo melhor resultado ser outro valor, foi utilizado outro cálculo:

\begin{equation}
\label{eqn:normalizationFairnessStep2}
	\begin{aligned}
	m'_{F_i} = 
	\begin{cases}
	1-\lvert \frac{1}{m_{F_i}}-1 \lvert & \text{caso $m_{F_i} > 1$}\\
	1-\lvert m_{F_i}-1 \lvert & \text{caso $0 \leqslant m_{F_i} \leqslant 1$}
	\end{cases}
	\end{aligned}
\end{equation}

Consolidando todos os casos, temos o cálculo para adequar as métricas $m_{F_i}$ a nova métrica $m'_{F_i}$:

\begin{equation}
\label{eqn:normalizationFairnessStep3}
	\begin{aligned}
	m'_{F_i} = 
	\begin{cases}
	1-\lvert \frac{1}{m_{F_i}}-1 \lvert & \text{caso $m_{F_i}$ seja \textit{Disparate impact} e $m_{F_i} > 1$}\\
	1-\lvert m_{F_i}-1 \lvert & \text{caso $m_{F_i}$ seja \textit{Disparate impact} e $0 \leqslant m_{F_i} \leqslant 1$}\\
	1-\lvert m_{F_i} \rvert & \text{caso contrário}
	\end{cases}
	\end{aligned}
\end{equation}

Após obter $m'_{F_i}$, é possível obter $S_F$ com o uso de uma média ponderada:

\begin{equation}
\label{eqn:FairnessScore}
	\begin{aligned}
	S_{F} = \frac{\sum_{i=1}^{n_{m_F}} w_{m_{F_i}} \times m'_{F_{i}}}{\sum_{i=1}^{n_{m_F}} w_{m_{F_i}}}
	\end{aligned}
\end{equation}

Nela, $n_{m_F}$ é o número de métricas de \textit{Fairness} utilizadas (para a árvore definida na Figura \ref{fig:ArvoreMetricas}, $n_{m_F} = 5$), $w_{m_{F_i}}$ é o peso dado para uma determinada métrica de \textit{Fairness} e $m'_{F_{i}}$ é o valor de uma determinada métrica de \textit{Fairness} na execução analisada que foi normalizada no intervalo $\left[0,1 \right]$. 

\subsubsection{Análise da configuração arquitetural}

Para consolidar as pontuações definitivas $S'_E$, $S'_F$ e $S'$ para cada configuração arquitetural, cada execução é agrupada de acordo com a configuração arquitetural utilizada e é realizada uma média aritmética simples de acordo com as $n'$ execuções encontradas para tal configuração.

\begin{equation}
\label{eqn:configScores}
	S'_F = \frac{\sum_{i=1}^{n'} S_{F_i}}{n'}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
	S'_E = \frac{\sum_{i=1}^{n'} S_{E_i}}{n'}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
	S' = \frac{\sum_{i=1}^{n'} S_i}{n'}
\end{equation}

Para facilitar a visualização das pontuações pelo Cientista de Dados, multiplica-se as pontuações $S'_E$, $S'_F$ e $S'$ por um fator arbitrário $X = 1000$ e arredonda-se em seguida, conforme exibido na equação \ref{eqn:roundedScore}.

\begin{equation}
\label{eqn:roundedScore}
	S'_F = \round*{X \times \frac{\sum_{i=1}^{n'} S_{F_i}}{n'}}\;\;\;\;\;\;\;\;\;\;
	S'_E = \round*{X \times \frac{\sum_{i=1}^{n'} S_{E_i}}{n'}}\;\;\;\;\;\;\;\;\;\;
	S' = \round*{X \times \frac{\sum_{i=1}^{n'} S_i}{n'}}
\end{equation}

\subsection{Planejador}

Para o Planejador, foram desenvolvidas as seguintes estratégias, cujo cada funcionamento é explicado abaixo, para a definição da melhor configuração arquitetural a ser executada pelo Módulo de ML em determinado contexto:

\begin{itemize}
\item \textbf{Filtragem de algoritmos:} Alguns algoritmos podem estar mal implementados ou seus modelos podem estar com métricas que necessitam uma melhor análise do Cientista de Dados para serem consideradas confiáveis. Para isso não acontecer, é possível realizar um filtro de acordo com as combinações de algoritmos consideradas confiáveis antes de selecionar os modelos ideais.
\item \textbf{Limiar de resultados da análise:} Pelo mesmo motivo da estratégia anterior, métricas não confiáveis significam que podem existir distorções na pontuação final determinada pelo Analisador. Para esse caso, foi criado um limiar de pontuação mínimo e máximo para determinar pontuações que podem ser consideradas confiáveis para avaliação.
\item \textbf{Obtenção das melhores configurações arquiteturais para execução:} As configurações arquiteturais que não foram filtradas nas estratégias anteriores são selecionadas de acordo com as maiores pontuações e são selecionados os parâmetros necessários para executar o Módulo de ML.
\end{itemize}

O Planejador verifica as configurações de cada estratégia e as executa. Após a execução de todas as estratégias, os parâmetros selecionados são passados para o Executor.

\subsection{Executor}

Para o Executor, os parâmetros selecionados na fase de planejamento são configurados como parâmetros para a execução do Módulo de ML, que executa os procedimentos detalhados na seção \ref{sec:ModuloML}.

\section{Atuação do Arcabouço Autonômico dentro do processo}

Com base nas etapas da AI Reference Architecture~\cite{IBM_2021}, foi desenhado um diagrama de atividades, presente na Figura \ref{fig:AIRoles}, determinando como o Arcabouço Autonômico Proposto se encaixaria no processo. É possível definir os seguintes papéis presentes no desenvolvimento de aplicações de \textit{Machine Learning} e como se encaixariam em um contexto onde o arcabouço se faria presente:

\begin{itemize}
\item \textbf{Especialista de Domínio:} É a pessoa que detém o conhecimento de todo o conjunto de regras do qual a aplicação deve respeitar. Seu conhecimento é atuante na comunicação das regras com os demais papéis, independente de seu conhecimento técnico. Está presente nas fases de Coleta, Organização e Infusão do ciclo.

\item \textbf{Engenheiro de Dados:} É a pessoa responsável pelos processos de coleta e transformação dos dados para o uso em outros processos, sejam eles de Softwares tradicionais ou aplicações de \textit{Machine Learning}. Pode aplicar processos de governança antes de definir que o dado esteja pronto para ser usado por outras pessoas. Embora não esteja colocado nas fases atuantes do Arcabouço Autonômico Proposto, sua participação é vital para que o arcabouço funcione corretamente, uma vez que os dados manipulados por ele são transportados para a Base de Conhecimento do mesmo. Está presente nas fases de Coleta e Organização do ciclo.

\item \textbf{Cientista de Dados:} É a pessoa responsável pela análise dos dados e do desenvolvimento do processo de \textit{Machine Learning} após a transformação e tratamento dos dados. Pode realizar tratamentos próprios antes do treinamento, como \textit{Encoding} (\textit{Label Encoding}/\textit{One-hot Encoding}), normalização, processos de regularização como aumentação de dados, para melhorar a performance do mesmo. Por esta gama de funções, o Arcabouço Autonômico Proposto atua exatamente nas fases onde o mesmo atua, sendo parte vital do seu uso. Está presente nas fases de Organização e Análise do ciclo.

\item \textbf{Engenheiro de Software:} É a pessoa responsável por usar o modelo de Machine Learning obtido na fase de Análise em aplicações que façam sentido para o uso do mesmo, como assistentes, automações, dashboards e relatórios. Está presente apenas na fase de Infusão, mas pode ser considerado na fase de Análise para verificar com o Cientista de Dados como está o andamento dos modelos desenvolvidos e desenhar alternativas caso os mesmos não estejam prontos para uso.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{images/Diagrama_Atividades.jpg}
\caption {Diagrama de atividades, com base na IBM AI Reference Architecture~\cite{IBM_2021}, indicando a subdivisão de cada papel no uso do Arcabouço Autonômico}
\label{fig:AIRoles}
\end{figure}

\chapter{Implementação do Arcabouço}

Este capítulo apresenta a ferramenta FairPEK que implementa a solução descrita no Capítulo~\ref{sec:metodologia}. A ferramenta utiliza um arcabouço cliente-servidor sendo que a seção~\ref{sec:VisaoGeral} descreve brevemente detalhes de implementação presentes nos módulos apresentados e a seção~\ref{sec:interface} descreve a Interface Humano-Computador implementada, referente ao módulo de Interface.

\section{Visão geral da implementação}
\label{sec:VisaoGeral}

O Módulo de ML, o Gerenciador Autonômico e o módulo de Engenharia de Dados foram desenvolvidos em Python pelo motivo de que grande parte dos algoritmos e bibliotecas utilizados já estão implementados com essa linguagem. Para a execução dos algoritmos com redução de viés no Módulo de ML, é utilizada a biblioteca AI Fairness 360, ou AIF360~\cite{AIF360_2022}, biblioteca da IBM que compila diversos algoritmos para este fim e facilita o cálculo das métricas de Fairness. Para os algoritmos sem redução de viés, é usado o scikit-learn~\cite{scikit_2022}. Um pequeno \textit{framework} também foi desenvolvido no Módulo de ML para facilitar o desenvolvimento com o padrão de arquitetura \textit{Pipes-and-Filters}. O Gerenciador Autonômico e o módulo de Engenharia de Dados usam a biblioteca Pandas~\cite{Pandas_2023} para auxiliar nos processos de transformação de dados e na organização dos dados presentes na Base de Conhecimento.

Durante o desenvolvimento do Módulo de ML e do Gerenciador Autonômico, foi notado que o número de configurações e a complexidade das mesmas era muito grande, ocasionando problemas na hora de documentar e detalhar todo o processo executado. Para facilitar tais configurações, foi criada uma Interface Humano-Computador onde é condensada toda a organização dos parâmetros, dos arquivos utilizados e da realização das execuções simples e autônoma do Módulo de ML. Também foi criado um Backend dentro do Gerenciador Autonômico para fazer a intermediação entre a interface e o mesmo.

O Backend também foi desenvolvido em Python para reusar códigos já desenvolvidos em etapas anteriores, usando o framework Flask~\cite{Flask_2023} para construir as requisições web e foi dividido em 3 camadas. A camada \textit{web} corresponde às requisições que constroem a ponte entre Frontend e Backend, a camada \textit{service} corresponde às funcionalidades e casos de uso que serão chamados pelas requisições, e a camada \textit{repo} corresponde às operações de leitura e escrita que serão realizadas nos arquivos do Módulo de ML.

Após a implementação e durante a elaboração dos experimentos, foi notado a necessidade de desenvolver uma documentação, presente nos Apêndices \ref{ann:DocInstall} e \ref{ann:DocMain}. Nela estão explicadas as tecnologias utilizadas para instalação e execução do sistema, além de passos para manutenção caso novos algoritmos e conjuntos de dados sejam adicionados a implementação do arcabouço.

\section{Interface}
\label{sec:interface}

A interface foi desenvolvida em JavaScript, aproveitando a facilidade e robustez dessa linguagem, juntamente com o extenso conjunto de ferramentas disponíveis para facilitar o desenvolvimento. Também foram utilizadas as bibliotecas React~\cite{React_2023} e Redux~\cite{Redux_2023}, além da biblioteca de componentes Material UI~\cite{MaterialUI_2023}. A escolha da Material UI proporciona economia de tempo ao aproveitar componentes prontos, enquanto oferece um \textit{look-and-feel} semelhante aos aplicativos do Sistema Operacional Android, seguindo as diretrizes do Material Design desenvolvido pelo Google. Isso é particularmente relevante, uma vez que o foco deste trabalho não demanda componentes específicos para a interface.

A interface foi nomeada de FairPEK, junção dos termos Fairness e MAPE-K, e possui os seguintes detalhes:

\subsubsection{Opções do Menu}

Conforme ilustrado na Figura \ref{fig:opcoesMenu}, o menu pode ser expandido e recolhido, onde suas opções são selecionáveis independente da configuração, e foram colocados ícones ao lado do nome de sua opção para que a localização das opções seja acessível mesmo com o menu recolhido.

\begin{figure}[H]
    \centering
    \subfigure[fig:opcoesMenu1][Opções de seleção de menu expandidas.]{\includegraphics[scale=0.35]{images/front/Menu.png}}
    \subfigure[fig:opcoesMenu2][Opções de seleção de menu recolhidas.]{\includegraphics[scale=0.35]{images/front/Metricas-Performance.png}}
    \caption{Comportamento das opções de menu.}
    \label{fig:opcoesMenu}
\end{figure}

No menu, as opções são divididas em Configuração e Execução. Em Configuração, há as opções Análise e Planejamento relativas às configurações presentes no Gerenciador Autonômico. Em Execução, há as opções Pipeline Manual e Pipeline Autônomo relativas às maneiras de como realizar uma execução do Módulo de ML.

\subsection{Opções para parametrização do Gerenciador Autonômico}

\subsubsection{Configurações para Análise}

Ao clicar a opção do menu "Análise", é exibida a tela ilustrada na Figura \ref{fig:configMetricas}. Ela é dividida em duas abas: Métricas de Avaliação e Métricas de Fairness, relativas aos grupos de métricas divididos no Gerenciador Autonômico. Em ambas as abas, há os campos de peso para avaliação, que simboliza o peso no cálculo da pontuação final, e o campo de métricas para uso, que determina quais métricas serão utilizadas para o cálculo da pontuação de cada grupo. Uma vez que há apenas duas abas, a alteração do campo de peso para avaliação de uma aba automaticamente alterará o campo de peso para avaliação da outra aba para complementar a soma de 100\% sem precisar de validações.

\begin{figure}[H]
    \centering
    \subfigure[fig:configMetricas1][Configuração para Métricas de Avaliação.]{\includegraphics[scale=0.35]{images/front/Metricas-Performance.png}}
    \subfigure[fig:configMetricas2][Configuração para Métricas de Fairness.]{\includegraphics[scale=0.35]{images/front/Metricas-Fairness.png}}
    \caption{Configuração das métricas para o Analisador do Gerenciador Autonômico.}
    \label{fig:configMetricas}
\end{figure}

A alteração do campo de métricas para uso pode resultar em uma mudança no número de métricas para as quais os pesos devem ser ajustados. Dado que é possível ter mais de duas métricas, a tarefa de assegurar automaticamente que a soma total das métricas atinja 100\% torna-se mais complexa, sendo substituída por um processo de validação conforme ilustrado na Figura \ref{fig:cenariosMetricas}. Ao clicar no botão "Salvar"~localizado no canto superior direito, é realizada a validação da soma de todas as métricas e, em caso positivo, chamada uma requisição que salva o arquivo com as opções selecionadas e seus respectivos valores. Após este procedimento, será exibida uma indicação de sucesso ou erro de validação no canto inferior esquerdo indicando se o arquivo foi salvo ou não.

\begin{figure}[H]
    \centering
    \subfigure[fig:cenariosMetricas1][Erros de validação ao concluir a operação.]{\includegraphics[scale=0.35]{images/front/Metricas-Erro.png}}
    \subfigure[fig:cenariosMetricas2][Indicação de sucesso da operação.]{\includegraphics[scale=0.35]{images/front/Metricas-Sucesso.png}}
    \caption{Cenários possíveis na configuração das métricas.}
    \label{fig:cenariosMetricas}
\end{figure}

\subsubsection{Configurações para Planejamento}

Ao clicar a opção do menu "Planejamento", é exibida a tela ilustrada na Figura \ref{fig:configPlanejamento}. Ela possui três opções, relativas às estratégias desenvolvidas para o Planejador do Gerenciador Autonômico. Ao clicar no botão "Salvar"~localizado no canto superior direito, são chamadas requisições que salvam os arquivos necessários e é exibida uma indicação de sucesso no canto inferior esquerdo.

\begin{figure}[H]
    \centering
    \subfigure[fig:configPlanejamento1][Opções para configurar o Planejador do Gerenciador Autonômico.]{\includegraphics[scale=0.35]{images/front/Planejamento.png}}
    \subfigure[fig:configPlanejamento2][Indicação de sucesso da operação.]{\includegraphics[scale=0.35]{images/front/Planejamento-Sucesso.png}}
    \caption{Configuração do Planejador do Gerenciador Autonômico.}
    \label{fig:configPlanejamento}
\end{figure}

Se a estratégia de restrição de algoritmos for selecionada, é exibido um submenu contendo as configurações arquiteturais que podem ser selecionadas, e que são salvas em um arquivo separado para serem filtrados na etapa de planejamento. Se a estratégia de restrição por um limiar de pontuação for selecionada, é exibido um slider com uma pontuação mínima e uma pontuação máxima, e ambas as pontuações são salvas em um arquivo separado para serem filtrados na etapa de planejamento.

\subsection{Opções para execução do Módulo de ML}

\subsubsection{Execução manual}

Ao clicar a opção do menu "Pipeline Manual", é exibida a tela ilustrada na Figura \ref{fig:pipelineManual}. Ela possui indicações de etapas divididas em Parametrização, Execução e Resultados, campos para selecionar o conjunto de dados e o atributo protegido e opções para selecionar onde a redução de viés será executada. Dependendo da opção selecionada, aparecem campos para selecionar o algoritmo de treinamento e o algoritmo de redução de viés. Ao clicar no botão "Executar"~localizado no canto superior direito, é feita uma requisição para executar o Módulo de ML com as opções selecionadas, é exibida uma indicação de sucesso no canto inferior esquerdo e a indicação de etapa é atualizada para a etapa de execução.

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineManual1][Opções para executar o Módulo de ML manualmente.]{\includegraphics[scale=0.35]{images/front/Pipeline-Manual-Parametros.png}}
    \subfigure[fig:pipelineManual2][Módulo de ML em execução.]{\includegraphics[scale=0.3575
    ]{images/front/Pipeline-Manual-Execucao.png}}
    \caption{Execução simples e manual do Módulo de ML.}
    \label{fig:pipelineManual}
\end{figure}

Após a execução ser concluída,  a indicação de etapa é atualizada para a etapa de resultados, conforme ilustração nas Figuras \ref{fig:pipelineManualInfo} e \ref{fig:pipelineManualMetricas}. Nela, os parâmetros gravados são organizados em 4 grupos: Execução, relativos aos parâmetros utilizados e estatísticas da execução, Métricas de Avaliação, relativas aos resultados das métricas de avaliação, Métricas de Fairness, relativas aos resultados das métricas de Fairness, e Pontuação, relativas ao cálculo realizado com as configurações utilizadas na parte de métricas.

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineManual1][Exibição dos parâmetros no resultado.]{\includegraphics[scale=0.4]{images/front/Pipeline-Manual-Resultado-Parametros.png}}
    \subfigure[fig:pipelineManual2][Exibição das pontuações no resultado.]{\includegraphics[scale=0.4]{images/front/Pipeline-Manual-Resultado-Pontuacao.png}}
    \caption{Informações do resultado do Módulo de ML.}
    \label{fig:pipelineManualInfo}
\end{figure}

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineManual1][Exibição das métricas de avaliação no resultado.]{\includegraphics[scale=0.35]{images/front/Pipeline-Manual-Resultado-Metricas-Performance.png}}
    \subfigure[fig:pipelineManual2][Exibição das métricas de fairness no resultado.]{\includegraphics[scale=0.35]{images/front/Pipeline-Manual-Resultado-Metricas-Fairness.png}}
    \caption{Métricas do resultado do Módulo de ML.}
    \label{fig:pipelineManualMetricas}
\end{figure}

\subsubsection{Execução autônoma}

Ao clicar a opção do menu "Pipeline Autônomo", é exibida a tela ilustrada na Figura \ref{fig:pipelineAutonomo}. Ela possui indicações de etapas divididas em Parametrização, Análise, Opções, Execução e Resultados e um campo para selecionar o conjunto de dados. Ao clicar no botão "Executar"~localizado no canto superior direito, é feita uma requisição para escolher o melhor conjunto de parâmetros baseado em execuções anteriores, é exibida uma indicação de sucesso no canto inferior esquerdo e a indicação de etapa é atualizada para a etapa de análise.

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineAutonomo1][Opções para configurar o Módulo de ML de forma autônoma.]{\includegraphics[scale=0.35]{images/front/Pipeline-Autonomo-Parametros.png}}
    \subfigure[fig:pipelineAutonomo2][Análise do Gerenciador Autonômico em execução.]{\includegraphics[scale=0.4]{images/front/Pipeline-Autonomo-Analise.png}}
    \caption{Execução autônoma do Módulo de ML gerenciada pelo Gerenciador Autonômico.}
    \label{fig:pipelineAutonomo}
\end{figure}

Após a etapa de análise ser concluída, a indicação de etapa é atualizada para a etapa de opções, conforme ilustração na Figura \ref{fig:pipelineAutonomoSelecao}. Nela, os parâmetros sugeridos são organizados nos mesmos grupos presentes nas Figuras \ref{fig:pipelineManualInfo} e \ref{fig:pipelineManualMetricas} e podem ser consultados para selecionar a melhor escolha possível das 5 melhores sugestões de acordo com a pontuação calculada, podendo contestar ou não a melhor escolha sugerida pelo Gerenciador Autonômico.

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineAutonomoSelecao1][Opções para seleção da configuração arquitetural para execução.]{\includegraphics[scale=0.35]{images/front/Pipeline-Autonomo-Selecao.png}}
    \subfigure[fig:pipelineAutonomoSelecao2][Parâmetros a serem utilizados para execução.]{\includegraphics[scale=0.35]{images/front/Pipeline-Autonomo-Selecao-Parametros.png}}
    \caption{Seleção da configuração arquitetural após análise.}
    \label{fig:pipelineAutonomoSelecao}
\end{figure}

Ao clicar novamente no botão "Executar"~localizado no canto superior direito, é feita uma requisição para executar o Módulo de ML com a opção selecionada, e a indicação de etapa é atualizada para a etapa de execução. Após a execução ser concluída,  a indicação de etapa é atualizada para a etapa de resultados, conforme ilustração na Figura \ref{fig:pipelineAutonomoResultado}. Nela, os parâmetros sugeridos são organizados nos mesmos grupos presentes na Figura \ref{fig:pipelineAutonomoSelecao}, mas desta vez refletem as métricas e pontuação da execução realizada pelo Módulo de ML.

\begin{figure}[H]
    \centering
    \subfigure[fig:pipelineAutonomoResultado1][Módulo de ML em execução.]{\includegraphics[scale=0.375]{images/front/Pipeline-Autonomo-Execucao.png}}
    \subfigure[fig:pipelineAutonomoResultado2][Resultados da execução realizada.]{\includegraphics[scale=0.35]{images/front/Pipeline-Autonomo-Resultado.png}}
    \caption{Execução do Módulo de ML após seleção.}
    \label{fig:pipelineAutonomoResultado}
\end{figure}

Após a implementação de todos os módulos presentes no arcabouço, foi possível realizar a execução de 680 treinamentos para armazenamento na base de conhecimento e obter a medição da pontuação de todas as configurações arquiteturais calculadas pelo Gerenciador Autonômico. Com isso, foi possível elaborar Estudos de Caso para verificar a viabilidade da solução proposta e a escolha das arquiteturas utilizadas.

\chapter{Estudo de Caso 1: Classificação de Crédito (German Credit Dataset)}

Este capítulo apresenta um Estudo de Caso executado pelo autor para verificar a viabilidade da solução proposta. Nele, ela foi usada para construir um modelo de classificação de crédito utilizando o conjunto de dados German Credit Dataset, que é um conjunto de dados de classificação binária com 1000 amostras. O conjunto de dados inclui variáveis como idade, emprego, histórico de crédito e renda. Os resultados obtidos e as discussões sobre esses resultados são analisados, permitindo verificar se a solução consegue extrair um equilíbrio entre qualidade e justiça.

\section{Contexto e limitações}

Neste Estudo de Caso, o real foco foi colocado em testar e verificar como o Gerenciador Autonômico se comporta em um cenário padrão de uso, com diversas execuções prévias do Módulo de ML. Como execuções anteriores estão armazenadas na Base de Conhecimento, o Gerenciador Autonômico pode executar uma análise através destes dados e determinar um plano indicando as configurações arquiteturais dos processos de treinamento de um modelo com melhores resultados para um determinado conjunto de dados. O Estudo de Caso foi realizado com o seguinte contexto:

\begin{itemize}
\item \textbf{Contexto:} Obter classificação de crédito de uma pessoa (apto ou não apto), por meio de uma série de \textit{features}.

\item \textbf{Conjunto de dados:} German Credit Dataset~\cite{ucigerman_2021}.

\item \textbf{Transformações realizadas no conjunto de dados:} Mudanças nos valores dos atributos para valores de interpretação mais simples, para que o Cientista de Dados possa categorizar o dado de forma mais fácil no Módulo de ML.

\item \textbf{Atributos protegidos:} Idade ou Nacionalidade

\item \textbf{Grupo privilegiado:} Idade: Maior ou igual a 25 anos; Nacionalidade: Alemã

\item \textbf{Grupo não-privilegiado:} Idade: Menor que 25 anos; Nacionalidade: Diferente da Alemã (Estrangeiro)

\end{itemize}

Para realizar este experimento, foi considerado o seguinte objetivo e consideradas as seguintes limitações:

\begin{itemize}
\item \textbf{Experimento:} Execução do Módulo de ML para obtenção de dados iniciais na base de conhecimento e discussão do Gerenciador Autonômico como facilitador da escolha das configurações arquiteturais para execução.

\item \textbf{Medição:} Serão coletadas as pontuações de cada grupo de métricas (Avaliação e Fairness) após uma execução do Gerenciador Autonômico, cujos cálculos foram explicados anteriormente na seção \ref{sec:GerenciadorAutonomico}. Tais pontuações também serão comparadas com os valores originais, presentes na Base de Conhecimento, para verificar a validade da pontuação.

\item \textbf{Obtenção dos dados:} A execução do Módulo de ML utilizando o Gerenciador Autonômico foi realizada com 3 pesagens diferentes na pontuação geral:

\begin{itemize}
\item 50\% para métricas de Avaliação e 50\% para métricas de Fairness, para uma configuração equilibrada.
\item 75\% para métricas de Avaliação e 25\% para métricas de Fairness, para uma configuração que prioriza a qualidade em detrimento da justiça.
\item 25\% para métricas de Avaliação e 75\% para métricas de Fairness, para uma configuração que prioriza a justiça em detrimento da qualidade.
\end{itemize}

São utilizadas as métricas Acurácia, Precisão, \textit{Recall}, \textit{F1-Score} e AUC como métricas de Avaliação e as métricas \textit{Statistical Parity Difference}, \textit{Equal Opportunity Difference}, \textit{Average Odds Difference}, \textit{Disparate Impact} e \textit{Theil Index} como métricas de \textit{Fairness}, todas com pesagens iguais em seus respectivos grupos.

\item \textbf{Pré-condição 1:} Execuções anteriores foram realizadas e já formaram uma Base de Conhecimento. Para o experimento, foi considerado ao menos 1 execução para cada configuração arquitetural, composta por conjunto de dados, atributo protegido e algoritmos utilizados nos componentes do Módulo de ML (Pré-processamento, Processamento/Treinamento e Pós-processamento/Validação).

\item \textbf{Pré-condição 2:} Cada execução do Módulo de ML gravada na Base de Conhecimento gerada para uso do Gerenciador Autonômico terá todas as métricas de Avaliação e métricas de \textit{Fairness} mencionadas, além de sua configuração arquitetural utilizada.

\item \textbf{Pré-condição 3:} Podem existir ruídos nos resultados finais devido ao German Credit Dataset ser uma base de dados com poucas amostras (apenas 1000), mas eles serão desconsiderados uma vez que o conjunto de dados ainda é considerado como \textit{benchmark} em alguns trabalhos acadêmicos \cite{Kamiran_2011}~\cite{Feldman_2015}~\cite{Celis_2019} e seus dados exemplificam muito bem uma situação real onde dados sensíveis podem ser utilizados e são possíveis de afetar a decisão de um modelo e, consequentemente, a situação de vida de uma pessoa.

\item \textbf{Restrição 1:} O algoritmo para redução de vieses é executado em apenas um dos componentes (Pré-processamento, Processamento/Treinamento ou Pós-""processamento""/Validação). Isto foi decidido pois não foram encontradas referências onde a aplicação desses algortimos em duas ou em todas as três etapas impacta em melhora nas métricas de \textit{Fairness}.

\item \textbf{Restrição 2:} Por não conseguir rodar com sucesso, foram removidas as configurações arquiteturais que rodaram o algoritmo \textit{Optimized Preprocessing}.

\item \textbf{Restrição 3:} Configurações arquiteturais também foram removidas da análise do Gerenciador Autonômico por apresentarem métricas com valores máximos, para evitar análises caso exista alguma falha não detectada na implementação. Como exemplo, as que rodaram o algoritmo \textit{Reject Option Classification}.

\item \textbf{Restrição 4:} Para evitar configurações arquiteturais com métricas ruins e também pelo mesmo motivo da restrição anterior, o intervalo de pontuação para análise foi limitado de 500 a 950.
\end{itemize}

\section{Resultados e Discussões}

Os resultados baseados nas pré-condições e restrições já comentadas na seção anterior estão presentes abaixo nas Tabelas \ref{tbl:ScoreMAPEKGeral5050}, \ref{tbl:ScoreMAPEKGeral7525} e \ref{tbl:ScoreMAPEKGeral2575}:

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKGeral5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Regressão Logística & Equalized Odds & 968 & 860 & \textbf{914} \\
Nacionalidade & Nenhum & Random Forest & Calibrated Equalized Odds & 902 & 922 & \textbf{912} \\
Nacionalidade & Nenhum & Gradient Boosting & Calibrated Equalized Odds & 870 & 925 & \textbf{898} \\
Idade & Nenhum & Gradient Boosting & Equalized Odds & 927 & 862 & \textbf{894} \\
Idade & Reweighing & Gradient Boosting & Nenhum & 804 & 931 & \textbf{868} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKGeral7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Regressão Logística & Equalized Odds & 968 & 860 & \textbf{941} \\
Idade & Nenhum & Gradient Boosting & Equalized Odds & 927 & 862 & \textbf{910} \\
Nacionalidade & Nenhum & Random Forest & Calibrated Equalized Odds & 902 & 922 & \textbf{907} \\
Nacionalidade & Nenhum & Gradient Boosting & Calibrated Equalized Odds & 870 & 925 & \textbf{883} \\
Idade & Nenhum & Random Forest & Equalized Odds & 898 & 799 & \textbf{874} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKGeral2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{928} \\
Nacionalidade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{928} \\
Idade & Nenhum & Adversarial Debiasing & Nenhum & 742 & 979 & \textbf{920} \\
Nacionalidade & Reweighing & Support Vector Machines & Nenhum & 755 & 972 & \textbf{918} \\
Nacionalidade & Learning Fair Representations & Support Vector Machines & Nenhum & 755 & 972 & \textbf{918} \\
\end{tabular}}
\end{center}
\end{table}

Nessas execuções, 2 observações são destacadas. A primeira é o fato da predominância de algoritmos com redução de viés para o componente de pós-processamento especialmente em configurações que priorizavam qualidade, contrariando o esperado de que os algoritmos com redução de viés aumentavam justiça em detrimento da qualidade. A segunda é a predominância de algoritmos com redução de viés para o componente de pré-processamento em configurações que priorizavam justiça, principalmente pois todos as execuções usavam \textit{Support Vector Machines} como algoritmo de treinamento.

Diante destas 2 predominâncias envolvendo todas as configurações arquiteturais executadas, novos experimentos com restrições adicionais foram realizados para obter observações mais detalhadas a respeito dos resultados:

\begin{itemize}
\item Uso apenas de configurações arquiteturais com o uso de algoritmos com redução de viés para o componente de \mbox{pré-processamento}.
\item Uso apenas de configurações arquiteturais com o uso de algoritmos com redução de viés para o componente de \mbox{processamento}.
\item Uso apenas de configurações arquiteturais com o uso de algoritmos com redução de viés para o componente de \mbox{pós-processamento}.
\item Uso apenas de configurações arquiteturais sem o uso de algoritmos com redução de viés.
\end{itemize}

As pré-condições, restrições e pesagens nas pontuações usadas anteriormente foram mantidas e seus resultados estão presentes abaixo nas Tabelas \ref{tbl:ScoreMAPEKPreproc5050}, \ref{tbl:ScoreMAPEKPreproc7525}, \ref{tbl:ScoreMAPEKPreproc2575}, \ref{tbl:ScoreMAPEKInproc5050}, \ref{tbl:ScoreMAPEKInproc7525}, \ref{tbl:ScoreMAPEKInproc2575}, \ref{tbl:ScoreMAPEKPostproc5050}, \ref{tbl:ScoreMAPEKPostproc7525}, \ref{tbl:ScoreMAPEKPostproc2575}, \ref{tbl:ScoreMAPEKNoproc5050}, \ref{tbl:ScoreMAPEKNoproc7525} e \ref{tbl:ScoreMAPEKNoproc2575}:

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pré-processamento - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPreproc5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Reweighing & Gradient Boosting & Nenhum & 804 & 931 & \textbf{868} \\
Idade & Learning Fair Representations & Gradient Boosting & Nenhum & 804 & 931 & \textbf{868} \\
Idade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{868} \\
Nacionalidade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{868} \\
Nacionalidade & Learning Fair Representations & Support Vector Machines & Nenhum & 755 & 972 & \textbf{864} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pré-processamento - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPreproc7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Reweighing & Gradient Boosting & Nenhum & 804 & 931 & \textbf{836} \\
Idade & Learning Fair Representations & Gradient Boosting & Nenhum & 804 & 931 & \textbf{836} \\
Nacionalidade & Learning Fair Representations & Gradient Boosting & Nenhum & 811 & 878 & \textbf{828} \\
Nacionalidade & Reweighing & Gradient Boosting & Nenhum & 811 & 878 & \textbf{828} \\
Nacionalidade & Learning Fair Representations & Random Forest & Nenhum & 801 & 883 & \textbf{821} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pré-processamento - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPreproc2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{928} \\
Nacionalidade & Disparate Impact Remover & Support Vector Machines & Nenhum & 747 & 989 & \textbf{928} \\
Nacionalidade & Learning Fair Representations & Support Vector Machines & Nenhum & 755 & 972 & \textbf{918} \\
Nacionalidade & Reweighing & Support Vector Machines & Nenhum & 755 & 972 & \textbf{918} \\
Idade & Learning Fair Representations & Support Vector Machines & Nenhum & 755 & 969 & \textbf{916} \\
\end{tabular}}
\end{center}
\end{table}

Nas configurações arquiteturais utilizando apenas algoritmos para redução de viés de pré-processamento, percebe-se a predominância dos algoritmos de treinamento \textit{Gradient Boosting} e \textit{Support Vector Machines}, sendo o \textit{Gradient Boosting} predominante em configurações priorizando qualidade e o \textit{Support Vector Machines} predominante em configurações priorizando justiça, o que começa a explicar a sua predominância também presente no resultado geral. Também é possível perceber mais 2 observações: A primeira observação é que a análise de apenas uma categoria de algoritmos dá mais clareza em ver como o cálculo utilizado nas 3 configurações faz com que o equilíbrio de ambas as métricas se torna mais importante do que a prioridade apenas em qualidade ou apenas em justiça, uma vez que há exemplos de conjuntos de algoritmos com pontuações ligeiramente maiores em Avaliação que acabaram sendo pior avaliados pois a pontuação em \textit{Fairness} está bem menor, e vice-versa. A segunda observação é que o uso de um atributo protegido diferente (e, por consequência, com tratamento de dados diferente) e de um algoritmo de treinamento parecem impactar tanto quanto ou até mais que o próprio algoritmo com redução de viés no dado.

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de processamento - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKInproc5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Adversarial Debiasing & Nenhum & 742 & 979 & \textbf{860} \\
Nacionalidade & Nenhum & Grid Search Reduction & Nenhum & 789 & 895 & \textbf{845} \\
Idade & Nenhum & Meta Fair Classifier & Nenhum & 776 & 910 & \textbf{843} \\
Idade & Nenhum & Exponentiated Gradient Reduction & Nenhum & 811 & 869 & \textbf{840} \\
Nacionalidade & Nenhum & Rich Subgroup Fairness & Nenhum & 791 & 856 & \textbf{824} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de processamento - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKInproc7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Exponentiated Gradient Reduction & Nenhum & 811 & 869 & \textbf{826} \\
Nacionalidade & Nenhum & Grid Search Reduction & Nenhum & 789 & 895 & \textbf{820} \\
Idade & Nenhum & Meta Fair Classifier & Nenhum & 776 & 910 & \textbf{809} \\
Nacionalidade & Nenhum & Exponentiated Gradient Reduction & Nenhum & 807 & 810 & \textbf{808} \\
Nacionalidade & Nenhum & Rich Subgroup Fairness & Nenhum & 791 & 856 & \textbf{807} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de processamento - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKInproc2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Adversarial Debiasing & Nenhum & 742 & 979 & \textbf{920} \\
Idade & Nenhum & Meta Fair Classifier & Nenhum & 776 & 910 & \textbf{876} \\
Nacionalidade & Nenhum & Grid Search Reduction & Nenhum & 795 & 895 & \textbf{870} \\
Idade & Nenhum & Exponentiated Gradient Reduction & Nenhum & 811 & 869 & \textbf{854} \\
Nacionalidade & Nenhum & Prejudice Remover & Nenhum & 770 & 874 & \textbf{848} \\
\end{tabular}}
\end{center}
\end{table}

Nas configurações arquiteturais utilizando apenas algoritmos para redução de viés de processamento, percebe-se uma variedade maior nos algoritmos, até porque não há usos de redução de viés em um pré ou um pós-processamento, com destaque para o \textit{Adversarial Debiasing} que foi bem avaliado pela pontuação alta em \textit{Fairness}. Nelas, as 2 observações percebidas em configurações arquiteturais utilizando apenas algoritmos com redução de viés no dado são reforçadas por uma maior variedade de pontuações e pelo algoritmo \textit{Exponentiated Gradient Reduction} com 2 exemplos diferentes na Tabela \ref{tbl:ScoreMAPEKInproc7525}, onde o uso da Nacionalidade como atributo protegido possui pontuações de Avaliação e \textit{Fairness} piores que a Idade e concluindo que o processamento utilizado no atributo protegido pode afetar todas as métricas.

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pós-processamento - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPostproc5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Regressão Logística & Equalized Odds & 968 & 860 & \textbf{914} \\
Nacionalidade & Nenhum & Random Forest & Calibrated Equalized Odds & 902 & 922 & \textbf{912} \\
Nacionalidade & Nenhum & Gradient Boosting & Calibrated Equalized Odds & 870 & 925 & \textbf{898} \\
Idade & Nenhum & Gradient Boosting & Equalized Odds & 927 & 862 & \textbf{894} \\
Idade & Nenhum & Random Forest & Equalized Odds & 898 & 799 & \textbf{849} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pós-processamento - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPostproc7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Idade & Nenhum & Regressão Logística & Equalized Odds & 968 & 860 & \textbf{941} \\
Idade & Nenhum & Gradient Boosting & Equalized Odds & 927 & 862 & \textbf{910} \\
Nacionalidade & Nenhum & Random Forest & Calibrated Equalized Odds & 902 & 922 & \textbf{907} \\
Nacionalidade & Nenhum & Gradient Boosting & Calibrated Equalized Odds & 870 & 925 & \textbf{883} \\
Idade & Nenhum & Random Forest & Equalized Odds & 898 & 799 & \textbf{874} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Algoritmos para redução de viés de pós-processamento - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKPostproc2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Nacionalidade & Nenhum & Random Forest & Calibrated Equalized Odds & 902 & 922 & \textbf{917} \\
Nacionalidade & Nenhum & Gradient Boosting & Calibrated Equalized Odds & 870 & 925 & \textbf{911} \\
Idade & Nenhum & Regressão Logística & Equalized Odds & 968 & 860 & \textbf{887} \\
Idade & Nenhum & Gradient Boosting & Equalized Odds & 927 & 862 & \textbf{878} \\
Idade & Nenhum & Random Forest & Equalized Odds & 898 & 799 & \textbf{824} \\
\end{tabular}}
\end{center}
\end{table}

Nas configurações arquiteturais utilizando apenas algoritmos para redução de viés de pós-processamento, surpreende o fato de que as configurações arquiteturais obtiveram as melhores pontuações em Avaliação e as piores pontuações em \textit{Fairness}, podendo indicar uma característica dos algoritmos \textit{Equalized Odds} e \textit{Calibrated Equalized Odds}. Entretanto, por conta da grande melhora por parte das métricas de Avaliação, tais configurações arquiteturais possuem um maior equilíbrio entre Avaliação e \textit{Fairness} e acabam garantindo maiores pontuações na média, justificando as melhores pontuações nas primeiras execuções onde foram considerados Uso dos algoritmos implementados. Fora isto, as demais observações anteriores também se aplicam nestas execuções.

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Sem uso de algoritmos para redução de viés - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKNoproc5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Nacionalidade & Nenhum & Support Vector Machines & Nenhum & 755 & 972 & \textbf{864} \\
Idade & Nenhum & Support Vector Machines & Nenhum & 755 & 969 & \textbf{862} \\
Nacionalidade & Nenhum & Random Forest & Nenhum & 802 & 885 & \textbf{844} \\
Nacionalidade & Nenhum & Regressão Logística & Nenhum & 782 & 865 & \textbf{824} \\
Nacionalidade & Nenhum & Gradient Boosting & Nenhum & 817 & 784 & \textbf{800} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Sem uso de algoritmos para redução de viés - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKNoproc7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Nacionalidade & Nenhum & Random Forest & Nenhum & 802 & 885 & \textbf{823} \\
Nacionalidade & Nenhum & Gradient Boosting & Nenhum & 817 & 784 & \textbf{809} \\
Nacionalidade & Nenhum & Support Vector Machines & Nenhum & 755 & 972 & \textbf{809} \\
Idade & Nenhum & Support Vector Machines & Nenhum & 755 & 969 & \textbf{808} \\
Idade & Nenhum & Gradient Boosting & Nenhum & 817 & 778 & \textbf{807} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Sem uso de algoritmos para redução de viés - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKNoproc2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Nacionalidade & Nenhum & Support Vector Machines & Nenhum & 755 & 972 & \textbf{918} \\
Idade & Nenhum & Support Vector Machines & Nenhum & 755 & 969 & \textbf{916} \\
Nacionalidade & Nenhum & Random Forest & Nenhum & 802 & 885 & \textbf{864} \\
Nacionalidade & Nenhum & Regressão Logística & Nenhum & 782 & 865 & \textbf{844} \\
Idade & Nenhum & Regressão Logística & Nenhum & 782 & 802 & \textbf{797} \\
\end{tabular}}
\end{center}
\end{table}

Olhando as configurações arquiteturais sem algoritmos para redução de viés, é curioso notar que, ao comparar com configurações arquiteturais equivalentes mas com algoritmos usando redução de viés no dado, é possível notar que a hipótese principal se confirma em sua grande maioria: As pontuações em Avaliação são ligeiramente maiores e as pontuações em \textit{Fairness} são ligeiramente menores. É possível notar uma exceção na configuração arquitetural envolvendo o algoritmo \textit{Random Forest}, mas nos outros casos a hipótese é verificada com sucesso. Ao comparar com configurações arquiteturais usando algoritmos com redução de viés no treinamento tal hipótese também se confirma, entretanto o uso de \textit{Support Vector Machines} parece ser uma exceção a regra, implicando que o uso do algoritmo possibilita modelos mais justos para o conjunto de dados utilizado. Ao comparar com configurações arquiteturais usando algoritmos com redução de viés no resultado, a hipótese não se confirma devido a observação da grande melhora por parte das métricas de Avaliação nas configurações arquiteturais usando algoritmos com redução de viés no resultado, mas ao verificar a pontuação em \textit{Fairness} é possível notar uma ligeira melhora. Há a exceção de configurações arquiteturais envolvendo \textit{Support Vector Machines} que são melhores nas configurações arquiteturais sem algoritmos com redução de viés, mas no uso de outros algoritmos ocorre melhora na pontuação em \textit{Fairness}.

Após a verificação das pontuações, foram catalogadas todas as métricas de todos os conjuntos de algoritmos em seus valores máximo, mínimo e médio para verificar a eficácia destas pontuações, definidas nas Tabelas \ref{tbl:PerformanceMetrics} e \ref{tbl:FairnessMetrics} exibidas abaixo.

\begin{table}[H]
    \begin{center} 
        \caption{Métricas de Avaliação das execuções no Módulo de ML}
        \label{tbl:PerformanceMetrics}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
            \multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{10}{c}{Métricas} \\
            \hline
            Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & \multicolumn{2}{c|}{Acurácia} & \multicolumn{2}{c|}{Precisão} & \multicolumn{2}{c|}{Recall} & \multicolumn{2}{c|}{F1-Score} & \multicolumn{2}{c}{AUC} \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,715 & \textbf{Mínimo} & 0,7143 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,8309 & \textbf{Mínimo} & 0,5219 \\
             & & & & \textbf{Máximo} & 0,715 & \textbf{Máximo} & 0,7143 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,8309 & \textbf{Máximo} & 0,5219 \\
             & & & & \textbf{Média} & 0,715 & \textbf{Média} & 0,7143 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,8309 & \textbf{Média} & 0,5219 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,715 & \textbf{Mínimo} & 0,7143 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,8309 & \textbf{Mínimo} & 0,5219 \\
             & & & & \textbf{Máximo} & 0,715 & \textbf{Máximo} & 0,7143 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,8309 & \textbf{Máximo} & 0,5219 \\
             & & & & \textbf{Média} & 0,715 & \textbf{Média} & 0,7143 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,8309 & \textbf{Média} & 0,5219 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,76 & \textbf{Mínimo} & 0,7784 & \textbf{Mínimo} & 0,9007 & \textbf{Mínimo} & 0,8442 & \textbf{Mínimo} & 0,6474 \\
             & & & & \textbf{Máximo} & 0,79 & \textbf{Máximo} & 0,8037 & \textbf{Máximo} & 0,9291 & \textbf{Máximo} & 0,8618 & \textbf{Máximo} & 0,6933 \\
             & & & & \textbf{Média} & 0,774 & \textbf{Média} & 0,7933 & \textbf{Média} & 0,9192 & \textbf{Média} & 0,8515 & \textbf{Média} & 0,6731 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,75 & \textbf{Mínimo} & 0,7661 & \textbf{Mínimo} & 0,9291 & \textbf{Mínimo} & 0,8397 & \textbf{Mínimo} & 0,6256 \\
             & & & & \textbf{Máximo} & 0,75 & \textbf{Máximo} & 0,7661 & \textbf{Máximo} & 0,9291 & \textbf{Máximo} & 0,8397 & \textbf{Máximo} & 0,6256 \\
             & & & & \textbf{Média} & 0,75 & \textbf{Média} & 0,7661 & \textbf{Média} & 0,9291 & \textbf{Média} & 0,8397 & \textbf{Média} & 0,6256 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,79 & \textbf{Mínimo} & 0,8194 & \textbf{Mínimo} & 0,9007 & \textbf{Mínimo} & 0,8581 & \textbf{Mínimo} & 0,7131 \\
             & & & & \textbf{Máximo} & 0,79 & \textbf{Máximo} & 0,8194 & \textbf{Máximo} & 0,9007 & \textbf{Máximo} & 0,8581 & \textbf{Máximo} & 0,7131 \\
             & & & & \textbf{Média} & 0,79 & \textbf{Média} & 0,8194 & \textbf{Média} & 0,9007 & \textbf{Média} & 0,8581 & \textbf{Média} & 0,7131 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,79 & \textbf{Mínimo} & 0,8235 & \textbf{Mínimo} & 0,8936 & \textbf{Mínimo} & 0,8571 & \textbf{Mínimo} & 0,718 \\
             & & & & \textbf{Máximo} & 0,79 & \textbf{Máximo} & 0,8235 & \textbf{Máximo} & 0,8936 & \textbf{Máximo} & 0,8571 & \textbf{Máximo} & 0,718 \\
             & & & & \textbf{Média} & 0,79 & \textbf{Média} & 0,8235 & \textbf{Média} & 0,8936 & \textbf{Média} & 0,8571 & \textbf{Média} & 0,718 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,75 & \textbf{Mínimo} & 0,7661 & \textbf{Mínimo} & 0,9291 & \textbf{Mínimo} & 0,8397 & \textbf{Mínimo} & 0,6256 \\
             & & & & \textbf{Máximo} & 0,75 & \textbf{Máximo} & 0,7661 & \textbf{Máximo} & 0,9291 & \textbf{Máximo} & 0,8397 & \textbf{Máximo} & 0,6256 \\
             & & & & \textbf{Média} & 0,75 & \textbf{Média} & 0,7661 & \textbf{Média} & 0,9291 & \textbf{Média} & 0,8397 & \textbf{Média} & 0,6256 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,775 & \textbf{Mínimo} & 0,8 & \textbf{Mínimo} & 0,9078 & \textbf{Mínimo} & 0,8505 & \textbf{Mínimo} & 0,6827 \\
             & & & & \textbf{Máximo} & 0,775 & \textbf{Máximo} & 0,8 & \textbf{Máximo} & 0,9078 & \textbf{Máximo} & 0,8505 & \textbf{Máximo} & 0,6827 \\
             & & & & \textbf{Média} & 0,775 & \textbf{Média} & 0,8 & \textbf{Média} & 0,9078 & \textbf{Média} & 0,8505 & \textbf{Média} & 0,6827 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,775 & \textbf{Mínimo} & 0,8 & \textbf{Mínimo} & 0,9078 & \textbf{Mínimo} & 0,8505 & \textbf{Mínimo} & 0,6827 \\
             & & & & \textbf{Máximo} & 0,775 & \textbf{Máximo} & 0,8 & \textbf{Máximo} & 0,9078 & \textbf{Máximo} & 0,8505 & \textbf{Máximo} & 0,6827 \\
             & & & & \textbf{Média} & 0,775 & \textbf{Média} & 0,8 & \textbf{Média} & 0,9078 & \textbf{Média} & 0,8505 & \textbf{Média} & 0,6827 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Disparate Impact Remover} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,705 & \textbf{Mínimo} & 0,705 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,827 & \textbf{Mínimo} & 0,5 \\
             & & & & \textbf{Máximo} & 0,705 & \textbf{Máximo} & 0,705 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,827 & \textbf{Máximo} & 0,5 \\
             & & & & \textbf{Média} & 0,705 & \textbf{Média} & 0,705 & \textbf{Média} & 1 & \textbf{Média} & 0,827 & \textbf{Média} & 0,5 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Disparate Impact Remover} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,705 & \textbf{Mínimo} & 0,705 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,827 & \textbf{Mínimo} & 0,5 \\
             & & & & \textbf{Máximo} & 0,705 & \textbf{Máximo} & 0,705 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,827 & \textbf{Máximo} & 0,5 \\
             & & & & \textbf{Média} & 0,705 & \textbf{Média} & 0,705 & \textbf{Média} & 1 & \textbf{Média} & 0,827 & \textbf{Média} & 0,5 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,715 & \textbf{Mínimo} & 0,7143 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,8309 & \textbf{Mínimo} & 0,5219 \\
             & & & & \textbf{Máximo} & 0,715 & \textbf{Máximo} & 0,7143 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,8309 & \textbf{Máximo} & 0,5219 \\
             & & & & \textbf{Média} & 0,715 & \textbf{Média} & 0,7143 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,8309 & \textbf{Média} & 0,5219 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,785 & \textbf{Mínimo} & 0,8063 & \textbf{Mínimo} & 0,9149 & \textbf{Mínimo} & 0,8571 & \textbf{Mínimo} & 0,6947 \\
             & & & & \textbf{Máximo} & 0,785 & \textbf{Máximo} & 0,8063 & \textbf{Máximo} & 0,9149 & \textbf{Máximo} & 0,8571 & \textbf{Máximo} & 0,6947 \\
             & & & & \textbf{Média} & 0,785 & \textbf{Média} & 0,8063 & \textbf{Média} & 0,9149 & \textbf{Média} & 0,8571 & \textbf{Média} & 0,6947 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,785 & \textbf{Mínimo} & 0,8063 & \textbf{Mínimo} & 0,9149 & \textbf{Mínimo} & 0,8571 & \textbf{Mínimo} & 0,6947 \\
             & & & & \textbf{Máximo} & 0,785 & \textbf{Máximo} & 0,8063 & \textbf{Máximo} & 0,9149 & \textbf{Máximo} & 0,8571 & \textbf{Máximo} & 0,6947 \\
             & & & & \textbf{Média} & 0,785 & \textbf{Média} & 0,8063 & \textbf{Média} & 0,9149 & \textbf{Média} & 0,8571 & \textbf{Média} & 0,6947 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,76 & \textbf{Mínimo} & 0,7831 & \textbf{Mínimo} & 0,8936 & \textbf{Mínimo} & 0,84 & \textbf{Mínimo} & 0,6559 \\
             & & & & \textbf{Máximo} & 0,79 & \textbf{Máximo} & 0,8113 & \textbf{Máximo} & 0,922 & \textbf{Máximo} & 0,86 & \textbf{Máximo} & 0,7032 \\
             & & & & \textbf{Média} & 0,772 & \textbf{Média} & 0,7949 & \textbf{Média} & 0,9121 & \textbf{Média} & 0,8494 & \textbf{Média} & 0,6746 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,715 & \textbf{Mínimo} & 0,7143 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,8309 & \textbf{Mínimo} & 0,5219 \\
             & & & & \textbf{Máximo} & 0,715 & \textbf{Máximo} & 0,7143 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,8309 & \textbf{Máximo} & 0,5219 \\
             & & & & \textbf{Média} & 0,715 & \textbf{Média} & 0,7143 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,8309 & \textbf{Média} & 0,5219 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,715 & \textbf{Mínimo} & 0,7143 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,8309 & \textbf{Mínimo} & 0,5219 \\
             & & & & \textbf{Máximo} & 0,715 & \textbf{Máximo} & 0,7143 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,8309 & \textbf{Máximo} & 0,5219 \\
             & & & & \textbf{Média} & 0,715 & \textbf{Média} & 0,7143 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,8309 & \textbf{Média} & 0,5219 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Adversarial Debiasing} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,295 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0,5 \\
             & & & & \textbf{Máximo} & 0,705 & \textbf{Máximo} & 0,7097 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,827 & \textbf{Máximo} & 0.5105 \\
             & & & & \textbf{Média} & 0,6317 & \textbf{Média} & 0,5888 & \textbf{Média} & 0,8168 & \textbf{Média} & 0,6842 & \textbf{Média} & 0,503 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Grid Search Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,75 & \textbf{Mínimo} & 0,7791 & \textbf{Mínimo} & 0,9007 & \textbf{Mínimo} & 0,8355 & \textbf{Mínimo} & 0,6453 \\
             & & & & \textbf{Máximo} & 0,78 & \textbf{Máximo} & 0,7939 & \textbf{Máximo} & 0,9362 & \textbf{Máximo} & 0,8562 & \textbf{Máximo} & 0,6764 \\
             & & & & \textbf{Média} & 0,765 & \textbf{Média} & 0,7857 & \textbf{Média} & 0,9167 & \textbf{Média} & 0,8461 & \textbf{Média} & 0,6596 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Meta Fair Classifier} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,73 & \textbf{Mínimo} & 0,7278 & \textbf{Mínimo} & 0,9291 & \textbf{Mínimo} & 0,8374 & \textbf{Mínimo} & 0,5522 \\
             & & & & \textbf{Máximo} & 0,755 & \textbf{Máximo} & 0,7661 & \textbf{Máximo} & 0,9858 & \textbf{Máximo} & 0,8474 & \textbf{Máximo} & 0,6256 \\
             & & & & \textbf{Média} & 0,742 & \textbf{Média} & 0,7466 & \textbf{Média} & 0,9617 & \textbf{Média} & 0,8402 & \textbf{Média} & 0,5893 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Exponentiated Gradient Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,785 & \textbf{Mínimo} & 0,8063 & \textbf{Mínimo} & 0,9149 & \textbf{Mínimo} & 0,8571 & \textbf{Mínimo} & 0,6947 \\
             & & & & \textbf{Máximo} & 0,785 & \textbf{Máximo} & 0,8063 & \textbf{Máximo} & 0,9149 & \textbf{Máximo} & 0,8571 & \textbf{Máximo} & 0,6947 \\
             & & & & \textbf{Média} & 0,785 & \textbf{Média} & 0,8063 & \textbf{Média} & 0,9149 & \textbf{Média} & 0,8571 & \textbf{Média} & 0,6947 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Rich Subgroup Fairness} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,76 & \textbf{Mínimo} & 0,808 & \textbf{Mínimo} & 0,8653 & \textbf{Mínimo} & 0,8356 & \textbf{Mínimo} & 0,6869 \\
             & & & & \textbf{Máximo} & 0,76 & \textbf{Máximo} & 0,808 & \textbf{Máximo} & 0,8653 & \textbf{Máximo} & 0,8356 & \textbf{Máximo} & 0,6869 \\
             & & & & \textbf{Média} & 0,76 & \textbf{Média} & 0,808 & \textbf{Média} & 0,8653 & \textbf{Média} & 0,8356 & \textbf{Média} & 0,6869 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Exponentiated Gradient Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,775 & \textbf{Mínimo} & 0,8038 & \textbf{Mínimo} & 0,9007 & \textbf{Mínimo} & 0,8495 & \textbf{Mínimo} & 0,6876 \\
             & & & & \textbf{Máximo} & 0,785 & \textbf{Máximo} & 0,8101 & \textbf{Máximo} & 0,9078 & \textbf{Máximo} & 0,8562 & \textbf{Máximo} & 0,6997 \\
             & & & & \textbf{Média} & 0,7788 & \textbf{Média} & 0,8057 & \textbf{Média} & 0,9043 & \textbf{Média} & 0,8521 & \textbf{Média} & 0,6915 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Prejudice Remover} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,735 & \textbf{Mínimo} & 0,7683 & \textbf{Mínimo} & 0,8936 & \textbf{Mínimo} & 0,8262 & \textbf{Mínimo} & 0,6248 \\
             & & & & \textbf{Máximo} & 0,735 & \textbf{Máximo} & 0,7683 & \textbf{Máximo} & 0,8936 & \textbf{Máximo} & 0,8262 & \textbf{Máximo} & 0,6248 \\
             & & & & \textbf{Média} & 0,735 & \textbf{Média} & 0,7683 & \textbf{Média} & 0,8936 & \textbf{Média} & 0,8262 & \textbf{Média} & 0,6248 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,965 & \textbf{Mínimo} & 0,9589 & \textbf{Mínimo} & 0,9929 & \textbf{Mínimo} & 0,9756 & \textbf{Mínimo} & 0,9456 \\
             & & & & \textbf{Máximo} & 0,965 & \textbf{Máximo} & 0,9589 & \textbf{Máximo} & 0,9929 & \textbf{Máximo} & 0,9756 & \textbf{Máximo} & 0,9456 \\
             & & & & \textbf{Média} & 0,965 & \textbf{Média} & 0,9589 & \textbf{Média} & 0,9929 & \textbf{Média} & 0,9756 & \textbf{Média} & 0,9456 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Calibrated Equalized Odds} & \textbf{Mínimo} & 0,82 & \textbf{Mínimo} & 0,7966 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,8868 & \textbf{Mínimo} & 0,6949 \\
             & & & & \textbf{Máximo} & 0,93 & \textbf{Máximo} & 0,9097 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,9527 & \textbf{Máximo} & 0,8814 \\
             & & & & \textbf{Média} & 0,8925 & \textbf{Média} & 0,87 & \textbf{Média} & 1 & \textbf{Média} & 0,9299 & \textbf{Média} & 0,8178 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Calibrated Equalized Odds} & \textbf{Mínimo} & 0,84 & \textbf{Mínimo} & 0,8150 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,8981 & \textbf{Mínimo} & 0,7288 \\
             & & & & \textbf{Máximo} & 0,875 & \textbf{Máximo} & 0,8494 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,9186 & \textbf{Máximo} & 0,7881 \\
             & & & & \textbf{Média} & 0,855 & \textbf{Média} & 0,8296 & \textbf{Média} & 1 & \textbf{Média} & 0,9068 & \textbf{Média} & 0,7542 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,905 & \textbf{Mínimo} & 0,9769 & \textbf{Mínimo} & 0,8723 & \textbf{Mínimo} & 0,9283 & \textbf{Mínimo} & 0,9249 \\
             & & & & \textbf{Máximo} & 0,915 & \textbf{Máximo} & 0,9919 & \textbf{Máximo} & 0,9007 & \textbf{Máximo} & 0,9373 & \textbf{Máximo} & 0,9277 \\
             & & & & \textbf{Média} & 0,9083 & \textbf{Média} & 0,9869 & \textbf{Média} & 0,8818 & \textbf{Média} & 0,9313 & \textbf{Média} & 0,9268 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,8 & \textbf{Mínimo} & 0,9697 & \textbf{Mínimo} & 0,7234 & \textbf{Mínimo} & 0,8361 & \textbf{Mínimo} & 0,8532 \\
             & & & & \textbf{Máximo} & 0,915 & \textbf{Máximo} & 0,9903 & \textbf{Máximo} & 0,9078 & \textbf{Máximo} & 0,9377 & \textbf{Máximo} & 0,92 \\
             & & & & \textbf{Média} & 0,8733 & \textbf{Média} & 0,9789 & \textbf{Média} & 0,8392 & \textbf{Média} & 0,9011 & \textbf{Média} & 0,897 \\
        \end{tabular}}
    \end{center}
\end{table}
\begin{table}[H]
    \begin{center}
        \caption{Métricas de \textit{Fairness} das execuções no Módulo de ML}
        \label{tbl:FairnessMetrics}
        \resizebox{\linewidth}{!}{%
        \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
            \multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{10}{c}{Métricas} \\
            \hline
            Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & \multicolumn{2}{c|}{Statistical Parity Difference} & \multicolumn{2}{c|}{Equal Opportunity Difference} & \multicolumn{2}{c|}{Average Odds Difference} & \multicolumn{2}{c|}{Disparate Impact} & \multicolumn{2}{c}{Theil Index} \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0209 & \textbf{Mínimo} & -0,0075 & \textbf{Mínimo} & -0,0296 & \textbf{Mínimo} & 0,9791 & \textbf{Mínimo} & 0,0615 \\
             & & & & \textbf{Máximo} & -0,0209 & \textbf{Máximo} & -0,0075 & \textbf{Máximo} & -0,0296 & \textbf{Máximo} & 0,9791 & \textbf{Máximo} & 0,0615 \\
             & & & & \textbf{Média} & -0,0209 & \textbf{Média} & -0,0075 & \textbf{Média} & -0,0296 & \textbf{Média} & 0,9791 & \textbf{Média} & 0,0615 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,0238 & \textbf{Mínimo} & 0,0084 & \textbf{Mínimo} & 0,0348 & \textbf{Mínimo} & 1,0244 & \textbf{Mínimo} & 0,0615 \\
             & & & & \textbf{Máximo} & 0,0238 & \textbf{Máximo} & 0,0084 & \textbf{Máximo} & 0,0348 & \textbf{Máximo} & 1,0244 & \textbf{Máximo} & 0,0615 \\
             & & & & \textbf{Média} & 0,0238 & \textbf{Média} & 0,0084 & \textbf{Média} & 0,0348 & \textbf{Média} & 1,0244 & \textbf{Média} & 0,0615 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,09831 & \textbf{Mínimo} & 0,0273 & \textbf{Mínimo} & -0,2191 & \textbf{Mínimo} & 0,8894 & \textbf{Mínimo} & 0,0955 \\
             & & & & \textbf{Máximo} & 0,03898 & \textbf{Máximo} & 0,1899 & \textbf{Máximo} & -0,1378 & \textbf{Máximo} & 1,0501 & \textbf{Máximo} & 0,1173 \\
             & & & & \textbf{Média} & -0,0520 & \textbf{Média} & 0,0733 & \textbf{Média} & -0,1806 & \textbf{Média} & 0.9428 & \textbf{Média} & 0,1045 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,1518 & \textbf{Mínimo} & -0,0752 & \textbf{Mínimo} & -0,2014 & \textbf{Mínimo} & 0,8482 & \textbf{Mínimo} & 0,1013 \\
             & & & & \textbf{Máximo} & -0,1518 & \textbf{Máximo} & -0,0752 & \textbf{Máximo} & -0,2014 & \textbf{Máximo} & 0,8482 & \textbf{Máximo} & 0,1013 \\
             & & & & \textbf{Média} & -0,1518 & \textbf{Média} & -0,0752 & \textbf{Média} & -0,2014 & \textbf{Média} & 0,8482 & \textbf{Média} & 0,1013 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,1134 & \textbf{Mínimo} & 0,2923 & \textbf{Mínimo} & -0,1211 & \textbf{Mínimo} & 1,1702 & \textbf{Mínimo} & 0,1137 \\
             & & & & \textbf{Máximo} & 0,1134 & \textbf{Máximo} & 0,2923 & \textbf{Máximo} & -0,1211 & \textbf{Máximo} & 1,1702 & \textbf{Máximo} & 0,1137 \\
             & & & & \textbf{Média} & 0,1134 & \textbf{Média} & 0,2923 & \textbf{Média} & -0,1211 & \textbf{Média} & 1,1702 & \textbf{Média} & 0,1137 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,2411 & \textbf{Mínimo} & -0,1971 & \textbf{Mínimo} & -0,2537 & \textbf{Mínimo} & 0,7 & \textbf{Mínimo} & 0,1183 \\
             & & & & \textbf{Máximo} & -0,2411 & \textbf{Máximo} & -0,1971 & \textbf{Máximo} & -0,2537 & \textbf{Máximo} & 0,7 & \textbf{Máximo} & 0,1183 \\
             & & & & \textbf{Média} & -0,2411 & \textbf{Média} & -0,1971 & \textbf{Média} & -0,2537 & \textbf{Média} & 0,7 & \textbf{Média} & 0,1183 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,1518 & \textbf{Mínimo} & -0,0752 & \textbf{Mínimo} & -0,2014 & \textbf{Mínimo} & 0,8482 & \textbf{Mínimo} & 0,1013 \\
             & & & & \textbf{Máximo} & -0,1518 & \textbf{Máximo} & -0,0752 & \textbf{Máximo} & -0,2014 & \textbf{Máximo} & 0,8482 & \textbf{Máximo} & 0,1013 \\
             & & & & \textbf{Média} & -0,1518 & \textbf{Média} & -0,0752 & \textbf{Média} & -0,2014 & \textbf{Média} & 0,8482 & \textbf{Média} & 0,1013 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0595 & \textbf{Mínimo} & -0,0523 & \textbf{Mínimo} & -0,0517 & \textbf{Mínimo} & 0,9265 & \textbf{Mínimo} & 0,1118 \\
             & & & & \textbf{Máximo} & -0,0595 & \textbf{Máximo} & -0,0523 & \textbf{Máximo} & -0,0517 & \textbf{Máximo} & 0,9265 & \textbf{Máximo} & 0,1118 \\
             & & & & \textbf{Média} & -0,0595 & \textbf{Média} & -0,0523 & \textbf{Média} & -0,0517 & \textbf{Média} & 0,9265 & \textbf{Média} & 0,1118 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0595 & \textbf{Mínimo} & -0,0523 & \textbf{Mínimo} & -0,0517 & \textbf{Mínimo} & 0,9265 & \textbf{Mínimo} & 0,1118 \\
             & & & & \textbf{Máximo} & -0,0595 & \textbf{Máximo} & -0,0523 & \textbf{Máximo} & -0,0517 & \textbf{Máximo} & 0,9265 & \textbf{Máximo} & 0,1118 \\
             & & & & \textbf{Média} & -0,0595 & \textbf{Média} & -0,0523 & \textbf{Média} & -0,0517 & \textbf{Média} & 0,9265 & \textbf{Média} & 0,1118 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Disparate Impact Remover} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,0573 \\
             & & & & \textbf{Máximo} & 0 & \textbf{Máximo} & 0 & \textbf{Máximo} & 0 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,0573 \\
             & & & & \textbf{Média} & 0 & \textbf{Média} & 0 & \textbf{Média} & 0 & \textbf{Média} & 1 & \textbf{Média} & 0,0573 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Disparate Impact Remover} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,0573 \\
             & & & & \textbf{Máximo} & 0 & \textbf{Máximo} & 0 & \textbf{Máximo} & 0 & \textbf{Máximo} & 1 & \textbf{Máximo} & 0,0573 \\
             & & & & \textbf{Média} & 0 & \textbf{Média} & 0 & \textbf{Média} & 0 & \textbf{Média} & 1 & \textbf{Média} & 0,0573 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0209 & \textbf{Mínimo} & -0,0075 & \textbf{Mínimo} & -0,0296 & \textbf{Mínimo} & 0,9791 & \textbf{Mínimo} & 0,0615 \\
             & & & & \textbf{Máximo} & -0,0209 & \textbf{Máximo} & -0,0075 & \textbf{Máximo} & -0,0296 & \textbf{Máximo} & 0,9791 & \textbf{Máximo} & 0,0615 \\
             & & & & \textbf{Média} & -0,0209 & \textbf{Média} & -0,0075 & \textbf{Média} & -0,0296 & \textbf{Média} & 0,9791 & \textbf{Média} & 0,0615 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0931 & \textbf{Mínimo} & 0,0423 & \textbf{Mínimo} & -0,2202 & \textbf{Mínimo} & 0,8953 & \textbf{Mínimo} & 0,1055 \\
             & & & & \textbf{Máximo} & -0,0931 & \textbf{Máximo} & 0,0423 & \textbf{Máximo} & -0,2202 & \textbf{Máximo} & 0,8953 & \textbf{Máximo} & 0,1055 \\
             & & & & \textbf{Média} & -0,0931 & \textbf{Média} & 0,0423 & \textbf{Média} & -0,2202 & \textbf{Média} & 0,8953 & \textbf{Média} & 0,1055 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0931 & \textbf{Mínimo} & 0,0423 & \textbf{Mínimo} & -0,2202 & \textbf{Mínimo} & 0,8953 & \textbf{Mínimo} & 0,1055 \\
             & & & & \textbf{Máximo} & -0,0931 & \textbf{Máximo} & 0,0423 & \textbf{Máximo} & -0,2202 & \textbf{Máximo} & 0,8953 & \textbf{Máximo} & 0,1055 \\
             & & & & \textbf{Média} & -0,0931 & \textbf{Média} & 0,0423 & \textbf{Média} & -0,2202 & \textbf{Média} & 0,8953 & \textbf{Média} & 0,1055 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0983 & \textbf{Mínimo} & 0,0197 & \textbf{Mínimo} & -0,2289 & \textbf{Mínimo} & 0,8894 & \textbf{Mínimo} & 0,1025 \\
             & & & & \textbf{Máximo} & 0,0285 & \textbf{Máximo} & 0,1673 & \textbf{Máximo} & -0,1405 & \textbf{Máximo} & 1,0367 & \textbf{Máximo} & 0,1237 \\
             & & & & \textbf{Média} & -0,0604 & \textbf{Média} & 0,0658 & \textbf{Média} & -0,1895 & \textbf{Média} & 0,933 & \textbf{Média} & 0,1095 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Reweighing} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0209 & \textbf{Mínimo} & -0,0075 & \textbf{Mínimo} & -0,0296 & \textbf{Mínimo} & 0,9791 & \textbf{Mínimo} & 0,0615 \\
             & & & & \textbf{Máximo} & -0,0209 & \textbf{Máximo} & -0,0075 & \textbf{Máximo} & -0,0296 & \textbf{Máximo} & 0,9791 & \textbf{Máximo} & 0,0615 \\
             & & & & \textbf{Média} & -0,0209 & \textbf{Média} & -0,0075 & \textbf{Média} & -0,0296 & \textbf{Média} & 0,9791 & \textbf{Média} & 0,0615 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Learning Fair Representations} & \multirow{3}{*}{Support Vector Machines} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,0238 & \textbf{Mínimo} & 0,0084 & \textbf{Mínimo} & 0,0348 & \textbf{Mínimo} & 1,0244 & \textbf{Mínimo} & 0,0615 \\
             & & & & \textbf{Máximo} & 0,0238 & \textbf{Máximo} & 0,0084 & \textbf{Máximo} & 0,0348 & \textbf{Máximo} & 1,0244 & \textbf{Máximo} & 0,0615 \\
             & & & & \textbf{Média} & 0,0238 & \textbf{Média} & 0,0084 & \textbf{Média} & 0,0348 & \textbf{Média} & 1,0244 & \textbf{Média} & 0,0615 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Adversarial Debiasing} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & -0,0086 & \textbf{Mínimo} & 1 & \textbf{Mínimo} & 0,0573 \\
             & & & & \textbf{Máximo} & 0,0104 & \textbf{Máximo} & 0,042 & \textbf{Máximo} & 0,0017 & \textbf{Máximo} & 1,0109 & \textbf{Máximo} & 1,2208 \\
             & & & & \textbf{Média} & 0,0032 & \textbf{Média} & 0,0106 & \textbf{Média} & -0,0012 & \textbf{Média} & N/A & \textbf{Média} & 0,2629 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Grid Search Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,0826 & \textbf{Mínimo} & 0,0273 & \textbf{Mínimo} & -0,1933 & \textbf{Mínimo} & 0,9071 & \textbf{Mínimo} & 0,0932 \\
             & & & & \textbf{Máximo} & -0,0512 & \textbf{Máximo} & 0,0648 & \textbf{Máximo} & -0,1659 & \textbf{Máximo} & 0,9424 & \textbf{Máximo} & 0,1204 \\
             & & & & \textbf{Média} & -0,0695 & \textbf{Média} & 0,0442 & \textbf{Média} & -0,1827 & \textbf{Média} & 0,9218 & \textbf{Média} & 0,1076 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Meta Fair Classifier} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,1548 & \textbf{Mínimo} & -0,0943 & \textbf{Mínimo} & -0,1849 & \textbf{Mínimo} & 0,8289 & \textbf{Mínimo} & 0,0652 \\
             & & & & \textbf{Máximo} & -0,0208 & \textbf{Máximo} & 0,0168 & \textbf{Máximo} & -0,0406 & \textbf{Máximo} & 0,9783 & \textbf{Máximo} & 0,1013 \\
             & & & & \textbf{Média} & -0,0926 & \textbf{Média} & -0,0408 & \textbf{Média} & -0,1186 & \textbf{Média} & 0,8979 & \textbf{Média} & 0,0802 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Exponentiated Gradient Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,1339 & \textbf{Mínimo} & -0,1146 & \textbf{Mínimo} & -0,1328 & \textbf{Mínimo} & 0,837 & \textbf{Mínimo} & 0,1055 \\
             & & & & \textbf{Máximo} & -0,1339 & \textbf{Máximo} & -0,1146 & \textbf{Máximo} & -0,1328 & \textbf{Máximo} & 0,837 & \textbf{Máximo} & 0,1055 \\
             & & & & \textbf{Média} & -0,1339 & \textbf{Média} & -0,1146 & \textbf{Média} & -0,1328 & \textbf{Média} & 0,837 & \textbf{Média} & 0,1055 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Rich Subgroup Fairness} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,1402 & \textbf{Mínimo} & -0,0103 & \textbf{Mínimo} & -0,2638 & \textbf{Mínimo} & 0,8423 & \textbf{Mínimo} & 0,1427 \\
             & & & & \textbf{Máximo} & -0,1402 & \textbf{Máximo} & -0,0103 & \textbf{Máximo} & -0,2638 & \textbf{Máximo} & 0,8423 & \textbf{Máximo} & 0,1427 \\
             & & & & \textbf{Média} & -0,1402 & \textbf{Média} & -0,0103 & \textbf{Média} & -0,2638 & \textbf{Média} & 0,8423 & \textbf{Média} & 0,1427 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Exponentiated Gradient Reduction} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & -0,2199 & \textbf{Mínimo} & -0,1053 & \textbf{Mínimo} & -0,2989 & \textbf{Mínimo} & 0,7801 & \textbf{Mínimo} & 0,1101 \\
             & & & & \textbf{Máximo} & -0,2147 & \textbf{Máximo} & -0,0977 & \textbf{Máximo} & -0,2903 & \textbf{Máximo} & 0,7853 & \textbf{Máximo} & 0,1165 \\
             & & & & \textbf{Média} & -0,2186 & \textbf{Média} & -0,1015 & \textbf{Média} & -0,2943 & \textbf{Média} & 0,7814 & \textbf{Média} & 0,1135 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Prejudice Remover} & \multirow{3}{*}{Nenhum} & \textbf{Mínimo} & 0,0442 & \textbf{Mínimo} & 0,1523 & \textbf{Mínimo} & -0,1049 & \textbf{Mínimo} &  1,057 & \textbf{Mínimo} & 0,1274 \\
             & & & & \textbf{Máximo} & 0,0442 & \textbf{Máximo} & 0,1523 & \textbf{Máximo} & -0,1049 & \textbf{Máximo} & 1,057 & \textbf{Máximo} & 0,1274 \\
             & & & & \textbf{Média} & 0,0442 & \textbf{Média} & 0,1523 & \textbf{Média} & -0,1049 & \textbf{Média} & 1,057 & \textbf{Média} & 0,1274 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Regressão Logística} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,1726 & \textbf{Mínimo} & 0,0084 & \textbf{Mínimo} & 0,3042 & \textbf{Mínimo} & 1.2458 & \textbf{Mínimo} & 0.0159 \\
             & & & & \textbf{Máximo} & 0,1726 & \textbf{Máximo} & 0,0084 & \textbf{Máximo} & 0,3042 & \textbf{Máximo} & 1.2458 & \textbf{Máximo} & 0.0159 \\
             & & & & \textbf{Média} & 0,1726 & \textbf{Média} & 0,0084 & \textbf{Média} & 0,3042 & \textbf{Média} & 1.2458 & \textbf{Média} & 0.0159 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Calibrated Equalized Odds} & \textbf{Mínimo} & -0,1193 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0,1207 & \textbf{Mínimo} & 0,8658 & \textbf{Mínimo} & 0,02303 \\
             & & & & \textbf{Máximo} & -0,0041 & \textbf{Máximo} & 0 & \textbf{Máximo} & 0,3103 & \textbf{Máximo} & 0,9954 & \textbf{Máximo} & 0,046 \\
             & & & & \textbf{Média} & -0,08 & \textbf{Média} & 0 & \textbf{Média} & 0,1853 & \textbf{Média} & 0,91 & \textbf{Média} & 0,0314 \\
            \hline
            \multirow{3}{*}{Nacionalidade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Calibrated Equalized Odds} & \textbf{Mínimo} & -0,0617 & \textbf{Mínimo} & 0 & \textbf{Mínimo} & 0,2155 & \textbf{Mínimo} & 0,9306 & \textbf{Mínimo} & 0,0362 \\
             & & & & \textbf{Máximo} & -0,025 & \textbf{Máximo} & 0 & \textbf{Máximo} & 0,2759 & \textbf{Máximo} & 0,9719 & \textbf{Máximo} & 0,0428 \\
             & & & & \textbf{Média} & -0,0407 & \textbf{Média} & 0 & \textbf{Média} & 0,25 & \textbf{Média} & 0,9542 & \textbf{Média} & 0,0401 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Gradient Boosting} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,1176 & \textbf{Mínimo} & 0,1177 & \textbf{Mínimo} & 0,1256 & \textbf{Mínimo} & 1,1955 & \textbf{Mínimo} & 0,0786 \\
             & & & & \textbf{Máximo} & 0,1563 & \textbf{Máximo} & 0,1513 & \textbf{Máximo} & 0,2088 & \textbf{Máximo} & 1,25 & \textbf{Máximo} & 0,0964 \\
             & & & & \textbf{Média} & 0,1305 & \textbf{Média} & 0,1401 & \textbf{Média} & 0,1534 & \textbf{Média} & 1,2137 & \textbf{Média} & 0,0905 \\
            \hline
            \multirow{3}{*}{Idade} & \multirow{3}{*}{Nenhum} & \multirow{3}{*}{Random Forest} & \multirow{3}{*}{Equalized Odds} & \textbf{Mínimo} & 0,1682 & \textbf{Mínimo} & 0,1092 & \textbf{Mínimo} & 0,2139 & \textbf{Mínimo} & 1,2743 & \textbf{Mínimo} & 0,0751 \\
             & & & & \textbf{Máximo} & 0,2426 & \textbf{Máximo} & 0,3277 & \textbf{Máximo} & 0,2546 & \textbf{Máximo} & 1,5094 & \textbf{Máximo} & 0,2193 \\
             & & & & \textbf{Média} & 0,1974 & \textbf{Média} & 0,1905 & \textbf{Média} & 0,2286 & \textbf{Média} & 1,3571 & \textbf{Média} & 0,1279 \\
            \end{tabular}}
        \end{center}
    \end{table}

Com tais dados, é possível reforçar com clareza observações notadas anteriormente:

\begin{itemize}
\item Boas métricas de Avaliação nos casos onde algoritmos para redução de viés de pós-processamento foram utilizados, e boas métricas de \textit{Fairness} nos casos onde o algoritmo \textit{Support Vector Machines} foi utilizado, indicando que as pontuações obtidas no experimento são diretamente proporcionais ao ranqueamento das métricas presentes nos dados.
\item Assim como nas pontuações obtidas no experimento, há mudanças nas métricas de \textit{Fairness} conforme o algoritmo de treinamento ou o atributo protegido é alterado, independente de utilizar algoritmos para redução de viés ou não em sua configuração arquitetural.
\end{itemize}

A análise dos resultados das Tabelas \ref{tbl:PerformanceMetrics} e \ref{tbl:FairnessMetrics} confirma que o cálculo utilizado no Analisador do Gerenciador Autonômico foi capaz de consolidar as métricas existentes de forma eficiente, proporcionando uma visão geral das configurações arquiteturais utilizadas sem comprometer a interpretação dos resultados. Analisando as métricas isoladamente, é difícil identificar qual configuração arquitetural oferece o melhor equilíbrio entre dois grupos de métricas com contextos completamente diferentes, diante da grande quantidade de métricas, a diferença extremamente pequena entre os resultados e a grande quantidade de configurações arquiteturais. Nesse contexto, a consolidação das métricas em grupos simplifica a visualização de quais configurações arquiteturais são mais equilibradas, e o uso de pesos para cada métrica e para cada grupo pode calibrar qual o melhor equilíbrio desejado para determinada situação.

Deste modo, pode-se concluir também que, em um contexto de desenvolvimento, o processo simplifica a decisão do Cientista de Dados e reduz significantemente o tempo para obtenção e implantação de um modelo otimizado, pois não exigirá execuções em diversos algoritmos uma vez que já há uma base de conhecimento prévia. Além disso, poderá poupar processamento e custos para a resolução de diversos outros problemas, uma vez que as execuções economizadas pelas equipes que utilizariam esse processo abrem margem para que outras equipes utilizem esse processamento.

\chapter{Estudo de Caso 2: Classificação de Crédito (Lendingclub Dataset) e evolução do sistema}

Neste Estudo de Caso, foi realizada uma evolução do sistema adicionando um novo conjunto de dados mais próximo de conjuntos de dados reais. Além de reforçar a versatilidade do Gerenciador Autonômico em diferentes contextos, um maior foco foi colocado na manutenção do sistema, discutindo se as etapas e arquiteturas escolhidas são viáveis para evoluir e manter o Módulo de ML sem grandes deteriorações nas ideias originais de seu desenvolvimento.

\section{Contexto e limitações}

O experimento foi realizado com o seguinte conjunto de dados e dado o seguinte contexto:

\begin{itemize}
\item \textbf{Contexto:} Obter classificação de crédito de uma pessoa (apto ou não apto), por meio de uma série de \textit{features}.

\item \textbf{Conjunto de dados:} Lendingclub Dataset~\cite{lendingclub_2022}.

\item \textbf{Transformações realizadas no conjunto de dados:} Filtragem de linhas que não definiam classificação boa ou ruim de crédito; seleção de 20 \textit{features} utilizando algoritmo de informação mútua~\cite{Ross_2014}, mais uma \textit{feature} para caracterizar dado sensível (atributo protegido) e \textit{feature} de classificação de crédito, totalizando 22 \textit{features} no total.

\item \textbf{Atributos protegidos:} Renda

\item \textbf{Grupo privilegiado:} Renda de 1 ou mais salários mínimos

\item \textbf{Grupo não-privilegiado:} Renda de menos de 1 salário mínimo

\end{itemize}

Para realizar este Estudo de Caso, foi considerado o seguinte objetivo e consideradas as seguintes limitações:

\begin{itemize}
\item \textbf{Experimento:} Realização das mesmas condições do Estudo de Caso 1 para discussão da manutenção do sistema e reforço da viabilidade do Gerenciador Autonômico em casos mais próximos do mundo real.

\item \textbf{Medição:} Além das medições do Estudo de Caso 1, serão medidas a quantidade de linhas modificadas e sua relação com a quantidade de linhas escritas.

\item \textbf{Obtenção dos dados:} A execução do Módulo de ML utilizando o Gerenciador Autonômico foi realizada nas mesmas condições do Estudo de Caso 1. A contagem de linhas e arquivos foi realizada executando o comando \textbf{find ./src -name '*.py' | xargs wc -l} para os arquivos Python e o comando \textbf{find ./ml-ui/src -name '*.js' | xargs wc -l} para os arquivos Javascript presentes no projeto, excluindo-se os arquivos \textbf{\_\_init\_\_.py} que não possuem linhas de código e são criados apenas para o Python utilizar códigos de arquivos que estão dentro de outras pastas.

\item \textbf{Pré-condições:} Por se tratar de um outro conjunto de dados, a pré-condição 3 foi desconsiderada. as pré-condições 1 e 2 do Estudo de Caso 1 foram mantidas.

\item \textbf{Restrições:} As 4 restrições determinadas no Estudo de Caso 1 foram mantidas, com uma diferença: Na Restrição 4, como o \textit{Lendingclub Dataset} possui métricas com valores melhores do que as apresentadas no \textit{German Credit Dataset}, o intervalo de pontuação para análise foi ampliado de 500 a 980, mas as motivações para esse valor permanecem as mesmas.
\end{itemize}

\section{Resultados e Discussões}

Os resultados baseados nas pré-condições e restrições já comentadas na seção anterior estão presentes abaixo nas Tabelas \ref{tbl:ScoreMAPEKLendingclubGeral5050}, \ref{tbl:ScoreMAPEKLendingclubGeral7525} e \ref{tbl:ScoreMAPEKLendingclubGeral2575}:

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubGeral5050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Learning Fair Representations & Random Forest & Nenhum & 991 & 968 & \textbf{979} \\
Renda & Nenhum & Gradient Boosting & Equalized Odds & 988 & 969 & \textbf{978} \\
Renda & Reweighing & Random Forest & Nenhum & 991 & 963 & \textbf{977} \\
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{977} \\
Renda & Reweighing & Gradient Boosting & Nenhum & 987 & 964 & \textbf{976} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubGeral7525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Nenhum & Regressão Logística & Equalized Odds & 985 & 965 & \textbf{980} \\
Renda & Learning Fair Representations & Gradient Boosting & Nenhum & 987 & 960 & \textbf{980} \\
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{979} \\
Renda & Nenhum & Grid Search Reduction & Nenhum & 989 & 950 & \textbf{979} \\
Renda & Reweighing & Regressão Logística & Nenhum & 981 & 965 & \textbf{977} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubGeral2575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{975} \\
Renda & Nenhum & Gradient Boosting & Equalized Odds & 988 & 969 & \textbf{974} \\
Renda & Learning Fair Representations & Random Forest & Nenhum & 991 & 968 & \textbf{973} \\
Renda & Nenhum & Exponentiated Gradient Reduction & Nenhum & 986 & 966 & \textbf{971} \\
Renda & Reweighing & Gradient Boosting & Nenhum & 987 & 964 & \textbf{970} \\
\end{tabular}}
\end{center}
\end{table}

Nessas execuções, a principal observação notada foi a mudança de configurações arquiteturais com melhores desempenhos, com predominância do algoritmo \textit{Learning Fair Representations} para redução de viés e Regressão Logística para algoritmo de treinamento. Também se nota que algoritmos para redução de viés de pós-processamento e algoritmos de treinamento como \textit{Support Vector Machines} não foram tão eficientes quanto no Estudo de Caso anterior. Com isso, pode-se concluir que, ao mudar o contexto do problema e os dados envolvidos, o Gerenciador Autonômico pode ajudar a enxergar tais sutilezas e ajudar em uma decisão de forma mais eficiente e ágil. Entretanto, os dados e metadados obtidos não ajudaram a entender o porquê de tais sutilezas acontecerem.

Para processar o Lendingclub Dataset, foram necessárias modificações para adicionar este conjunto de dados como opção no Módulo de ML e realizar a evolução do sistema com base nesta adição. Estas foram contadas de acordo com seus \textit{commits} realizados no repositório e exibidos  na Tabela \ref{tbl:ManutencaoPipelineDataset}:

\begin{table}[H]
\begin{center}
  \caption{Quantidade de modificações realizadas ao adicionar um novo conjunto de dados ao Módulo de ML}
\label{tbl:ManutencaoPipelineDataset}
  \resizebox{\linewidth}{!}{%
{\def\arraystretch{1.5}
\begin{tabular}{c|c|c|c|c|c|c}
Módulo & \makecell{Quantidade de\\linhas alteradas} & Total de linhas & \makecell{Quantidade de\\arquivos alterados} & Total de arquivos & \makecell{Porcentagem de\\linhas alteradas} & \makecell{Porcentagem de\\arquivos alterados} \\
\hline
\makecell{Engenharia de Dados} & 122 & 277 & 2 & 3 & \textbf{44,04\%} & \textbf{66,67\%} \\
Módulo de ML & 76 & 1982 & 5 & 38 & \textbf{3,84\%} & \textbf{13,16\%} \\
Gerenciador Autonômico & 4 & 889 & 1 & 17 & \textbf{0,45\%} & \textbf{5,88\%} \\
\makecell{Interface} & 13 & 2905 & 2 & 14 & \textbf{0,45\%} & \textbf{14,29\%} \\
\hline
\textbf{TOTAL} & \textbf{215} & \textbf{6053} & \textbf{10} & \textbf{72} & \textbf{3,55\%} & \textbf{13,89\%} \\
\end{tabular}}}
\end{center}
\end{table}

A primeira conclusão que é possível discutir é relativa à modificação no Gerenciador Autonômico, que se baseou em parametrizações adicionais para a integração entre o Gerenciador Autonômico e a Interface e não afetou os componentes baseados na arquitetura MAPE-K. Este fato e a grande diferença entre os resultados dos Estudo de Caso 1 e 2 reforçam a autonomia proposta neste módulo, possibilitando configurações diferentes baseadas nos metadados capturados no Módulo de ML.

Os elementos na Interface exigiram poucas modificações, podendo ser resumidos a simples adições para colocar a opção do novo conjunto de dados. As maiores modificações foram realizadas no Módulo de ML e, principalmente, no módulo de Engenharia de Dados. Das modificações no Módulo de ML, a grande parte (34 linhas, ou 44,74\% das linhas) foi realizada no pré-processamento do dado para o cálculo dos algoritmos de treinamento. Embora a arquitetura \textit{Pipes-and-Filters} tenha a consequência de escrever algumas linhas a mais para elas devido a quantidade de classes criadas no Módulo de ML (2 para pipes e 1 para filtro), etapas de processamento e transformação dos dados serão o foco de tempo e modificações por parte dos Engenheiros e Cientistas de Dados.

O uso da arquitetura \textit{Pipes-and-Filters} permite o encapsulamento dos algoritmos e a separação de interesses de forma simples, fazendo com que o código presente no Módulo de ML possa ter escolhas mais elegantes para um bom \textit{Design} do código. Como exemplo disso há o Trecho \ref{cod:DataPostprocessExample}, onde há grande flexibilidade para configurar o método de pós-processamento, podendo ser expandido conforme novas classes de filtro forem implementadas com apenas a adição de um novo item no array \textbf{unbias\_postproc\_options}.

\begin{lstlisting}[language=Python, caption=Método para seleção de algoritmo para redução de viés de pós-processamento,label=cod:DataPostprocessExample]
    def data_postprocess(self,unbias_postproc_algorithm,
    						  test_pipe,prediction_pipe,fairness_pipe):
        unbias_postproc_options = [
            (UnbiasPostProcAlgorithms.EQUALIZED_ODDS,
             EqualizedOddsFilter()),
            (UnbiasPostProcAlgorithms.CALIBRATED_EQUALIZED_ODDS,
             CalibratedEqualizedOddsFilter()),
            (UnbiasPostProcAlgorithms.REJECT_OPTION_CLASSIFICATION,
             RejectOptionClassificationFilter())
        ]

        for option, filter in unbias_postproc_options:
            if unbias_postproc_algorithm == option:
                init_pipe = test_pipe + prediction_pipe +
                			fairness_pipe[
    	            			'unprivileged_group',
	                			'privileged_group'
                			]
                init_pipe >= filter == prediction_pipe
                break

        return prediction_pipe
\end{lstlisting}

Dito isso, realizar a manutenção/evolução do sistema é relativamente simples, desde que se saiba os arquivos onde as modificações serão realizadas. Por isso, a criação de uma documentação é extremamente importante para que um novo desenvolvedor entenda o todo do sistema e não adicione linhas em trechos desnecessários.

\chapter{Estudo de Caso 3: Evolução do sistema com outros desenvolvedores}

Neste Estudo de Caso, foi realizada uma evolução do sistema adicionando um novo algoritmo de classificação. O foco ainda é na manutenção do sistema, mas desta vez foi colocado para outros desenvolvedores desenvolverem a solução. A discussão se concentra na avaliação da versatilidade e simplicidade das arquiteturas escolhidas para permitir que outros desenvolvedores entendam o contexto do sistema e façam novas evoluções.

\section{Contexto e limitações}

O experimento foi realizado com o seguinte conjunto de dados e dado o seguinte contexto:

\begin{itemize}
\item \textbf{Contexto:} Obter classificação de crédito de uma pessoa (apto ou não apto), por meio de uma série de \textit{features}.

\item \textbf{Conjunto de dados:} Lendingclub Dataset~\cite{lendingclub_2022}.

\item \textbf{Transformações realizadas no conjunto de dados:} Filtragem de linhas que não definiam classificação boa ou ruim de crédito; seleção de 20 \textit{features} utilizando algoritmo de informação mútua~\cite{Ross_2014}, mais uma \textit{feature} para caracterizar dado sensível (atributo protegido) e \textit{feature} de classificação de crédito, totalizando 22 \textit{features} no total.

\item \textbf{Atributos protegidos:} Renda

\item \textbf{Grupo privilegiado:} Renda de 1 ou mais salários mínimos

\item \textbf{Grupo não-privilegiado:} Renda de menos de 1 salário mínimo

\end{itemize}

Para realizar este Estudo de Caso, foi considerado o seguinte objetivo e consideradas as seguintes limitações:

\begin{itemize}
\item \textbf{Experimento:} Realização das mesmas condições do Estudo de Caso 2 para discussão da manutenção do sistema, porém com outro desenvolvedor adicionando um novo algoritmo.

\item \textbf{Medição:} As mesmas medições do Estudo de Caso 2, atualizadas para o contexto deste Estudo de Caso.

\item \textbf{Obtenção dos dados:} Mesmas condições do Estudo de Caso 2, observações do desenvolvedor através de um questionário, e pontos de melhoria obtidos com o mesmo durante a sessão de desenvolvimento.

\item \textbf{Pré-condições:} Mesmas pré-condições do Estudo de Caso 2.

\item \textbf{Restrições:} Mesmas restrições do Estudo de Caso 2.
\end{itemize}

\section{Resultados e Discussões}

Foi realizada uma sessão de desenvolvimento com um outro desenvolvedor, com a duração de aproximadamente 1 hora e 40 minutos, onde este adicionou um novo algoritmo de classificação dentro do Módulo de ML e realizou a evolução do sistema com a documentação montada durante o Estudo de Caso anterior. Durante a sessão, foram notados e corrigidos, através de observação e \textit{feedbacks} do desenvolvedor, os seguintes itens:

\begin{itemize}
\item \textbf{Adaptações para Linux} - Como uma das máquinas que testamos rodava o sistema operacional Linux, certas estruturas de pastas mudam em relação ao Windows e tais mudanças foram atualizadas na documentação.
\item \textbf{Fixar versão de biblioteca} - Durante o período de testes, uma biblioteca foi atualizada e isto causou incompatibilidades com outras. A solução foi fixá-la no arquivo onde elas são informadas.
\item \textbf{Instalação em PCs sem GPU Nvidia e com GPU Nvidia} - Como uma das máquinas que testamos não tinha GPU Nvidia, foi colocado um modo apenas para uso de CPU. Para máquinas com GPUs Nvidia, foi colocada na documentação a orientação para instalar o CUDA Toolkit.
\item \textbf{Correções de bugs} - Foram encontrados 2 bugs durante o desenvolvimento, 1 na Interface e outro no Gerenciador Autonômico, por falta de detalhes na documentação. Foram detectados rapidamente, corrigidos e suas causas foram adicionadas na documentação.
\item \textbf{Correções na documentação} - Foram colocados modificações em métodos que faltaram constar na documentação para obter certas parametrizações dos módulos presentes na solução, além de deixar mais explícito onde cada arquivo se encontra.
\end{itemize}

Por causa de alguns itens presentes acima, uma pequena parte da sessão foi gasto em auxílios e dúvidas para que o desenvolvedor pudesse realizar o desenvolvimento sem que precisasse gastar o tempo identificando problemas que não foram de responsabilidade dele. Depois da sessão, o desenvolvedor preencheu um questionário presente no Apêndice \ref{ann:Questionary}, refletindo um perfil com certa experiência na área de dados e desenvolvimento. No geral, as impressões foram positivas e a implementação ocorreu sem dificuldades. Embora a intenção inicial era que o desenvolvedor apenas se guiasse pela documentação e o auxílio durante a sessão acabou facilitado o entendimento por parte do desenvolvedor, ele próprio considerou a adaptação a tal experimento rápida.

Outro item foi notado durante todo o desenvolvimento do sistema: alertas de vulnerabilidades das bibliotecas utilizadas no projeto e disponibilizados no GitHub. Isso reforça o fato de que o processo de Engenharia de Software para evolução do sistema é contínuo, mas que existem certas situações que estão fora do seu escopo. A própria AIF360 é uma biblioteca que implementa vários algoritmos com redução de viés baseadas em artigos disponibilizados na comunidade acadêmica mas ainda peca pela não atualização dos mesmos, fazendo com que muitos algoritmos utilizados necessitem de bibliotecas desatualizadas para funcionar.

O algoritmo de classificação escolhido para o desenvolvimento foi o Naive Bayes~\cite{Naive_Bayes_2004}, que também é bastante difundido como classificador. Após o desenvolvimento, os resultados baseados nas pré-condições e restrições já comentadas na seção anterior estão presentes abaixo nas Tabelas \ref{tbl:ScoreMAPEKLendingclubCaso35050}, \ref{tbl:ScoreMAPEKLendingclubCaso37525} e \ref{tbl:ScoreMAPEKLendingclubCaso32575}:

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 50\% Avaliação/50\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubCaso35050}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Learning Fair Representations & Random Forest & Nenhum & 991 & 968 & \textbf{979} \\
Renda & Nenhum & Gradient Boosting & Equalized Odds & 988 & 969 & \textbf{978} \\
Renda & Reweighing & Random Forest & Nenhum & 991 & 963 & \textbf{977} \\
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{977} \\
Renda & Reweighing & Gradient Boosting & Nenhum & 987 & 964 & \textbf{976} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 75\% Avaliação/25\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubCaso37525}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Nenhum & Regressão Logística & Equalized Odds & 985 & 965 & \textbf{980} \\
Renda & Learning Fair Representations & Gradient Boosting & Nenhum & 987 & 960 & \textbf{980} \\
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{979} \\
Renda & Nenhum & Grid Search Reduction & Nenhum & 989 & 950 & \textbf{979} \\
Renda & Reweighing & Regressão Logística & Nenhum & 981 & 965 & \textbf{977} \\
\end{tabular}}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
  \caption{Melhores configurações escolhidas pelo Gerenciador Autonômico \\ Uso dos algoritmos implementados - 25\% Avaliação/75\% \textit{Fairness}}
\label{tbl:ScoreMAPEKLendingclubCaso32575}
  \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\multicolumn{4}{c|}{Configuração arquitetural} & \multicolumn{3}{c}{Pontuação} \\
\hline
Atributo protegido & Pré-processamento & Treinamento & Pós-processamento & Avaliação & Fairness & \textbf{Geral} \\
\hline
Renda & Nenhum & Naive Bayes & Calibrated Equalized Odds & 991 & 976 & \textbf{980} \\
Renda & Learning Fair Representations & Regressão Logística & Nenhum & 981 & 973 & \textbf{975} \\
Renda & Nenhum & Gradient Boosting & Equalized Odds & 988 & 969 & \textbf{974} \\
Renda & Learning Fair Representations & Random Forest & Nenhum & 991 & 968 & \textbf{973} \\
Renda & Nenhum & Exponentiated Gradient Reduction & Nenhum & 986 & 966 & \textbf{971} \\
\end{tabular}}
\end{center}
\end{table}

Para o contexto do Lendingclub Dataset a adição do Naive Bayes no Módulo de ML mostrou seu valor, apresentando uma pontuação acima do esperado para configurações que privilegiam justiça e só não foi mais destacada por causa do limiar máximo estabelecido de 980. Entretanto, como já visto em Estudos de Caso anteriores, pode não funcionar em outros contextos, e os dados e metadados estabelecidos não mostraram evidências do porquê o Naive Bayes teve um comportamento positivo para este conjunto de dados.

As evoluções realizadas no Módulo de ML para adicionar o Naive Bayes foram contadas de acordo com seus \textit{commits} realizados no repositório e exibidos  na Tabela \ref{tbl:ManutencaoPipelineDataset}:

\begin{table}[H]
\begin{center}
  \caption{Quantidade de modificações realizadas ao adicionar um novo algoritmo ao Módulo de ML}
\label{tbl:ManutencaoPipelineCaso3}
  \resizebox{\linewidth}{!}{%
{\def\arraystretch{1.5}
\begin{tabular}{c|c|c|c|c|c|c}
Módulo & \makecell{Quantidade de\\linhas alteradas} & Total de linhas & \makecell{Quantidade de\\arquivos alterados} & Total de arquivos & \makecell{Porcentagem de\\linhas alteradas} & \makecell{Porcentagem de\\arquivos alterados} \\
\hline
\makecell{Engenharia de Dados} & 0 & 277 & 0 & 3 & \textbf{0,00\%} & \textbf{0,00\%} \\
Módulo de ML & 60 & 2042 & 5 & 39 & \textbf{2,94\%} & \textbf{12,82\%} \\
Gerenciador Autonômico & 2 & 891 & 2 & 17 & \textbf{0,23\%} & \textbf{11,77\%} \\
\makecell{Interface} & 71 & 2948 & 3 & 14 & \textbf{2,41\%} & \textbf{21,43\%} \\
\hline
\textbf{TOTAL} & \textbf{132} & \textbf{6157} & \textbf{9} & \textbf{72} & \textbf{2,14\%} & \textbf{12,50\%} \\
\end{tabular}}}
\end{center}
\end{table}

Desta vez, o único módulo do sistema sem necessidade de modificações foi o módulo de Engenharia de Dados. Entretanto, a modificação necessária no Gerenciador Autonômico foi a adição do Naive Bayes como parametrização, que poderia ser transferida para um arquivo externo de configuração e não exigir modificações futuramente.

No geral, foram exigidas menos modificações que a evolução proposta no Estudo de Caso anterior. O único módulo que fugiu de tal observação que exigiu mais modificações foi a Interface, em grande parte por causa de um único componente presente na tela de Configurações para Planejamento do Gerenciador Autonômico. Fora esta exceção, adicionar um algoritmo é uma evolução mais simples de ser feita, e junto com a documentação criada um desenvolvedor com relativa experiência consegue executar essa tarefa sem grandes dificuldades.

\chapter{Conclusões}

Neste projeto, a AI Reference Architecture permitiu visualizar com clareza quais pessoas, quais etapas e projetos para desenvolvimento de funcionalidades e aplicações são necessários para ir da obtenção dos dados, passando pela implantação do modelo até chegar ao consumo pelo cliente final. Deste modo, é possível traçar melhores planejamentos para desenvolvimento de uma aplicação baseada em Inteligência Artificial. 

Pode-se dizer também que o conjunto da arquitetura de \textit{Pipes-and-Filters} com a arquitetura MAPE-K se adaptou muito bem na implementação dos objetivos principais. Com a arquitetura \textit{Pipes-and-Filters}, foi possível encapsular todos os procedimentos presentes na elaboração de um modelo de ML em etapas coesas e trocá-las caso haja a necessidade de teste com outro algoritmo, atributo protegido ou conjunto de dados. Com a arquitetura MAPE-K, foi possível realizar um fluxo para que os dados obtidos no processo fossem filtrados, analisados para que haja uma tomada de decisão automática. Com a junção destes conceitos, foi possível estabelecer um processo automatizado, dependendo apenas dos próprios dados obtidos em execuções anteriores para a automação ser executada.

Ao usar a arquitetura MAPE-K para possibilitar uma escolha autônoma de algoritmos e processamento do conjunto de dados foram notadas algumas vantagens. É um modelo de organização conhecido, o que facilita a manutenção do desenvolvedor que já possui conhecimento desta arquitetura. Ela permite separar muito bem as etapas para executar um plano e, com isso, reconfigurar o Gerenciador Autonômico para executar as opções com melhores resultados. Esta separação também auxilia na manutenção, uma vez que o ciclo de monitoria, análise, planejamento, execução e obtenção de conhecimento é um formato muito bem definido para análise de dados e execução de ações, facilitando o entendimento de seu funcionamento e, consequentemente, de seu código.

Ao usar uma interface humano-computador para intermediar as interações entre o Gerenciador Autonômico e as escolhas de um usuário, foi possível obter uma melhor compreensão do funcionamento do sistema. A interface permitiu explicar melhor como funcionam as etapas, como funciona o cálculo para análise e como as configurações presentes para a etapa de planejamento afetam o resultado final. Tal compreensão do funcionamento do sistema é importante para a sustentabilidade do mesmo, sendo possível treinar novos usuários com mais eficiência e contribuir para a expansão de seu uso. O uso da interface também levou a uma conclusão inesperada: Com alguns ajustes, é possível adaptar o sistema para processos de MLOps, uma vez que a base de conhecimento gerada pode ajudar na decisão de retornar modelos mais antigos, porém com menos ruídos em seus dados e, consequentemente, melhores métricas no geral. 

\section{Limitações}

Pode-se notar alguns pontos na implementação que não irão ser resolvidos apenas pela escolha da arquitetura. Embora o \textit{Pipes-and-Filters} seja um padrão que facilite o encapsulamento e a modularização, durante o processo de desenvolvimento da solução quem desenvolve terá de equilibrar memória, armazenamento e performance. Os conjuntos de dados avaliados nos Estudos de Caso puderam ser armazenados em memória e garantir performance máxima, mas em um contexto onde \textit{gigabytes}/\textit{terabytes} de dados são consumidos diariamente é necessário sacrificar performance e realizar as operações em dispositivos de armazenamento para manter a aplicação estável e com baixo custo, evitando gastos com infra-estrutura.

Ainda não é recomendável uma autonomia completa devido aos problemas ainda enfrentados pelo tema \textit{Fairness}. A literatura revisada sobre \textit{Fairness} verifica apenas problemas de classificação binária, e para evoluções e novos métodos é provável que ocorram refatorações no arcabouço. Também há de se considerar que, mesmo que o arcabouço evolua para abrigar outros tipos de problemas, o contexto do problema é importante ao se avaliar se o modelo é considerado bom ou não. O uso de pesos para as métricas e diferentes estratégias nas fases de análise e planejamento do Gerenciador Autonômico ajudam a definir o contexto para uma avaliação, mas ainda vai depender de um Cientista de Dados e/ou de um Especialista de Domínio para entender quais as necessidades do problema analisado e se os resultados são aceitáveis para a publicação de um modelo otimizado e justo. A avaliação por seres humanos ainda garantiria a segurança e privacidade necessárias para entrar em conformidade com a lei, uma vez que há o envolvimento de dados considerados sensíveis.

A qualidade dos dados coletados pelo Gerenciador Autonômico também é um ponto que deverá ser observado conforme o uso do sistema for crescente: É possível ter resultados fora do comum (\textit{outliers}), alguns atributos podem não permitir uma avaliação acurada, ou a quantidade de atributos pode não ser suficiente para avaliar após evoluções no processo de análise. É preciso realizar processos de limpeza dos dados conforme novas execuções no Módulo de ML e atualizações no Gerenciador Autonômico forem realizadas.

Apesar da autonomia tornar o processo de decisão por uma gama de algoritmos mais simples e rápido para o Cientista de Dados, o número de configurações necessárias para dar autonomia ao arcabouço pode ser um entrave para que este opte por este tipo de abordagem e fique com a abordagem mais tradicional. Este fato, além de mostrar a importância de uma boa documentação para facilitar o entendimento da pessoa que irá usar a solução, mostra como a interface simplifica o entendimento do processo utilizado para a obtenção da configuração arquitetural e pode ser um caminho mais interessante para uma maior adoção do que a implementação de configurações no próprio projeto, que são mais simples de serem implementadas mas podem ser consideradas frustrantes para uma pessoa fora deste processo de desenvolvimento. 

\section{Trabalhos Futuros}

Como trabalhos futuros, é possível determinar algumas possibilidades para os módulos mais importantes e para o arcabouço em si:

\begin{itemize}
\item No Gerenciador Autonômico, o Analisador pode realizar uma análise mais profunda aumentando o número de indicadores, considerando grupos além de métricas de \textit{Fairness} e métricas de Avaliação e adaptações para abordagens baseadas em \textit{Logic Scoring of Preference} (LSP) e métodos de \textit{Multi-Criteria Decision Making} (MCDM). 

\item No Módulo de ML, é possível introduzir técnicas para melhora dos resultados como \textit{Data Augmentation} e \textit{K-Fold Cross-Validation} e introduzir soluções para \textit{AI Explainability} como forma de garantir mais transparência. 

\item Também é possível mudar o foco para solucionar problemas de MLOps, utilizando o Gerenciador Autonômico para determinar uma melhor implantação em caso de piora nas métricas de um modelo já utilizado por clientes, além de funcionalidades como notificação para tais casos de diminuição de métricas, mecanismos para implantação dos modelos obtidos, opções de visualização dos dados, um sistema para versionamento dos conjuntos de dados para armazenamento e economia de espaço.
\end{itemize}

% As referências:
\bibliographystyle{plain}
\bibliography{full,thesis}

% Os anexos, se houver, vêm depois das referências:
%\backmatter
\appendix
\chapter{Documentação de Instalação}
\label{ann:DocInstall}

\section{Introdução}

Esta documentação foi criada com o objetivo de guiar o desenvolvedor de Software a entender, configurar e manter este sistema, que é dividido em 5 módulos principais:

\begin{itemize}
    \item {\textbf{Engenharia de dados:}} Etapa criada com o objetivo de simular processos de transformação e limpeza de dados.
    \item {\textbf{Workflow de IA:}} Etapa para execução de um Pipeline que simula o desenvolvimento de uma aplicação automatizada de IA, desde uma categorização dos dados mais específica do que na etapa anterior, passando pelo algoritmo utilizado e finalizando obtendo métricas para determinar qualidade do resultado final.
    \item {\textbf{Gerenciador Autonômico:}} Etapa que executa um componente para automatizar todas as etapas do Workflow, com o objetivo de evitar com que perca-se tempo em execuções manuais que podem demorar dependendo do algoritimo e do conjunto de dados utilizado.
    \item {\textbf{Interface:}} Etapa criada com o objetivo de simular a etapa anterior, porém de modo a proporcionar uma experiência de usuário mais simples e intuitiva. É dividida em duas partes:
    \begin{itemize}
        \item {\textbf{Frontend:}} Parte visual, exibida em um navegador.
        \item \textbf{{Backend:}} Parte onde o Frontend se comunica para obter os dados e montar o visual corretamente, de forma que corresponda a configurações utilizadas pelo Gerenciador Autonômico.
    \end{itemize}
\end{itemize}

\section{Programas necessários para instalação}

\begin{itemize}
    \item {\textbf{Python:}} É a linguagem de programação utilizada para montar e executar todas as etapas com a exceção do Frontend da interface. É disponível no site \url{https://www.python.org/} e é necessária a versão \textbf{3.8}, \textbf{3.9} ou \textbf{3.10}. A versão \textbf{3.11} apresentou problemas de compatibilidade devido a mudanças em seu funcionamento.
    \item {\textbf{Node.js:}} É o programa necessário para montar o Frontend da interface. É disponível no site \url{https://nodejs.org/} e foi testado na versão \textbf{16.14.2}, embora outras versões podem ser executadas sem problemas de compatibilidade.
    \item {\textbf{Git:}} É o programa necessário para realizar o download do código-fonte e realizar atualizações no mesmo. É disponível no site \url{https://git-scm.com/} e foi testado na versão \textbf{2.35.1}, embora outras versões podem ser executadas sem problemas de compatibilidade.
	\item {\textbf{CUDA Toolkit:}} É a biblioteca necessária para rodar alguns dos algoritmos presentes no Workflow. É disponível no site \url{https://developer.nvidia.com/cuda-toolkit-archives} e foi testado na versão \textbf{11}, mas não houve testes se versões posteriores são compatíveis. É compatível apenas para GPUs Nvidia, caso não tiver há uma versão apenas para CPUs disponível.
\end{itemize}

\section{Instalação do sistema}

A partir desta parte, os exemplos serão realizados utilizando o Git Bash no Sistema Operacional Windows. Entretanto, no Linux e no Mac os passos são semelhantes por ambos também utilizarem esta linha de comando.

\subsection{Obtenção do código-fonte}

O sistema se encontra no repositório \url{https://github.com/tenazatto/MsC}. Para obter seu código-fonte, basta digitar o seguinte comando:

\begin{quote}\textbf{git clone https://github.com/tenazatto/MsC.git}\end{quote}

O Git baixará todos os arquivos e após o download é possível ver a pasta e seus arquivos na pasta \textbf{MsC}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{images/doc-install/git-clone.png}
\label{fig:DocInstallGitClone}
\end{figure}

\subsection{Montagem de ambiente}

Para evitar problemas de versão com bibliotecas de outros projetos instalados, é possível criar um ambiente virtual para realizar a instalação das bibliotecas separadamente. Para criar, é necessário o \textbf{virtualenv} instalado no Python. Caso ele não esteja instalado, ele é obtido através do comando:

\begin{quote}\textbf{pip install virtualenv}\end{quote}

Para criar um novo ambiente virtual, é preciso digitar o comando

\begin{quote}\textbf{python3 -m venv ./(nome do ambiente)}\end{quote}

Como exemplo, nesta documentação foi criado o documento \textbf{testenv}

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{images/doc-install/virtualenv-sh.png}
\label{fig:DocInstallVirtualEnvShell}
\end{figure}

Após o término, aparecerá uma nova pasta de mesmo nome

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{images/doc-install/virtualenv.png}
\label{fig:DocInstallVirtualEnvFolder}
\end{figure}

Após criar o ambiente virtual, é preciso ativá-lo para utilizar

\begin{quote}
\textbf{.\textbackslash (nome do ambiente)\textbackslash Scripts\textbackslash activate.bat} (Windows - Prompt de Comando)
\end{quote}
\begin{quote}
\textbf{source ./(nome do ambiente)/Scripts/activate} (Windows - Bash)
\end{quote}
\begin{quote}
\textbf{source ./(nome do ambiente)/bin/activate} (Linux)
\end{quote}

Para verificar se o ambiente foi ativado, é possível verificar, ao digitar qualquer comando no bash, que o nome do ambiente virtual aparece logo abaixo.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.4]{images/doc-install/virtualenv-check.png}
\label{fig:DocInstallVirtualEnvCheck}
\end{figure}

Para desativar o ambiente virtual, é preciso digitar o comando

\begin{quote}\textbf{deactivate}\end{quote}

Para verificar se o ambiente foi desativado, é possível verificar, ao digitar qualquer comando no bash, que o nome do ambiente virtual não irá mais aparecer até ser ativado novamente.

No caso do Node.js não é necessário realizar tais etapas, pois a instalação das bibliotecas nesta documentação é realizada de maneira local

\subsection{Instalação das bibliotecas}

\subsubsection{Python}

Com o ambiente virtual criado e ativado, é possível utilizar os arquivos \textbf{src/requirements.txt}, dependendo das configurações da sua máquina, para instalar todas as bibliotecas necessárias através do comando

\begin{quote}\textbf{pip install -r ./src/requirements-nvidia.txt} (GPU Nvidia)\end{quote}
\begin{quote}\textbf{pip install -r ./src/requirements-cpu.txt} (CPU)\end{quote}

Estes arquivos foram preparados apenas para rodar de acordo com o hardware apresentado. A execução de ambos os comandos não é necessária e nem recomendável.

\subsubsection{Node.js}

Para o Node.js, como o arquivo \textbf{package.json} está dentro da pasta \textbf{ml-ui}, é possível acessar essa pasta e digitar o comando

\begin{quote}\textbf{npm install}\end{quote}

\section{Execução do sistema}

\subsection{Engenharia de dados}

Dentro da pasta \textbf{MsC} e com o ambiente virtual criado e ativado, para rodar a etapa de Engenharia de dados basta digitar o seguinte comando

\begin{quote}\textbf{python -m src.data\_engineering.data\_engineering\_start -{}-data (Opção)}\end{quote}

No momento, há 3 opções disponíveis:

\begin{itemize}
    \item {\textbf{GERMAN\_CREDIT:}} Manipula o German Credit Dataset, cujo arquivo está na localização \textbf{datasets/german.data}, para utilização no Workflow.
    \item {\textbf{LENDINGCLUB:}} Baixa e manipula o Lendingclub Dataset para utilização no Workflow.
    \item {\textbf{METRICS:}} Obtém o maior valor, menor valor e a média de cada métrica para cada Workflow já executado.
\end{itemize}

\subsection{Workflow de IA}

Dentro da pasta \textbf{MsC} e com o ambiente virtual criado e ativado, para rodar todos os Workflows possíveis basta digitar o seguinte comando

\begin{quote}\textbf{python -m src.pipeline.pipeline\_start -{}-dataset (Opção)}\end{quote}

No momento, há 4 opções disponíveis:

\begin{itemize}
    \item {\textbf{ADULT\_INCOME\_SEX:}} Executa os Workflows para o Adult Income Dataset, cujo arquivo está na localização \textbf{datasets/adult.csv}, utilizando Sexo (Masculino/Feminino) como atributo protegido.
    \item {\textbf{GERMAN\_CREDIT\_FOREIGN:}} Executa os Workflows para o German Credit Dataset, cujo arquivo é manipulado na etapa anterior, utilizando Nacionalidade (Alemão/Estrangeiro) como atributo protegido.
    \item {\textbf{GERMAN\_CREDIT\_AGE:}} Executa os Workflows para o German Credit Dataset, cujo arquivo é manipulado na etapa anterior, utilizando Idade (-25 anos/25 ou + anos) como atributo protegido.
    \item {\textbf{LENDINGCLUB\_INCOME:}} Executa os Workflows para o Lendingclub Dataset, cujo arquivo é manipulado na etapa anterior, utilizando Renda (-1 salário mínimo/1 ou + salários mínimos) como atributo protegido.
\end{itemize}

Após a execução, é possível ver a geração das métricas dentro da pasta \textbf{output/metrics}, necessárias para a execução da próxima etapa.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{images/doc-install/metrics.png}
\label{fig:DocInstallMetrics}
\end{figure}

\subsection{Autonomia do Workflow}

Dentro da pasta \textbf{MsC}, com o ambiente virtual criado e ativado e com pelo menos um Workflow executado, é possível verificar como a etapa de autonomia funciona com o seguinte comando

\begin{quote}\textbf{python -m src.mapek.mapek\_start}\end{quote}

Nesta etapa, ele escolhe o Workflow que apresentou as melhores métricas, porém a filtragem por conjunto de dados está desenvolvida até o momento apenas na próxima etapa. Como ele roda ininterruptamente, é preciso interromper sua execução.

\subsection{Interface}

\subsubsection{Backend}

Dentro da pasta \textbf{MsC}, com o ambiente virtual criado e ativado e com pelo menos um Workflow executado, é possível rodar o Backend da interface com o seguinte comando

\begin{quote}\textbf{python -m src.api.flask\_start}\end{quote}

Ele vai iniciar um servidor na porta 8080, necessário para rodar as requisições que o Frontend vai solicitar

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{images/doc-install/backend-start.png}
\label{fig:DocInstallVirtualEnvBackendStart}
\end{figure}

\subsubsection{Frontend}

Dentro da pasta \textbf{MsC/ml-ui}, é possível rodar o Frontend da interface com o seguinte comando

\begin{quote}\textbf{npm start}\end{quote}

Ele vai iniciar o navegador acessando um servidor na porta 3000, e deverá iniciar a tela no menu de Análise

\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{images/doc-install/ml-ui.png}
\label{fig:DocInstallMLUI}
\end{figure}

\chapter{Documentação de Manutenção}
\label{ann:DocMain}

\section{Introdução}

Esta documentação foi criada com o objetivo de guiar o desenvolvedor de Software a entender, configurar e manter este sistema, que é dividido em 4 módulos principais:

\begin{itemize}
    \item {\textbf{Engenharia de dados:}} Etapa criada com o objetivo de simular processos de transformação e limpeza de dados.
    \item {\textbf{Workflow de IA:}} Etapa para execução de um Pipeline que simula o desenvolvimento de uma aplicação automatizada de IA, desde uma categorização dos dados mais específica do que na etapa anterior, passando pelo algoritmo utilizado e finalizando obtendo métricas para determinar qualidade do resultado final.
    \item {\textbf{Autonomia do Workflow (Gerenciador Autonômico):}} Etapa que executa um componente para automatizar todas as etapas do Workflow, com o objetivo de evitar com que perca-se tempo em execuções manuais que podem demorar dependendo do algoritimo e do conjunto de dados utilizado.
    \item {\textbf{Interface:}} Etapa criada com o objetivo de simular a etapa anterior, porém de modo a proporcionar uma experiência de usuário mais simples e intuitiva. É dividida em duas partes:
    \begin{itemize}
        \item {\textbf{Frontend:}} Parte visual, exibida em um navegador.
        \item \textbf{{Backend:}} Parte onde o Frontend se comunica para obter os dados e montar o visual corretamente, de forma que corresponda a configurações utilizadas pelo Gerenciador Autonômico.
    \end{itemize}
\end{itemize}

\section{Arquitetura do Workflow e Framework}

O Workflow usa uma arquitetura chamada de Pipes-and-Filters, onde Pipes, que transportam os dados, são ligados por Filters, que realizam as manipulações desses dados. Pipes e Filters se encadeiam para formar uma sequência de operações, caracterizando todo o Pipeline. Para auxiliar no desenvolvimento, um pequeno framework foi criado, para facilitar as operações necessárias entre Pipes e Filters. 

\subsection{Estrutura}

Pipes herdam a classe \textbf{BasePipe}. Essa classe possui o atributo \textbf{value}, que caracteriza os dados transformados, em formato de um dicionário Python

\begin{lstlisting}[language=Python, label=cod:FairnessPipeClass]
class FairnessPipe(BasePipe):
    privileged\_group = []
    unprivileged_group = []

    label_names = []
    protected_attribute_names = []

    optim_options = {}

    def __init__(self):
        self.value = {
            'privileged_group': self.privileged_group,
            'unprivileged_group': self.unprivileged_group,
            'label_names': self.label_names,
            'protected_attribute_names': self.protected_attribute_names,
            'optim_options': self.optim_options
        }
\end{lstlisting}

Filters herdam a classe \textbf{BaseFilter}. Essa classe possui dois Pipes (input e output) e um método chamado \textbf{execute}, que é onde as operações de transformação do Pipe de input são executadas para serem colocadas no Pipe de output

\begin{lstlisting}[language=Python, label=cod:TrainTestSplitClass]
from sklearn.model_selection import train_test_split

from src.pipeline.pipe_filter.pipe import BaseFilter


class TrainTestSplit(BaseFilter):
    test_size = 0.2

    def __init__(self, test_size=0.2):
        self.test_size = test_size

    def execute(self):
        df_x = self.input['df_x']
        df_y = self.input['df_y']

        x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=self.test_size, random_state=42)

        self.output = {
            'x_train': x_train,
            'x_test': x_test,
            'y_train': y_train,
            'y_test': y_test,
            'checksum': self.input['checksum']
        }
\end{lstlisting}

\subsection{Operações}

No Framework, estão presentes as seguintes operações:

\subsection{Ligação de Pipe com Filter}

Para juntar um Pipe com um Filter, basta realizar o comando \textbf{>=}, caracterizando o transporte a um Filter.

\begin{lstlisting}[language=Python, label=cod:PipeToFilter]
    Pipe1() >= Filter1()
\end{lstlisting}

Se o Pipe estiver carregando os dados corretos, o Filter será automaticamente executado.

\subsection{Ligação de Filter com Pipe}

Para juntar um Filter com um Pipe, basta realizar o comando \textbf{==}, caracterizando o transporte a um Pipe.

\begin{lstlisting}[language=Python, label=cod:FilterToPipe]
    Filter1() == Pipe1()
\end{lstlisting}

Se o Filter esiver corretamente implementado, o Pipe será caracterizado como a saída desse mesmo Filter.

\subsection{Seleção parcial de dados presentes no Pipe}

Para selecionar apenas alguns atributos presentes no Pipe, basta adicionar colchetes e colocar os campos desejados.

\begin{lstlisting}[language=Python, label=cod:PartialPipe]
    pipe1['campo1', 'campo2', 'campo3']
\end{lstlisting}

\subsection{Junção de Pipes}

Pra juntar os dados de dois pipes em um só, basta realizar o comando \textbf{+}, caracterizando uma junção de Pipes.

\begin{lstlisting}[language=Python, label=cod:MergePipes]
    pipe1 + pipe2
\end{lstlisting}

\section{Incrementos no Workflow}

Aqui estão dois exemplos de como é possível realizar a manuntenção do Workflow

\subsection{Adicionando um novo Conjunto de Dados}

\subsubsection{Engenharia de dados}

\begin{enumerate}
\item Geralmente conjuntos de dados não vem filtrados, e é preciso um trabalho de Engenharia de dados para realizar experimentos com melhores resultados. Neste sistema, a parte de Engenharia de dados se encontra na pasta \textbf{src/data\_engineering}.

\item Os métodos onde os processamentos são realizados ficam no arquivo \textbf{data\_engi\-neering.py}. Para adicionar um novo processamento, basta adicionar um novo método neste arquivo.

\item Importar o novo método no arquivo \textbf{data\_engineering\_start.py}.

\item Adicionar uma nova opção no parâmetro \textbf{choices} presente no método \textbf{parser.add\-\_argument}

\begin{lstlisting}[language=Python, label=cod:ParserAddArgument]
    parser.add_argument("--data", help="Selecao do gerador do conjunto de dados tratado",
                        choices=['GERMAN_CREDIT', 'LENDINGCLUB', 'METRICS'])
\end{lstlisting}

\item Adicionar uma nova condição com a opção adicionada
\end{enumerate}

\subsubsection{Workflow}

\begin{enumerate}
\item Neste sistema, a parte do Workflow se encontra na pasta \textbf{src/pipeline}. Dentro dela, os Pipes que armazenam os conjuntos de dados se encontram na pasta \textbf{processors/preprocessors/data}. Dentro dela, no arquivo \textbf{dataset.py}, criar uma classe que herda a classe \textbf{Dataset}, preenchendo os atributos \textbf{dataset\_path} com o caminho do arquivo.

\begin{lstlisting}[language=Python, label=cod:FairnessPipe]
class LendingclubDataset(Dataset):
    dataset_path = 'datasets/lendingclub_dataset.csv'

    def __init__(self):
        super().__init__()
\end{lstlisting}

\item Criar um novo arquivo.

\item Criar uma classe que herda a classe \textbf{FairnessPipe}, preenchendo os atributos \textbf{privileged\_group}, \textbf{unprivileged\_group}, \textbf{label\_names}, \textbf{protected\_attribute\-\_names} e \textbf{optim\_options}.

\begin{lstlisting}[language=Python, label=cod:FairnessPipe]
class LendingclubIncomeFairnessPipe(FairnessPipe):
    privileged_group = [{'annual_inc': 1}]
    unprivileged_group = [{'annual_inc': 0}]

    label_names = ['loan_status']
    protected_attribute_names = ['annual_inc']

    optim_options = {
        "distortion_fun": get_distortion_german,
        "epsilon": 0.05,
        "clist": [0.99, 1.99, 2.99],
        "dlist": [.1, 0.05, 0]
    }

    def __init__(self):
        super().__init__()
\end{lstlisting}

\item Criar uma classe que herda a classe \textbf{FairnessPreprocessor}, preenchendo o método \textbf{dataset\_preprocess} e retornando um DataFrame Pandas de dados de entrada e um DataFrame de dados de saída.

\begin{lstlisting}[language=Python, label=cod:FairnessPreprocessor]
class LendingclubIncomePreprocessor(FairnessPreprocessor):
    def dataset_preprocess(self, df):
        df.info()

        SAMPLE_PERCENTAGE = 100
        df_sample_nok = df[df['loan_status'] == 'Charged Off'].sample(frac=SAMPLE_PERCENTAGE/100)
        df_sample_ok = df[df['loan_status'] == 'Fully Paid'].sample(frac=SAMPLE_PERCENTAGE / 100)
        df_sample = pd.concat([df_sample_ok, df_sample_nok])

        df_x = df_sample.drop('loan_status', axis=1)
        df_y = pd.DataFrame(df_sample.loan_status)

        return df_x, df_y
\end{lstlisting}

\item Na pasta \textbf{src/pipeline/processors} e dentro do arquivo \textbf{enums.py}, colocar novas opções nas classes \textbf{Datasets} e \textbf{Preprocessors}.

\begin{lstlisting}[language=Python, label=cod:EnumOptions]
class Datasets(ExtendedEnum):
    ADULT_INCOME = 1
    GERMAN_CREDIT = 2
    LENDINGCLUB = 3


class Preprocessors(ExtendedEnum):
    SEX = 1
    AGE = 2
    FOREIGN = 3
    INCOME = 4
\end{lstlisting}

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{validation.py}, atualizar a variável \textbf{existant\_preprocessors} com as novas opções colocadas no item anterior.

\begin{lstlisting}[language=Python, label=cod:ValidationPreprocessors]
        existant_preprocessors = \
            (dataset == Datasets.ADULT_INCOME and preprocessor == Preprocessors.SEX) or \
            (dataset == Datasets.GERMAN_CREDIT and preprocessor == Preprocessors.AGE) or \
            (dataset == Datasets.GERMAN_CREDIT and preprocessor == Preprocessors.FOREIGN) or \
            (dataset == Datasets.LENDINGCLUB and preprocessor == Preprocessors.INCOME)
\end{lstlisting}

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{pipeline.py}, encontrar o método \textbf{select\_data\_preprocessor}, atualizar a variável \textbf{options} com as novas opções e classes implementadas nos itens anteriores.

\begin{lstlisting}[language=Python, label=cod:PipelinePreprocessors]
    def select_data_preprocessor(self, dataset, preprocessor):
        choice = [dataset, preprocessor]
        options = [
            ([Datasets.ADULT_INCOME, Preprocessors.SEX], (AdultDataset(), AdultSexPreprocessor(), AdultSexFairnessPipe())),
            ([Datasets.GERMAN_CREDIT, Preprocessors.AGE], (GermanDataset(), GermanAgePreprocessor(), GermanAgeFairnessPipe())),
            ([Datasets.GERMAN_CREDIT, Preprocessors.FOREIGN], (GermanDataset(), GermanForeignPreprocessor(), GermanForeignFairnessPipe())),
            ([Datasets.LENDINGCLUB, Preprocessors.INCOME], (LendingclubDataset(), LendingclubIncomePreprocessor(), LendingclubIncomeFairnessPipe())),
        ]

        for option, pipe_filter in options:
            if choice == option:
                dataset_pipe, data_preprocessor_filter, fairness_pipe = pipe_filter
                preprocessed_data_pipe = dataset_pipe >= data_preprocessor_filter == self.new_pipe()
                break

        return preprocessed_data_pipe, fairness_pipe
\end{lstlisting}

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{pipeline\_start.py}, adicionar uma nova opção no parâmetro \textbf{choices} presente no método \textbf{parser.add\_argument}.

\begin{lstlisting}[language=Python, label=cod:ParserAddArgumentPipeline]
    parser.add_argument("--dataset", help="Conjunto de dados tratado com atributo protegido",
                        choices=['ADULT_INCOME_SEX',
                                 'GERMAN_CREDIT_FOREIGN', 'GERMAN_CREDIT_AGE',
                                 'LENDINGCLUB_INCOME'])
\end{lstlisting}

\item No mesmo arquivo, adicionar uma nova condição com a opção adicionada.

\begin{lstlisting}[language=Python, label=cod:IfPipelineStart]
    if args.dataset == 'ADULT_INCOME_SEX':
        datasets.append((Datasets.ADULT_INCOME, Preprocessors.SEX))
    elif args.dataset == 'ADULT_INCOME_FOREIGN':
        datasets.append((Datasets.GERMAN_CREDIT, Preprocessors.FOREIGN))
    elif args.dataset == 'GERMAN_CREDIT_AGE':
        datasets.append((Datasets.GERMAN_CREDIT, Preprocessors.AGE))
    elif args.dataset == 'LENDINGCLUB_INCOME':
        datasets.append((Datasets.LENDINGCLUB, Preprocessors.INCOME))]
\end{lstlisting}

\end{enumerate}

\subsubsection{Interface (Backend)}

\begin{enumerate}
\item Neste sistema, a parte do Backend se encontra na pasta \textbf{src/api}. Dentro dela, no arquivo \textbf{repo/pipeline.py}, adicionar a opção no método \textbf{get\_dataset}.

\begin{lstlisting}[language=Python, label=cod:AddDataset]
    def get_dataset(self, dataset):
        indexes = {
            'Datasets.ADULT_INCOME': Datasets.ADULT_INCOME,
            'Datasets.GERMAN_CREDIT': Datasets.GERMAN_CREDIT,
            'Datasets.LENDINGCLUB': Datasets.LENDINGCLUB,
        }

        return next(filter(lambda a: a[0] == dataset, indexes.items()))[1]
\end{lstlisting}

\item Na pasta \textbf{src/api} e dentro do arquivo \textbf{repo/pipeline.py}, adicionar a opção no método \textbf{get\_preprocessor}.

\begin{lstlisting}[language=Python, label=cod:AddDataset]
    def get_preprocessor(self, preprocessor):
        indexes = {
            'Preprocessors.SEX': Preprocessors.SEX,
            'Preprocessors.AGE': Preprocessors.AGE,
            'Preprocessors.FOREIGN': Preprocessors.FOREIGN
        }

        return next(filter(lambda a: a[0] == preprocessor, indexes.items()))[1]
\end{lstlisting}
\end{enumerate}

\subsubsection{Interface (Frontend)}

\begin{enumerate}
\item Neste sistema, a parte do Frontend se encontra na pasta \textbf{ml-ui/src}. Dentro dela, no arquivo \textbf{Auto-Pipeline-Menu.js}, adicionar a opção colocada na etapa de Workflow no componente Select onde estão as outras opções de conjunto de dados.

\begin{lstlisting}[language=Python, label=cod:AddDatasetAuto]
<Select
  sx={{fontSize: '14px'}}
  value={dataset}
  onChange={handleDatasetChange}
  displayEmpty
  inputProps={{ 'aria-label': 'Without label' }}
>
  <MenuItem value={'Datasets.ADULT_INCOME'}>Adult Income Dataset</MenuItem>
  <MenuItem value={'Datasets.GERMAN_CREDIT'}>German Credit Dataset</MenuItem>
  <MenuItem value={'Datasets.LENDINGCLUB'}>Lendingclub Dataset</MenuItem>
</Select>
\end{lstlisting}

\item Na pasta \textbf{ml-ui/src} e dentro do arquivo \textbf{Manual-Pipeline-Menu.js}, adicionar a opção colocada na etapa de Workflow no componente Select onde estão as outras opções de conjunto de dados.

\begin{lstlisting}[language=Python, label=cod:AddDatasetManual]
<Select
  sx={{fontSize: '14px'}}
  value={dataset}
  onChange={handleDatasetChange}
  displayEmpty
  inputProps={{ 'aria-label': 'Without label' }}
>
  <MenuItem value={'Datasets.ADULT_INCOME'}>Adult Income Dataset</MenuItem>
  <MenuItem value={'Datasets.GERMAN_CREDIT'}>German Credit Dataset</MenuItem>
  <MenuItem value={'Datasets.LENDINGCLUB'}>Lendingclub Dataset</MenuItem>
</Select>
\end{lstlisting}

\item Na pasta \textbf{ml-ui/src} e dentro do arquivo \textbf{Manual-Pipeline-Menu.js}, adicionar a opção colocada na etapa de Workflow no componente Select onde estão as outras opções de pré-processadores.

\begin{lstlisting}[language=Python, label=cod:AddDatasetManual]
<FormControl sx={{ m: 1, width: 300, marginLeft: '35px' }}>
  {dataset === 'Datasets.ADULT_INCOME' ?
    <Select
      sx={{fontSize: '14px'}}
      value={protectedAtt}
      onChange={handleProtectedAttChange}
      displayEmpty
      inputProps={{ 'aria-label': 'Without label' }}
    >
      <MenuItem value={'Preprocessors.SEX'}>Sexo (Masculino/Feminino)</MenuItem>
    </Select>
  : dataset === 'Datasets.ADULT_INCOME' ?
    <Select
      sx={{fontSize: '14px'}}
      value={protectedAtt}
      onChange={handleProtectedAttChange}
      displayEmpty
      inputProps={{ 'aria-label': 'Without label' }}
    >
      <MenuItem value={'Preprocessors.AGE'}>Idade (-25 anos/+25 anos)</MenuItem>
      <MenuItem value={'Preprocessors.FOREIGN'}>Nacionalidade (Local/Estrangeiro)</MenuItem>
    </Select>
  :
    <Select
      sx={{fontSize: '14px'}}
      value={protectedAtt}
      onChange={handleProtectedAttChange}
      displayEmpty
      inputProps={{ 'aria-label': 'Without label' }}
    >
      <MenuItem value={'Preprocessors.INCOME'}>Renda (-1 Salario Minimo/1+ Salarios Minimos)</MenuItem>
    </Select>
  }
  <FormHelperText>Atributo protegido para medir justica</FormHelperText>
</FormControl>
\end{lstlisting}

\item Na pasta \textbf{ml-ui/src} e dentro do arquivo \textbf{Manual-Pipeline-Menu.js} adicionar a condição para a opção colocada na etapa de Workflow no método \textbf{handleDatasetChange}.

\begin{lstlisting}[language=Python, label=cod:AddDatasetManual]
  const handleDatasetChange = (event) => {
    setDataset(event.target.value);

    if (event.target.value === 'Datasets.ADULT_INCOME') {
      setProtectedAtt('Preprocessors.SEX');
    } else if (event.target.value === 'Datasets.GERMAN_CREDIT') {
      setProtectedAtt('Preprocessors.AGE');
    } else {
      setProtectedAtt('Preprocessors.INCOME');
    }
  }
\end{lstlisting}
\end{enumerate}

\subsection{Adicionando um novo Algoritmo}

\subsubsection{Workflow}

\begin{enumerate}
\item Neste sistema, a parte do Workflow se encontra na pasta \textbf{src/pipeline}. Dentro dela, os Filters que executam os algoritmos ficam dentro da pasta \textbf{processors}. Dentro dela, no arquivo \textbf{enums.py}, colocar novas opções na classe \textbf{Algorithms}.

\begin{lstlisting}[language=Python, label=cod:EnumAlgorithmOptions]
class Algorithms:
    LOGISTIC_REGRESSION = 1
    RANDOM_FOREST = 2
    GRADIENT_BOOST = 3
    SUPPORT_VECTOR_MACHINES = 4
    LINEAR_REGRESSION = 901
    DECISION_TREE = 902
    KERNEL_RIDGE = 903
\end{lstlisting}

A classe \textbf{Algorithms} serve para este exemplo em questão, mas para outros tipos de algoritmos as classes \textbf{UnbiasDataAlgorithms}, \textbf{UnbiasInProcAlgorithms} ou \textbf{UnbiasPostProcAlgorithms} podem ser mais adequadas.

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{validation.py}, adicionar uma nova condição para a variável \textbf{existant\_algorithms} no método \textbf{validate\_params}.

\begin{lstlisting}[language=Python, label=cod:ValidationAddArgumentPipeline]
            existant_algorithms = \
            (algorithm == Algorithms.LOGISTIC_REGRESSION and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (...)
            (algorithm == UnbiasInProcAlgorithms.GRID_SEARCH_REDUCTION and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.EQUALIZED_ODDS) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.CALIBRATED_EQUALIZED_ODDS) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.NOTHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.REJECT_OPTION_CLASSIFICATION) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.REWEIGHING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.DISPARATE_IMPACT_REMOVER and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.OPTIMIZED_PREPROCESSING and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING) or \
            (algorithm == Algorithms.NOVA_OPCAO and unbias_data_algorithm == UnbiasDataAlgorithms.LEARNING_FAIR_REPRESENTATIONS and unbias_postproc_algorithm == UnbiasPostProcAlgorithms.NOTHING)
\end{lstlisting}

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{pipeline.py}, colocar as novas opções colocadas no item anterior no método \textbf{find\_algorithm}.

\begin{lstlisting}[language=Python, label=cod:FindAlgorithm]
    def find_algorithm(self, algorithm):
        indexes = {
            'Algorithms.LOGISTIC_REGRESSION': 1,
            'Algorithms.RANDOM_FOREST': 2,
            'Algorithms.GRADIENT_BOOST': 3,
            'Algorithms.SUPPORT_VECTOR_MACHINES': 4,
            'Algorithms.LINEAR_REGRESSION': 901,
            'Algorithms.DECISION_TREE': 902,
            'Algorithms.KERNEL_RIDGE': 903,
            'UnbiasInProcAlgorithms.PREJUDICE_REMOVER': 101,
            'UnbiasInProcAlgorithms.ADVERSARIAL_DEBIASING': 102,
            'UnbiasInProcAlgorithms.EXPONENTIATED_GRADIENT_REDUCTION': 103,
            'UnbiasInProcAlgorithms.RICH_SUBGROUP_FAIRNESS': 104,
            'UnbiasInProcAlgorithms.GRID_SEARCH_REDUCTION': 105,
            'UnbiasInProcAlgorithms.META_FAIR_CLASSIFIER': 106,
            'UnbiasInProcAlgorithms.ART_CLASSIFIER': 107
        }

        return next(filter(lambda a: a[1] == algorithm, indexes.items()))[0]
\end{lstlisting}

\item Criar um novo arquivo na pasta categorizada pelo algoritmo a ser implementado. Para o exemplo exemplo ilustrado abaixo, como o Gradient Boosting é um algoritmo de treinamento e não foi projetado para redução de viés, os Filters que executam os algoritmos se encontram na pasta \textbf{processors/inprocessors/inproc\_algorithms}.

Seguem as pastas onde os respectivos Filters se encontram:

\begin{itemize}
\item \textbf{Algoritmo de treinamento sem redução de viés:} processors/inprocessors/inproc\_algorithms
\item \textbf{Algoritmo com redução de viés no dado (Pré-processamento):} processors/preprocessors/unbias\_algorithms
\item \textbf{Algoritmo com redução de viés no treinamento (Processamento):} processors/inprocessors/unbias\_algorithms
\item \textbf{Algoritmo com redução de viés no resultado (Pós-processamento):} processors/postprocessors
\end{itemize}

\item No arquivo criado, criar uma classe que herda a classe \textbf{BaseFilter}.

\begin{lstlisting}[language=Python, label=cod:BaseFilter]
class GradientBoostFilter(BaseFilter):
    weighed = False

    def __init__(self, weighed=False):
        self.weighed = weighed
\end{lstlisting}

\item Implementar nesta classe o método \textbf{execute}, atribuindo em \textbf{self.output} um dicionário Python com os atributos \textbf{y\_pred} e \textbf{scores}.

\begin{lstlisting}[language=Python, label=cod:FilterExecute]
    def execute(self):
        y_pred, scores = self.gradient_boost_weighed() if self.weighed else self.gradient_boost()

        self.output = {
            'y_pred': y_pred,
            'scores': scores
        }
\end{lstlisting}

\item Na pasta \textbf{src/pipeline} e dentro do arquivo \textbf{pipeline.py}, encontrar o método \textbf{process}, atualizar a variável \textbf{process\_options} com as novas opções e classes implementadas nos itens anteriores.

\begin{lstlisting}[language=Python, label=cod:ProcessAlgorithm]
    def process(self, process_pipe, algorithm, unbias_data_algorithm):
        weighed_algorithm = unbias_data_algorithm == UnbiasDataAlgorithms.REWEIGHING or \
                            unbias_data_algorithm == UnbiasDataAlgorithms.LEARNING_FAIR_REPRESENTATIONS

        process_options = [
            (Algorithms.LOGISTIC_REGRESSION, LogisticRegressionFilter(weighed=weighed_algorithm)),
            (Algorithms.RANDOM_FOREST, RandomForestFilter(weighed=weighed_algorithm)),
            (Algorithms.GRADIENT_BOOST, GradientBoostFilter(weighed=weighed_algorithm)),
            (Algorithms.SUPPORT_VECTOR_MACHINES, SVMFilter(weighed=weighed_algorithm)),
            (UnbiasInProcAlgorithms.PREJUDICE_REMOVER, PrejudiceRemoverFilter()),
            (UnbiasInProcAlgorithms.ADVERSARIAL_DEBIASING, AdversarialDebiasingFilter()),
            (UnbiasInProcAlgorithms.EXPONENTIATED_GRADIENT_REDUCTION, ExponentiatedGradientReductionFilter(algorithm=Algorithms.GRADIENT_BOOST)),
            (UnbiasInProcAlgorithms.RICH_SUBGROUP_FAIRNESS, RichSubgroupFairnessFilter(algorithm=Algorithms.DECISION_TREE)),
            (UnbiasInProcAlgorithms.META_FAIR_CLASSIFIER, MetaFairClassifierFilter()),
            (UnbiasInProcAlgorithms.GRID_SEARCH_REDUCTION, GridSearchReductionFilter(algorithm=Algorithms.RANDOM_FOREST))
        ]

        for option, filter in process_options:
            if algorithm == option:
                prediction_pipe = process_pipe >= filter == self.new_pipe()
                break

        return prediction_pipe
\end{lstlisting}

O método \textbf{process} serve para este exemplo em questão, mas para outros tipos de algoritmos os métodos \textbf{unbias\_data\_preprocessor} ou \textbf{data\_postprocess} podem ser mais adequados.

\item Na pasta \textbf{src/pipeline} dentro do arquivo \textbf{pipeline\_start.py}, adicionar as opções (adaptadas a opção corrente) na variável \textbf{process\_options}.

\begin{lstlisting}[language=Python, label=cod:ParserAddArgumentPipeline]
    process_options = [
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.NOTHING, UnbiasPostProcAlgorithms.NOTHING),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.NOTHING, UnbiasPostProcAlgorithms.EQUALIZED_ODDS),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.NOTHING,
         UnbiasPostProcAlgorithms.CALIBRATED_EQUALIZED_ODDS),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.NOTHING,
         UnbiasPostProcAlgorithms.REJECT_OPTION_CLASSIFICATION),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.REWEIGHING, UnbiasPostProcAlgorithms.NOTHING),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.DISPARATE_IMPACT_REMOVER,
         UnbiasPostProcAlgorithms.NOTHING),
        # (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.OPTIMIZED_PREPROCESSING, UnbiasPostProcAlgorithms.NOTHING),
        (Algorithms.NOVA_OPCAO, UnbiasDataAlgorithms.LEARNING_FAIR_REPRESENTATIONS,
         UnbiasPostProcAlgorithms.NOTHING)
    ]
\end{lstlisting}

\end{enumerate}

\subsubsection{MAPE-K}

\begin{enumerate}
\item Neste sistema, a parte do Backend se encontra na pasta \textbf{src/mapek}. Dentro dela, no arquivo \textbf{ml/planner.py}, colocar as novas opções colocadas no item anterior no método \textbf{find\_inproc\_algorithm}.

\begin{lstlisting}[language=Python, label=cod:findInprocAlgorithm]
    def find_inproc_algorithm(self, algorithm):
        indexes = {
            'Algorithms.LOGISTIC_REGRESSION': 1,
            'Algorithms.RANDOM_FOREST': 2,
            'Algorithms.GRADIENT_BOOST': 3,
            'Algorithms.SUPPORT_VECTOR_MACHINES': 4,
            'Algorithms.LINEAR_REGRESSION': 901,
            'Algorithms.DECISION_TREE': 902,
            'Algorithms.KERNEL_RIDGE': 903,
            'UnbiasInProcAlgorithms.PREJUDICE_REMOVER': 101,
            'UnbiasInProcAlgorithms.ADVERSARIAL_DEBIASING': 102,
            'UnbiasInProcAlgorithms.EXPONENTIATED_GRADIENT_REDUCTION': 103,
            'UnbiasInProcAlgorithms.RICH_SUBGROUP_FAIRNESS': 104,
            'UnbiasInProcAlgorithms.GRID_SEARCH_REDUCTION': 105,
            'UnbiasInProcAlgorithms.META_FAIR_CLASSIFIER': 106,
            'UnbiasInProcAlgorithms.ART_CLASSIFIER': 107
        }

        return next(filter(lambda a: a[0] == algorithm, indexes.items()))[1]
\end{lstlisting}
\end{enumerate}

\subsubsection{Interface (Backend)}

\begin{enumerate}
\item Neste sistema, a parte do Backend se encontra na pasta \textbf{src/api}. Dentro dela, no arquivo \textbf{repo/pipeline.py}, colocar as novas opções colocadas no item anterior no método \textbf{get\_inproc\_algorithm}.

\begin{lstlisting}[language=Python, label=cod:getInprocAlgorithm]
    def get_inproc_algorithm(self, algorithm):
        indexes = {
            'Algorithms.LOGISTIC_REGRESSION': 1,
            'Algorithms.RANDOM_FOREST': 2,
            'Algorithms.GRADIENT_BOOST': 3,
            'Algorithms.SUPPORT_VECTOR_MACHINES': 4,
            'Algorithms.LINEAR_REGRESSION': 901,
            'Algorithms.DECISION_TREE': 902,
            'Algorithms.KERNEL_RIDGE': 903,
            'UnbiasInProcAlgorithms.PREJUDICE_REMOVER': 101,
            'UnbiasInProcAlgorithms.ADVERSARIAL_DEBIASING': 102,
            'UnbiasInProcAlgorithms.EXPONENTIATED_GRADIENT_REDUCTION': 103,
            'UnbiasInProcAlgorithms.RICH_SUBGROUP_FAIRNESS': 104,
            'UnbiasInProcAlgorithms.GRID_SEARCH_REDUCTION': 105,
            'UnbiasInProcAlgorithms.META_FAIR_CLASSIFIER': 106,
            'UnbiasInProcAlgorithms.ART_CLASSIFIER': 107
        }

        return next(filter(lambda a: a[0] == algorithm, indexes.items()))[1]
\end{lstlisting}
\end{enumerate}

\subsubsection{Interface (Frontend)}

\begin{enumerate}
\item Neste sistema, a parte do Frontend se encontra na pasta \textbf{ml-ui/src}. Dentro dela, no arquivo \textbf{Manual-Pipeline-Menu.js}, adicionar a opção colocada na etapa de Workflow no componente Select onde estão as outras opções de algoritmos.

\begin{lstlisting}[language=Python, label=cod:AddAlgorithmManual]
<Select
  sx={{fontSize: '14px'}}
  value={trainAlgorithm}
  onChange={handleTrainAlgorithmChange}
  displayEmpty
  inputProps={{ 'aria-label': 'Without label' }}
>
  <MenuItem value={'Algorithms.LOGISTIC_REGRESSION'}>Logistic Regression</MenuItem>
  <MenuItem value={'Algorithms.RANDOM_FOREST'}>Random Forest</MenuItem>
  <MenuItem value={'Algorithms.GRADIENT_BOOST'}>Gradient Boost</MenuItem>
  <MenuItem value={'Algorithms.SUPPORT_VECTOR_MACHINES'}>Support Vector Machines</MenuItem>
</Select>
\end{lstlisting}

\item Na pasta \textbf{ml-ui/src} e dentro do arquivo \textbf{Planning-Menu.js}, adicionar as opções (adaptadas a opção corrente) na variável \textbf{validAlgorithms}.
\end{enumerate}

\begin{lstlisting}[language=Python, label=cod:AddPlanningConfig]
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.NOTHING", "UnbiasPostProcAlgorithms.NOTHING"],
      labels: ["Nome da nova opcao", "Sem metodo", "Sem metodo"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.NOTHING", "UnbiasPostProcAlgorithms.EQUALIZED_ODDS"],
      labels: ["Nome da nova opcao", "Sem metodo", "Equalized Odds"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.NOTHING", "UnbiasPostProcAlgorithms.CALIBRATED_EQUALIZED_ODDS"],
      labels: ["Nome da nova opcao", "Sem metodo", "Calibrated Equalized Odds"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.NOTHING", "UnbiasPostProcAlgorithms.REJECT_OPTION_CLASSIFICATION"],
      labels: ["Nome da nova opcao", "Sem metodo", "Reject Option Classification"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.REWEIGHING", "UnbiasPostProcAlgorithms.NOTHING"],
      labels: ["Nome da nova opcao", "Reweighing", "Sem metodo"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.DISPARATE_IMPACT_REMOVER", "UnbiasPostProcAlgorithms.NOTHING"],
      labels: ["Nome da nova opcao", "Disparate Impact Remover", "Sem metodo"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.OPTIMIZED_PREPROCESSING", "UnbiasPostProcAlgorithms.NOTHING"],
      labels: ["Nome da nova opcao", "Optimized Preprocessing", "Sem metodo"],
      selected: true
    },
    {
      options: ["Algorithms.NOVA_OPCAO", "UnbiasDataAlgorithms.LEARNING_FAIR_REPRESENTATIONS", "UnbiasPostProcAlgorithms.NOTHING"],
      labels: ["Nome da nova opcao", "Learning Fair Representations", "Sem metodo"],
      selected: true
    }
\end{lstlisting}

\chapter{Questionário Aplicado ao Desenvolvedor}
\label{ann:Questionary}

\textbf{Observação:} Nome e e-mail não disponibilizados para manter o anonimato.
\linebreak
\linebreak
\fbox{\begin{minipage}{38em}
\textbf{Nome:} ************

\textbf{E-mail:} *******************

\textbf{Anos de experiência em Desenvolvimento de Aplicações:}

\begin{itemize}
\item[\Square] Não tive experiência
\item[\Square] Até 2 Anos
\item[\Square] 2 a 5 Anos
\item[\XBox] Mais de 5 Anos
\end{itemize}

\textbf{Anos de experiência em Engenharia de Dados:} 

\begin{itemize}
\item[\Square] Não tive experiência
\item[\XBox] Até 2 Anos
\item[\Square] 2 a 5 Anos
\item[\Square] Mais de 5 Anos
\end{itemize}

\textbf{Anos de experiência em Ciência de Dados:} 

\begin{itemize}
\item[\Square] Não tive experiência
\item[\XBox] Até 2 Anos
\item[\Square] 2 a 5 Anos
\item[\Square] Mais de 5 Anos
\end{itemize}
\vskip 1cm
\end{minipage}}

\noindent
\fbox{\begin{minipage}{38em}
\textbf{Quais os pontos positivos ao realizar o desenvolvimento?}

A documentação possui informações detalhadas sobre o que é e como funciona o Framework, explicando as tecnologias usadas, como instalar e usar.
\linebreak
\linebreak
\textbf{O que poderia melhorar a experiência do desenvolvimento?}

Alguns pontos que levantei na documentação sobre a referência dos diretórios.
\linebreak
\linebreak
\textbf{Comentários/Sugestões adicionais a serem feitas?}

Consegui criar uma função nova com o método Naive Bayes com certa facilidade e gostei bastante do trabalho.
\end{minipage}}

\end{document}
